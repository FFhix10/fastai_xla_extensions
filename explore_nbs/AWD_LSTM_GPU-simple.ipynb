{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"VERSION = \"20200707\" #\"nightly\"  #\"20200515\" @param [\"1.5\" , \"20200325\", \"nightly\"]\n!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null\n!python pytorch-xla-env-setup.py --version $VERSION > /dev/null\n#import torch_xla.core.xla_model as xm","execution_count":1,"outputs":[{"output_type":"stream","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  5115  100  5115    0     0  10503      0 --:--:-- --:--:-- --:--:-- 10524\nCopying gs://tpu-pytorch/wheels/torch-nightly+20200707-cp37-cp37m-linux_x86_64.whl...\n/ [1 files][107.5 MiB/107.5 MiB]                                                \nOperation completed over 1 objects/107.5 MiB.                                    \nCopying gs://tpu-pytorch/wheels/torch_xla-nightly+20200707-cp37-cp37m-linux_x86_64.whl...\n/ [1 files][123.8 MiB/123.8 MiB]                                                \nOperation completed over 1 objects/123.8 MiB.                                    \nCopying gs://tpu-pytorch/wheels/torchvision-nightly+20200707-cp37-cp37m-linux_x86_64.whl...\n/ [1 files][  2.2 MiB/  2.2 MiB]                                                \nOperation completed over 1 objects/2.2 MiB.                                      \n\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n\u001b[31mERROR: kornia 0.3.1 has requirement torch==1.5.0, but you'll have torch 1.7.0a0+12b5bdc which is incompatible.\u001b[0m\n\u001b[31mERROR: allennlp 1.0.0 has requirement torch<1.6.0,>=1.5.0, but you'll have torch 1.7.0a0+12b5bdc which is incompatible.\u001b[0m\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\ndebconf: delaying package configuration, since apt-utils is not installed\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install https://github.com/butchland/fastai_xla_extensions/archive/master.zip > /dev/null\nimport fastai_xla_extensions.core","execution_count":2,"outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install fastai2 > /dev/null\nfrom fastai2.text.all import *","execution_count":3,"outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"default_device()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"device(type='xla', index=1)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = untar_data(URLs.HUMAN_NUMBERS)","execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#hide\nPath.BASE_PATH = path","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path.ls()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"(#2) [Path('valid.txt'),Path('train.txt')]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"default_device()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"device(type='xla', index=1)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"lines = L()\nwith open(path/'train.txt') as f: lines += L(*f.readlines())\nwith open(path/'valid.txt') as f: lines += L(*f.readlines())\nlines","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = ' . '.join([l.strip() for l in lines])\ntext[:100]","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = text.split(' ')\ntokens[:10]","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = L(*tokens).unique()\nvocab","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2idx = {w:i for i,w in enumerate(vocab)}\nnums = L(word2idx[i] for i in tokens)\nnums","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"(#63095) [0,1,2,1,3,1,4,1,5,1...]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"L((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3))","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"(#21031) [(['one', '.', 'two'], '.'),(['.', 'three', '.'], 'four'),(['four', '.', 'five'], '.'),(['.', 'six', '.'], 'seven'),(['seven', '.', 'eight'], '.'),(['.', 'nine', '.'], 'ten'),(['ten', '.', 'eleven'], '.'),(['.', 'twelve', '.'], 'thirteen'),(['thirteen', '.', 'fourteen'], '.'),(['.', 'fifteen', '.'], 'sixteen')...]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\nseqs","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1, 14]), 1),(tensor([ 1, 15,  1]), 16)...]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dede = default_device()\ndede","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"device(type='xla', index=1)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"bs = 64\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False, device=dede)\n","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dls.device)","execution_count":18,"outputs":[{"output_type":"stream","text":"xla:1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LMModel1(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        \n    def forward(self, x):\n        h = F.relu(self.h_h(self.i_h(x[:,0])))\n        h = h + self.i_h(x[:,1])\n        h = F.relu(self.h_h(h))\n        h = h + self.i_h(x[:,2])\n        h = F.relu(self.h_h(h))\n        return self.h_o(h)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, \n                metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)","execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.842783</td>\n      <td>2.060987</td>\n      <td>0.466841</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.416377</td>\n      <td>1.794350</td>\n      <td>0.468029</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.421291</td>\n      <td>1.695977</td>\n      <td>0.491799</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.380146</td>\n      <td>1.675543</td>\n      <td>0.475874</td>\n      <td>00:08</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/fastai2/callback/schedule.py:68: UserWarning: This overload of nonzero is deprecated:\n\tnonzero()\nConsider using one of the following signatures instead:\n\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n  idx = (pos >= pcts).nonzero().max()\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# n,counts = 0,torch.zeros(len(vocab)).to(dede)\n# for x,y in dls.valid:\n#     n += y.shape[0]\n#     for i in range_of(vocab): counts[i] += (y==i).long().sum()\n# idx = torch.argmax(counts)\n# idx, vocab[idx.item()], counts[idx].item()/n","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LMModel2(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        \n    def forward(self, x):\n        h = 0\n        for i in range(3):\n            h = h + self.i_h(x[:,i])\n            h = F.relu(self.h_h(h))\n        return self.h_o(h)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"ok\")","execution_count":23,"outputs":[{"output_type":"stream","text":"ok\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, \n                metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)","execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.819275</td>\n      <td>1.952159</td>\n      <td>0.463275</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.397904</td>\n      <td>1.836674</td>\n      <td>0.467079</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.421806</td>\n      <td>1.701965</td>\n      <td>0.491799</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.375733</td>\n      <td>1.638597</td>\n      <td>0.491324</td>\n      <td>00:07</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LMModel3(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0\n        \n    def forward(self, x):\n        for i in range(3):\n            self.h = self.h + self.i_h(x[:,i])\n            self.h = F.relu(self.h_h(self.h))\n        out = self.h_o(self.h)\n        self.h = self.h.detach()\n        return out\n    \n    def reset(self): self.h = 0","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = len(seqs)//bs\nm,bs,len(seqs)","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"(328, 64, 21031)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def group_chunks(ds, bs):\n    m = len(ds) // bs\n    new_ds = L()\n    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n    return new_ds","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(\n    group_chunks(seqs[:cut], bs), \n    group_chunks(seqs[cut:], bs), \n    device=dede,\n    bs=bs, drop_last=True, shuffle=False)","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy,\n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(10, 3e-3)","execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.746547</td>\n      <td>1.776332</td>\n      <td>0.475000</td>\n      <td>01:41</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.232632</td>\n      <td>1.775582</td>\n      <td>0.451683</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.079604</td>\n      <td>1.654388</td>\n      <td>0.518750</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.995012</td>\n      <td>1.644257</td>\n      <td>0.522596</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.959275</td>\n      <td>1.590590</td>\n      <td>0.543750</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.923169</td>\n      <td>1.542976</td>\n      <td>0.579567</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.853682</td>\n      <td>1.559927</td>\n      <td>0.593029</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.823374</td>\n      <td>1.624618</td>\n      <td>0.575481</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.784064</td>\n      <td>1.652384</td>\n      <td>0.583413</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.768636</td>\n      <td>1.690405</td>\n      <td>0.583413</td>\n      <td>00:08</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dls.device","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"device(type='xla', index=1)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sl = 16\nseqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n         for i in range(0,len(nums)-sl-1,sl))\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n                             group_chunks(seqs[cut:], bs),\n                             device=dede,\n                             bs=bs, drop_last=True, shuffle=False)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[L(vocab[o] for o in s) for s in seqs[0]]","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"[(#16) ['one','.','two','.','three','.','four','.','five','.'...],\n (#16) ['.','two','.','three','.','four','.','five','.','six'...]]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LMModel4(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0\n        \n    def forward(self, x):\n        outs = []\n        for i in range(sl):\n            self.h = self.h + self.i_h(x[:,i])\n            self.h = F.relu(self.h_h(self.h))\n            outs.append(self.h_o(self.h))\n        self.h = self.h.detach()\n        return torch.stack(outs, dim=1)\n    \n    def reset(self): self.h = 0","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_func(inp, targ):\n    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,\n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)","execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3.144958</td>\n      <td>3.037310</td>\n      <td>0.189941</td>\n      <td>00:24</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2.277009</td>\n      <td>1.923928</td>\n      <td>0.434408</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.742745</td>\n      <td>1.868318</td>\n      <td>0.391276</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.481346</td>\n      <td>1.868173</td>\n      <td>0.434652</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.315012</td>\n      <td>1.879646</td>\n      <td>0.486816</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.201695</td>\n      <td>1.741681</td>\n      <td>0.501302</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.088088</td>\n      <td>1.703475</td>\n      <td>0.569661</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.976272</td>\n      <td>1.831994</td>\n      <td>0.570068</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.898052</td>\n      <td>1.687140</td>\n      <td>0.597249</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.834525</td>\n      <td>1.671103</td>\n      <td>0.594645</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.778493</td>\n      <td>1.737159</td>\n      <td>0.616048</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.736576</td>\n      <td>1.792271</td>\n      <td>0.613444</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.704156</td>\n      <td>1.912772</td>\n      <td>0.611735</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.681350</td>\n      <td>1.798130</td>\n      <td>0.627441</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.668870</td>\n      <td>1.807569</td>\n      <td>0.624105</td>\n      <td>00:02</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LMModel5(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = torch.zeros(n_layers, bs, n_hidden).to(dede) # NOTE: needed to change this for TPU v3-8\n        \n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = h.detach()\n        return self.h_o(res)\n    \n    def reset(self): self.h.zero_()","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = Learner(dls, LMModel5(len(vocab), 64, 2), \n                loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)","execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3.029038</td>\n      <td>2.550593</td>\n      <td>0.421549</td>\n      <td>00:32</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2.138358</td>\n      <td>1.736395</td>\n      <td>0.471110</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.700264</td>\n      <td>1.903858</td>\n      <td>0.321370</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.513305</td>\n      <td>1.819270</td>\n      <td>0.398031</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.379344</td>\n      <td>1.757702</td>\n      <td>0.469482</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.229444</td>\n      <td>1.822556</td>\n      <td>0.524902</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.080847</td>\n      <td>1.939046</td>\n      <td>0.528971</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.961903</td>\n      <td>1.937196</td>\n      <td>0.533610</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.847459</td>\n      <td>1.785479</td>\n      <td>0.565592</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.747109</td>\n      <td>1.676711</td>\n      <td>0.570557</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.664296</td>\n      <td>1.642464</td>\n      <td>0.574463</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.600646</td>\n      <td>1.648378</td>\n      <td>0.571289</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.556706</td>\n      <td>1.632000</td>\n      <td>0.579102</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.529971</td>\n      <td>1.636282</td>\n      <td>0.580078</td>\n      <td>00:02</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.515655</td>\n      <td>1.638859</td>\n      <td>0.579915</td>\n      <td>00:02</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.forget_gate = nn.Linear(ni + nh, nh)\n        self.input_gate  = nn.Linear(ni + nh, nh)\n        self.cell_gate   = nn.Linear(ni + nh, nh)\n        self.output_gate = nn.Linear(ni + nh, nh)\n\n    def forward(self, input, state):\n        h,c = state\n        h = torch.stack([h, input], dim=1)\n        forget = torch.sigmoid(self.forget_gate(h))\n        c = c * forget\n        inp = torch.sigmoid(self.input_gate(h))\n        cell = torch.tanh(self.cell_gate(h))\n        c = c + inp * cell\n        out = torch.sigmoid(self.output_gate(h))\n        h = outgate * torch.tanh(c)\n        return h, (h,c)","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.ih = nn.Linear(ni,4*nh)\n        self.hh = nn.Linear(nh,4*nh)\n\n    def forward(self, input, state):\n        h,c = state\n        # One big multiplication for all the gates is better than 4 smaller ones\n        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)\n        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n        cellgate = gates[3].tanh()\n\n        c = (forgetgate*c) + (ingate*cellgate)\n        h = outgate * c.tanh()\n        return h, (h,c)","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = torch.arange(0,10); t","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.chunk(2)","execution_count":43,"outputs":[{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"(tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9]))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LMModel6(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = [torch.zeros(n_layers, bs, n_hidden).to(dede) for _ in range(2)] # NOTE: added to(dede)\n        \n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(res)\n    \n    def reset(self): \n        for h in self.h: h.zero_()","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = Learner(dls, LMModel6(len(vocab), 64, 2), \n                loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 1e-2)","execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3.039099</td>\n      <td>2.741736</td>\n      <td>0.267578</td>\n      <td>01:23</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2.157704</td>\n      <td>2.040138</td>\n      <td>0.269694</td>\n      <td>00:05</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.606695</td>\n      <td>1.855399</td>\n      <td>0.427653</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.346232</td>\n      <td>1.956562</td>\n      <td>0.497314</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.105539</td>\n      <td>2.198174</td>\n      <td>0.510824</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.843588</td>\n      <td>2.075727</td>\n      <td>0.543701</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.588275</td>\n      <td>1.943998</td>\n      <td>0.594808</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.389649</td>\n      <td>2.112300</td>\n      <td>0.666748</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.270380</td>\n      <td>1.860816</td>\n      <td>0.707275</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.204832</td>\n      <td>2.072631</td>\n      <td>0.695964</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.174363</td>\n      <td>2.038648</td>\n      <td>0.691732</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.128929</td>\n      <td>1.970773</td>\n      <td>0.725830</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.096488</td>\n      <td>2.031816</td>\n      <td>0.722738</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.076960</td>\n      <td>2.030923</td>\n      <td>0.721029</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.068076</td>\n      <td>2.021481</td>\n      <td>0.722575</td>\n      <td>00:03</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Dropout(Module):\n    def __init__(self, p): self.p = p\n    def forward(self, x):\n        if not self.training: return x\n        mask = x.new(*x.shape).bernoulli_(1-p)\n        return x * mask.div_(1-p)","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LMModel7(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h_o.weight = self.i_h.weight\n        self.h = [torch.zeros(n_layers, bs, n_hidden).to(dede) for _ in range(2)]\n        \n    def forward(self, x):\n        raw,h = self.rnn(self.i_h(x), self.h)\n        out = self.drop(raw)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(out),raw,out\n    \n    def reset(self): \n        for h in self.h: h.zero_()","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n                cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)])","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# added to test previous learner\nlearn.fit_one_cycle(5, 1e-2, wd=0.1)","execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2.264722</td>\n      <td>1.682903</td>\n      <td>0.498779</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.473552</td>\n      <td>1.307538</td>\n      <td>0.581217</td>\n      <td>00:06</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.882237</td>\n      <td>0.755654</td>\n      <td>0.749105</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.533465</td>\n      <td>0.623435</td>\n      <td>0.801758</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.373853</td>\n      <td>0.588726</td>\n      <td>0.807129</td>\n      <td>00:03</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(15, 1e-2, wd=0.1)","execution_count":51,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2.616411</td>\n      <td>1.819634</td>\n      <td>0.481201</td>\n      <td>00:13</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.705550</td>\n      <td>1.396919</td>\n      <td>0.600667</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.988706</td>\n      <td>0.695252</td>\n      <td>0.786133</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.564667</td>\n      <td>0.529431</td>\n      <td>0.845622</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.354679</td>\n      <td>0.463153</td>\n      <td>0.864583</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.250466</td>\n      <td>0.386827</td>\n      <td>0.885173</td>\n      <td>00:04</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.187196</td>\n      <td>0.333289</td>\n      <td>0.903890</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.155309</td>\n      <td>0.340555</td>\n      <td>0.897217</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.146060</td>\n      <td>0.358394</td>\n      <td>0.883301</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.125267</td>\n      <td>0.335579</td>\n      <td>0.892578</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.111357</td>\n      <td>0.315474</td>\n      <td>0.900472</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.102690</td>\n      <td>0.303824</td>\n      <td>0.904378</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.096979</td>\n      <td>0.295263</td>\n      <td>0.913086</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.093119</td>\n      <td>0.301017</td>\n      <td>0.906820</td>\n      <td>00:03</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.090860</td>\n      <td>0.302670</td>\n      <td>0.905192</td>\n      <td>00:03</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"metadata":{"trusted":false},"cell_type":"code","source":"sl = 16\nseqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n         for i in range(0,len(nums)-sl-1,sl))\ncut = int(len(seqs) * 0.8)\ndecpu = torch.device(\"cpu\")\ndls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n                             group_chunks(seqs[cut:], bs),\n                             device=decpu,\n                             bs=bs, drop_last=True, shuffle=False)\n\nprint(f\"dls.device={dls.device}\")\nclass LMModel7_CPU(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h_o.weight = self.i_h.weight\n        self.h = [torch.zeros(n_layers, bs, n_hidden).to(decpu) for _ in range(2)]\n        \n    def forward(self, x):\n        raw,h = self.rnn(self.i_h(x), self.h)\n        out = self.drop(raw)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(out),raw,out\n    \n    def reset(self): \n        for h in self.h: h.zero_()\n\nlearn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n\nlearn.fit_one_cycle(15, 1e-2, wd=0.1)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}
