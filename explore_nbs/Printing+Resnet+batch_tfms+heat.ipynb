{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Printing+Resnet+batch_tfms+heat.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPYaEcCdybK3oi85/L1TiLY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tyoc213/fastai_xla_extensions/blob/explorations1/explore_nbs/Printing%2BResnet%2Bbatch_tfms%2Bheat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GohprGE9IMbD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2b25e03d-c95f-4612-bff6-a4102f851d43"
      },
      "source": [
        "VERSION = \"20200707\" #\"nightly\"  #\"20200515\" @param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  5115  100  5115    0     0  53842      0 --:--:-- --:--:-- --:--:-- 53842\n",
            "Updating... This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-dev20200515 ...\n",
            "Collecting cloud-tpu-client\n",
            "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
            "Collecting google-api-python-client==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client) (4.1.3)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.17.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.17.2)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.12.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.2.8)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.52.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2018.9)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.12.2)\n",
            "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (49.1.0)\n",
            "Uninstalling torch-1.5.1+cu101:\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2020.6.20)\n",
            "Installing collected packages: google-api-python-client, cloud-tpu-client\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n",
            "Done updating TPU runtime\n",
            "  Successfully uninstalled torch-1.5.1+cu101\n",
            "Uninstalling torchvision-0.6.1+cu101:\n",
            "  Successfully uninstalled torchvision-0.6.1+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20200515-cp36-cp36m-linux_x86_64.whl...\n",
            "\\ [1 files][ 91.0 MiB/ 91.0 MiB]                                                \n",
            "Operation completed over 1 objects/91.0 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200515-cp36-cp36m-linux_x86_64.whl...\n",
            "\\ [1 files][119.5 MiB/119.5 MiB]                                                \n",
            "Operation completed over 1 objects/119.5 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200515-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.3 MiB/  2.3 MiB]                                                \n",
            "Operation completed over 1 objects/2.3 MiB.                                      \n",
            "Processing ./torch-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200515) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200515) (1.18.5)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.6.0a0+bf2bbd9\n",
            "Processing ./torch_xla-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+2b2085a\n",
            "Processing ./torchvision-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200515) (7.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200515) (1.6.0a0+bf2bbd9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200515) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20200515) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.7.0a0+a6073f0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (374 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 144465 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RJ_MYuzIPJC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "056a4ff9-db33-47e2-f11e-ad40d3bc23b1"
      },
      "source": [
        "!pip install https://github.com/fastai/fastcore/archive/master.zip\n",
        "!pip install https://github.com/fastai/fastai2/archive/master.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/fastai/fastcore/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/fastai/fastcore/archive/master.zip\n",
            "\u001b[K     - 3.5MB 2.1MB/s\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastcore==0.1.18) (1.18.5)\n",
            "Requirement already satisfied: dataclasses>='0.7' in /usr/local/lib/python3.6/dist-packages (from fastcore==0.1.18) (0.7)\n",
            "Building wheels for collected packages: fastcore\n",
            "  Building wheel for fastcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastcore: filename=fastcore-0.1.18-cp36-none-any.whl size=28891 sha256=36950ebbe97c27ebcc38471ed8b876294aa4547427aa1e604b3cabdaf90ed9eb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1e3_h2zu/wheels/68/af/c5/c8a9f7370515ab9b237b3fd59c04a0e7c5d4dedc7a6768a772\n",
            "Successfully built fastcore\n",
            "Installing collected packages: fastcore\n",
            "Successfully installed fastcore-0.1.18\n",
            "Collecting https://github.com/fastai/fastai2/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/fastai/fastai2/archive/master.zip\n",
            "\u001b[K     - 121.7MB 795kB/s\n",
            "\u001b[?25hRequirement already satisfied: fastcore in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.18) (0.1.18)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.18) (1.6.0a0+bf2bbd9)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.18) (0.7.0a0+a6073f0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.18) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.18) (1.0.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.18) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.18) (3.13)\n",
            "Requirement already satisfied: fastprogress>=0.1.22 in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.18) (0.2.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.18) (7.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.18) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.18) (1.4.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.18) (2.2.4)\n",
            "Requirement already satisfied: dataclasses>='0.7'; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastcore->fastai2==0.0.18) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastcore->fastai2==0.0.18) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->fastai2==0.0.18) (0.16.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai2==0.0.18) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai2==0.0.18) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai2==0.0.18) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai2==0.0.18) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai2==0.0.18) (2018.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai2==0.0.18) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai2==0.0.18) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai2==0.0.18) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai2==0.0.18) (2020.6.20)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->fastai2==0.0.18) (0.16.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2==0.0.18) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2==0.0.18) (0.7.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2==0.0.18) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2==0.0.18) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2==0.0.18) (3.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2==0.0.18) (2.0.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2==0.0.18) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2==0.0.18) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2==0.0.18) (49.1.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2==0.0.18) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2==0.0.18) (1.1.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->fastai2==0.0.18) (1.12.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->fastai2==0.0.18) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->fastai2==0.0.18) (3.1.0)\n",
            "Building wheels for collected packages: fastai2\n",
            "  Building wheel for fastai2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastai2: filename=fastai2-0.0.18-cp36-none-any.whl size=175594 sha256=2d921efd963d6f8fb2b1d7a710bf76a9be354aea3ecd090db9583f82a05ed1b3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1od2hht4/wheels/b2/28/3b/7a38644c6129e851e0db09b594949028266a62f4cb234fd1f3\n",
            "Successfully built fastai2\n",
            "Installing collected packages: fastai2\n",
            "Successfully installed fastai2-0.0.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x10cC5znIRie",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a8cb718-256c-48a0-b43d-a178fd948040"
      },
      "source": [
        "!pip install py-heat-magic"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting py-heat-magic\n",
            "  Downloading https://files.pythonhosted.org/packages/72/15/c5c56c91077ecb42a6415364cff41acb08a4f696f0261b3fc4025856d85c/py-heat-magic-0.0.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from py-heat-magic) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from py-heat-magic) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from py-heat-magic) (3.2.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from py-heat-magic) (5.5.0)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from py-heat-magic) (1.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from py-heat-magic) (1.0.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.6/dist-packages (from py-heat-magic) (1.1.1)\n",
            "Collecting nose\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 3.5MB/s \n",
            "\u001b[?25hCollecting py-heat\n",
            "  Downloading https://files.pythonhosted.org/packages/6b/e3/776c6f4f18eafbc39b44ad47160414077bfd64183cfd2c5b61ea43dd12b6/py-heat-0.0.6.tar.gz\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->py-heat-magic) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->py-heat-magic) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->py-heat-magic) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->py-heat-magic) (0.10.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->py-heat-magic) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->py-heat-magic) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->py-heat-magic) (4.3.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->py-heat-magic) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->py-heat-magic) (1.0.18)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->py-heat-magic) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->py-heat-magic) (49.1.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->py-heat-magic) (2.1.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->py-heat-magic) (5.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->py-heat-magic) (5.2.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->py-heat-magic) (5.2.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->py-heat-magic) (4.7.5)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->py-heat-magic) (4.10.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->py-heat-magic) (7.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->py-heat-magic) (2018.9)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.6/dist-packages (from sympy->py-heat-magic) (1.1.0)\n",
            "Collecting pprofile\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/db/198fd60559f77c5334f520e2a94cde5c8c6f5f4e65f82f5b22269e6e5ff2/pprofile-2.0.5.tar.gz (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->py-heat-magic) (1.12.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->py-heat-magic) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->py-heat-magic) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->py-heat-magic) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->py-heat-magic) (0.8.4)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->py-heat-magic) (5.0.7)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->py-heat-magic) (0.4.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->py-heat-magic) (0.6.0)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->py-heat-magic) (2.11.2)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->py-heat-magic) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->py-heat-magic) (3.1.5)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->py-heat-magic) (4.6.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->py-heat-magic) (1.4.2)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->py-heat-magic) (0.8.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->py-heat-magic) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->py-heat-magic) (4.5.3)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->py-heat-magic) (1.9.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->py-heat-magic) (19.0.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->py-heat-magic) (3.5.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert->jupyter->py-heat-magic) (2.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter->py-heat-magic) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->py-heat-magic) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->py-heat-magic) (20.4)\n",
            "Building wheels for collected packages: py-heat-magic, py-heat, pprofile\n",
            "  Building wheel for py-heat-magic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-heat-magic: filename=py_heat_magic-0.0.2-cp36-none-any.whl size=3161 sha256=f98d9a7fdf2188985e1a1d9825d997f3ba748b2359e86bbf2e48674407327521\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/4c/14/8858096734820b8a1eece9f174fe4a96d199ba247a671dc7ed\n",
            "  Building wheel for py-heat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-heat: filename=py_heat-0.0.6-py2.py3-none-any.whl size=7801 sha256=74513465d758625bd4d9c4348b5d9604068b6296111eaa919ebed7cf5c2ebf18\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/3d/b7/e191bd2f2d8c0a6fc5008a87cdf69d6bc009ee3e22149c3ef3\n",
            "  Building wheel for pprofile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pprofile: filename=pprofile-2.0.5-cp36-none-any.whl size=34838 sha256=ba4dd228bb5da0055d2151f28686e85c73f11a3e69248e5bd220e23e790eda39\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/7e/dc/fa6663a43280716fe9a4fedfcfaae1e22fc26c701afff093c6\n",
            "Successfully built py-heat-magic py-heat pprofile\n",
            "Installing collected packages: nose, pprofile, py-heat, py-heat-magic\n",
            "Successfully installed nose-1.3.7 pprofile-2.0.5 py-heat-0.0.6 py-heat-magic-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdG298JLIThO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext heat"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkz8dNMFIVhO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d5dffa50-0f40-4bc3-e3d4-afd977925122"
      },
      "source": [
        "%%heat\n",
        "#import pdb\n",
        "print(\"---------------------------------------------------------- START\")\n",
        "#import torch\n",
        "import torch_xla.core.xla_model as xm\n",
        "\n",
        "tpu_device = xm.xla_device()\n",
        "#tpu_device = torch.device('cuda:0') #torch.cuda.device(\"cuda:0\")#.current_device()\n",
        "print(f\"device is {tpu_device}\")\n",
        "print(\"---------------------------------------------------------- START:1\")\n",
        "\n",
        "from fastai2.vision.all import *\n",
        "path = untar_data(URLs.MNIST)\n",
        "datablock = DataBlock(\n",
        "    blocks=(ImageBlock,CategoryBlock),\n",
        "    get_items=get_image_files,\n",
        "    get_y=parent_label,\n",
        "    splitter=RandomSubsetSplitter(train_sz=0.001,valid_sz=0.001),#GrandparentSplitter(train_name='training',valid_name='testing'),\n",
        "#    item_tfms=Resize(28),\n",
        "    #batch_tfms=aug_transforms(size=224)\n",
        "    batch_tfms=aug_transforms(size=224,min_scale=0.75)\n",
        ")\n",
        "print(\"---------------------------------------------------------- START:2\")\n",
        "\n",
        "#datablock.summary(path)\n",
        "\n",
        "# %%\n",
        "printed_device=0\n",
        "printed_device2=0\n",
        "NUM_PIXELS=3 # 1 single channedl 3 rgb\n",
        "NUM_OUTPUTS=10 #2 for MNIST_TINY 10 MNIST\n",
        "class Lenet2(nn.Module):\n",
        "    \"\"\"Lenet with layers\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Lenet2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(NUM_PIXELS, 6, 3) # set 3 for first item if RGB\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        self.fc1 = nn.Linear(46656, 120) #(400, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, NUM_OUTPUTS) # Only 2 outputs (3 and 7) instead of 10\n",
        "    def forward(self, x):\n",
        "        global printed_device\n",
        "        if x.device != tpu_device:\n",
        "          print(f\"************** ######################### EL DEVICE ES {tpu_device} no es {x.device} ######################### **************\") if printed_device < 10 else noop\n",
        "          printed_device += 1\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "print(\"---------------------------------------------------------- START:3\")\n",
        "\n",
        "# %%\n",
        "dls_tpu = datablock.dataloaders(path,bs=1, device=tpu_device)\n",
        "dls_tpu.vocab, dls_tpu.show_batch()\n",
        "\n",
        "print(\"---------------------------------------------------------- START:4\")\n",
        "\n",
        "# %%\n",
        "def print_local(msg):\n",
        "  if False: return\n",
        "  print(msg)\n",
        "\n",
        "print(\"---------------------------------------------------------- START:5\")\n",
        "\n",
        "class XLAOptimProxy:\n",
        "    def __init__(self,opt:Optimizer):\n",
        "        # TRACE: print(\"XLAOptimProxy#inicializando __init__\")\n",
        "        self.opt = opt\n",
        "\n",
        "    def xla_step(self):\n",
        "        print(\"------------- xla optimizer!!!!!!!! BARRIER TRYE\")\n",
        "        xm.optimizer_step(self.opt,barrier=True) # sync on gradient update\n",
        "\n",
        "    def __getattr__(self,name):\n",
        "        if name == '___step___': # override proxying for step method\n",
        "                print_local(\"calling xla_step\")\n",
        "                return getattr(self,'xla_step')\n",
        "        # proxy everything else\n",
        "        #print_local(f\"calling XLAOptimProxy#{name}\")\n",
        "        return getattr(self.opt,name)\n",
        "\n",
        "\n",
        "print(\"---------------------------------------------------------- START:6\")\n",
        "\n",
        "@patch_to(ParamScheduler)\n",
        "def _update_val(self, pct):\n",
        "#        for n,f in self.scheds.items(): self.opt.set_hyper(n, f(pct))\n",
        "        for n,f in self.scheds.items():\n",
        "            v = f(pct)\n",
        "            #print_local(f\"---------------------- A f(pct) = {v}\")\n",
        "            self.opt.set_hyper(n, v)\n",
        "\n",
        "print(\"---------------------------------------------------------- START:7\")\n",
        "\n",
        "@patch_to(ParamScheduler)\n",
        "def after_batch(self):\n",
        "#        for p in self.scheds.keys(): self.hps[p].append(self.opt.hypers[-1][p])\n",
        "        for p in self.scheds.keys():\n",
        "            v = self.opt.hypers[-1][p]\n",
        "            #print_local(f\"---------------------- B after_batch ParamScheduler {v}\")\n",
        "            self.hps[p].append(v)\n",
        "\n",
        "print(\"---------------------------------------------------------- START:8 enbd STARET\")\n",
        "\n",
        "# %%\n",
        "####################################################################################333\n",
        "\n",
        "\n",
        "print(\"---------------------- STARTED class Learner\")\n",
        "\n",
        "@log_args(but='dls,model,opt_func,cbs')\n",
        "class Learner():\n",
        "    def __init__(self, dls, model, loss_func=None, opt_func=Adam, lr=defaults.lr, splitter=trainable_params, cbs=None,\n",
        "                 metrics=None, path=None, model_dir='models', wd=None, wd_bn_bias=False, train_bn=True,\n",
        "                 moms=(0.95,0.85,0.95)):\n",
        "        store_attr(self, \"dls,model,opt_func,lr,splitter,model_dir,wd,wd_bn_bias,train_bn,metrics,moms\")\n",
        "        self.training,self.create_mbar,self.logger,self.opt,self.cbs = False,True,print,None,L()\n",
        "        if loss_func is None:\n",
        "            loss_func = getattr(dls.train_ds, 'loss_func', None)\n",
        "            assert loss_func is not None, \"Could not infer loss function from the data, please pass a loss function.\"\n",
        "        self.loss_func = loss_func\n",
        "        self.path = Path(path) if path is not None else getattr(dls, 'path', Path('.'))\n",
        "        self.add_cbs([(cb() if isinstance(cb, type) else cb) for cb in L(defaults.callbacks)+L(cbs)])\n",
        "        self.model.to(self.dls.device)\n",
        "        if hasattr(self.model, 'reset'): self.model.reset()\n",
        "        self.epoch,self.n_epoch,self.loss = 0,1,tensor(0.)\n",
        "\n",
        "    @property\n",
        "    def metrics(self): return self._metrics\n",
        "    @metrics.setter\n",
        "    def metrics(self,v): self._metrics = L(v).map(mk_metric)\n",
        "\n",
        "    def _grab_cbs(self, cb_cls): return L(cb for cb in self.cbs if isinstance(cb, cb_cls))\n",
        "    def add_cbs(self, cbs): L(cbs).map(self.add_cb)\n",
        "    def remove_cbs(self, cbs): L(cbs).map(self.remove_cb)\n",
        "    def add_cb(self, cb):\n",
        "        old = getattr(self, cb.name, None)\n",
        "        assert not old or isinstance(old, type(cb)), f\"self.{cb.name} already registered\"\n",
        "        cb.learn = self\n",
        "        setattr(self, cb.name, cb)\n",
        "        self.cbs.append(cb)\n",
        "        return self\n",
        "\n",
        "    def remove_cb(self, cb):\n",
        "        if isinstance(cb, type): self.remove_cbs(self._grab_cbs(cb))\n",
        "        else:\n",
        "            cb.learn = None\n",
        "            if hasattr(self, cb.name): delattr(self, cb.name)\n",
        "            if cb in self.cbs: self.cbs.remove(cb)\n",
        "\n",
        "    @contextmanager\n",
        "    def added_cbs(self, cbs):\n",
        "        self.add_cbs(cbs)\n",
        "        try: yield\n",
        "        finally: self.remove_cbs(cbs)\n",
        "\n",
        "    @contextmanager\n",
        "    def removed_cbs(self, cbs):\n",
        "        self.remove_cbs(cbs)\n",
        "        try: yield self\n",
        "        finally: self.add_cbs(cbs)\n",
        "\n",
        "    def ordered_cbs(self, event): return [cb for cb in sort_by_run(self.cbs) if hasattr(cb, event)]\n",
        "\n",
        "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
        "    def _call_one(self, event_name):\n",
        "        assert hasattr(event, event_name)\n",
        "        myl = []\n",
        "        if False:\n",
        "            # NOTE: no va\n",
        "            return\n",
        "        # TRACE: print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}   SORT BY NAME\")\n",
        "        lll = sort_by_run(self.cbs)\n",
        "        # TRACE: print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}   SORT BY NAME {lll}\")\n",
        "        for cb in lll:\n",
        "            # TRACE: print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}   calling {cb}({event_name})\")\n",
        "            r=cb(event_name)\n",
        "            # TRACE: print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}   calling {cb}({event_name})\")\n",
        "            myl.append(r)\n",
        "        # TRACE: print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}   SORT BY NAME END\")\n",
        "        # TRACE: print(\"-------------------------------------------------------------------------------------------------\")\n",
        "        # TRACE: print(\"----------------------------------------------------------\")\n",
        "        # TRACE: print(\"------------------------\")\n",
        "        #[cb(event_name) for cb in sort_by_run(self.cbs)]\n",
        "\n",
        "    def _bn_bias_state(self, with_bias): return bn_bias_params(self.model, with_bias).map(self.opt.state)\n",
        "    def create_opt(self):\n",
        "        self.opt = self.opt_func(self.splitter(self.model), lr=self.lr)\n",
        "        if not self.wd_bn_bias:\n",
        "            for p in self._bn_bias_state(True ): p['do_wd'] = False\n",
        "        if self.train_bn:\n",
        "            for p in self._bn_bias_state(False): p['force_train'] = True\n",
        "\n",
        "    def _split(self, b):\n",
        "        i = getattr(self.dls, 'n_inp', 1 if len(b)==1 else len(b)-1)\n",
        "        self.xb,self.yb = b[:i],b[i:]\n",
        "\n",
        "    def all_batches(self):\n",
        "        self.n_iter = len(self.dl)\n",
        "        # TRACE: print(f\"Learner#all_batches@n_iter = {self.n_iter}\")\n",
        "        for o in enumerate(self.dl): self.one_batch(*o)\n",
        "\n",
        "    def one_batch(self, i, b):\n",
        "        # TRACE: print(f\"Learner#one_batch {i} b is the batch\") if i % 10000 == 0 else noop\n",
        "        self.iter = i\n",
        "        try:\n",
        "            self._split(b);                                  self('begin_batch')\n",
        "            self.pred = self.model(*self.xb);                self('after_pred')\n",
        "            if len(self.yb) == 0: return\n",
        "            self.loss = self.loss_func(self.pred, *self.yb); self('after_loss')\n",
        "            if not self.training: return\n",
        "            self.loss.backward();                            self('after_backward')\n",
        "            #self.opt.step();                                 self('after_step')\n",
        "            xm.optimizer_step(self.opt,barrier=True);                                 self('after_step')\n",
        "            self.opt.zero_grad()\n",
        "        except CancelBatchException:                         self('after_cancel_batch')\n",
        "        finally:                                             self('after_batch')\n",
        "\n",
        "    def _do_begin_fit(self, n_epoch):\n",
        "        # TRACE: print(\"Learner#_do_begin_fit\")\n",
        "        self.n_epoch,self.loss = n_epoch,tensor(0.);         self('begin_fit')\n",
        "\n",
        "    def _do_epoch_train(self):\n",
        "        try:\n",
        "            # TRACE: print(\"Learner#_do_epoch_train internal try\")\n",
        "            self.dl = self.dls.train;                        self('begin_train')\n",
        "            self.all_batches()\n",
        "        except CancelTrainException:                         self('after_cancel_train')\n",
        "        finally:                                             self('after_train')\n",
        "\n",
        "    def _do_epoch_validate(self, ds_idx=1, dl=None):\n",
        "        if dl is None: dl = self.dls[ds_idx]\n",
        "        try:\n",
        "            # TRACE: print(\"Learner#_do_epoch_validate@internal try\")\n",
        "            self.dl = dl;                                    self('begin_validate')\n",
        "            with torch.no_grad(): self.all_batches()\n",
        "        except CancelValidException:                         self('after_cancel_validate')\n",
        "        finally:                                             self('after_validate')\n",
        "\n",
        "    def _end_cleanup(self):\n",
        "        self.dl,self.xb,self.yb,self.pred,self.loss = None,(None,),(None,),None,None\n",
        "\n",
        "    @log_args(but='cbs')\n",
        "    def fit(self, n_epoch, lr=None, wd=None, cbs=None, reset_opt=False):\n",
        "        # TRACE: print(\"fit starts\")\n",
        "        with self.added_cbs(cbs):\n",
        "            # TRACE: print(\"fit starts with added callbacks\")\n",
        "            if reset_opt or not self.opt: self.create_opt()\n",
        "            if wd is None: wd = self.wd\n",
        "            if wd is not None: self.opt.set_hypers(wd=wd)\n",
        "            # TRACE: print(\"Learner#fit@set hypers\")\n",
        "            self.opt.set_hypers(lr=self.lr if lr is None else lr)\n",
        "            try:\n",
        "                # TRACE: print(\"Learner#fit@do begin fit\")\n",
        "                self._do_begin_fit(n_epoch)\n",
        "                # TRACE: print(\"Learner#fit@FOR each epoch LOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOP\")\n",
        "                for epoch in range(n_epoch):\n",
        "                    try:\n",
        "                        # TRACE: print(f\"Learner#fit@epoch... begin... {epoch}\")\n",
        "                        self.epoch=epoch;          self('begin_epoch')\n",
        "                        # TRACE: print(f\"Learner#fit@epoch... train... {epoch}\")\n",
        "                        self._do_epoch_train()\n",
        "                        # TRACE: print(f\"Learner#fit@epoc validate {epoch}\")\n",
        "                        self._do_epoch_validate()\n",
        "                    except CancelEpochException:   self('after_cancel_epoch')\n",
        "                    finally:                       self('after_epoch')\n",
        "\n",
        "            except CancelFitException:             self('after_cancel_fit')\n",
        "            finally:\n",
        "                # TRACE: self('after_fit')\n",
        "                self._end_cleanup()\n",
        "                # TRACE: print(\"Learner#fit@end cleanup\")\n",
        "\n",
        "    def validate(self, ds_idx=1, dl=None, cbs=None):\n",
        "        if dl is None: dl = self.dls[ds_idx]\n",
        "        with self.added_cbs(cbs), self.no_logging(), self.no_mbar():\n",
        "            self(_before_epoch)\n",
        "            self._do_epoch_validate(ds_idx, dl)\n",
        "            self(_after_epoch)\n",
        "        return getattr(self, 'final_record', None)\n",
        "\n",
        "    @delegates(GatherPredsCallback.__init__)\n",
        "    def get_preds(self, ds_idx=1, dl=None, with_input=False, with_decoded=False, with_loss=False, act=None,\n",
        "                  inner=False, reorder=True, **kwargs):\n",
        "        if dl is None: dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n",
        "        if reorder and hasattr(dl, 'get_idxs'):\n",
        "            idxs = dl.get_idxs()\n",
        "            dl = dl.new(get_idxs = _ConstantFunc(idxs))\n",
        "        cb = GatherPredsCallback(with_input=with_input, with_loss=with_loss, **kwargs)\n",
        "        ctx_mgrs = [self.no_logging(), self.added_cbs(cb), self.no_mbar()]\n",
        "        if with_loss: ctx_mgrs.append(self.loss_not_reduced())\n",
        "        with ExitStack() as stack:\n",
        "            for mgr in ctx_mgrs: stack.enter_context(mgr)\n",
        "            self(event.begin_epoch if inner else _before_epoch)\n",
        "            self._do_epoch_validate(dl=dl)\n",
        "            self(event.after_epoch if inner else _after_epoch)\n",
        "            if act is None: act = getattr(self.loss_func, 'activation', noop)\n",
        "            res = cb.all_tensors()\n",
        "            pred_i = 1 if with_input else 0\n",
        "            if res[pred_i] is not None:\n",
        "                res[pred_i] = act(res[pred_i])\n",
        "                if with_decoded: res.insert(pred_i+2, getattr(self.loss_func, 'decodes', noop)(res[pred_i]))\n",
        "            if reorder and hasattr(dl, 'get_idxs'): res = nested_reorder(res, tensor(idxs).argsort())\n",
        "            return tuple(res)\n",
        "        self._end_cleanup()\n",
        "\n",
        "    def predict(self, item, rm_type_tfms=None, with_input=False):\n",
        "        dl = self.dls.test_dl([item], rm_type_tfms=rm_type_tfms, num_workers=0)\n",
        "        inp,preds,_,dec_preds = self.get_preds(dl=dl, with_input=True, with_decoded=True)\n",
        "        i = getattr(self.dls, 'n_inp', -1)\n",
        "        inp = (inp,) if i==1 else tuplify(inp)\n",
        "        dec = self.dls.decode_batch(inp + tuplify(dec_preds))[0]\n",
        "        dec_inp,dec_targ = map(detuplify, [dec[:i],dec[i:]])\n",
        "        res = dec_targ,dec_preds[0],preds[0]\n",
        "        if with_input: res = (dec_inp,) + res\n",
        "        return res\n",
        "\n",
        "    def show_results(self, ds_idx=1, dl=None, max_n=9, shuffle=True, **kwargs):\n",
        "        if dl is None: dl = self.dls[ds_idx].new(shuffle=shuffle)\n",
        "        b = dl.one_batch()\n",
        "        _,_,preds = self.get_preds(dl=[b], with_decoded=True)\n",
        "        self.dls.show_results(b, preds, max_n=max_n, **kwargs)\n",
        "\n",
        "    def show_training_loop(self):\n",
        "        indent = 0\n",
        "        for s in _loop:\n",
        "            if s.startswith('Start'): print(f'{\" \"*indent}{s}'); indent += 2\n",
        "            elif s.startswith('End'): indent -= 2; print(f'{\" \"*indent}{s}')\n",
        "            else: print(f'{\" \"*indent} - {s:15}:', self.ordered_cbs(s))\n",
        "\n",
        "    @contextmanager\n",
        "    def no_logging(self): return replacing_yield(self, 'logger', noop)\n",
        "    @contextmanager\n",
        "    def no_mbar(self):    return replacing_yield(self, 'create_mbar', False)\n",
        "\n",
        "    @contextmanager\n",
        "    def loss_not_reduced(self):\n",
        "        if hasattr(self.loss_func, 'reduction'): return replacing_yield(self.loss_func, 'reduction', 'none')\n",
        "        else: return replacing_yield(self, 'loss_func', partial(self.loss_func, reduction='none'))\n",
        "\n",
        "    @delegates(save_model)\n",
        "    def save(self, file, **kwargs):\n",
        "        file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n",
        "        save_model(file, self.model, getattr(self,'opt',None), **kwargs)\n",
        "\n",
        "    @delegates(load_model)\n",
        "    def load(self, file, with_opt=None, device=None, **kwargs):\n",
        "        if device is None: device = self.dls.device\n",
        "        if self.opt is None: self.create_opt()\n",
        "        file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n",
        "        load_model(file, self.model, self.opt, device=device, **kwargs)\n",
        "        return self\n",
        "\n",
        "Learner.x,Learner.y = add_props(lambda i,x: detuplify((x.xb,x.yb)[i]))\n",
        "###################################################################33333\n",
        "\n",
        "def _is_instance(f, gs):\n",
        "    tst = [g if type(g) in [type, 'function'] else g.__class__ for g in gs]\n",
        "    for g in tst:\n",
        "        if isinstance(f, g) or f==g: return True\n",
        "    return False\n",
        "\n",
        "def _is_first(f, gs):\n",
        "    for o in L(getattr(f, 'run_after', None)):\n",
        "        if _is_instance(o, gs): return False\n",
        "    for g in gs:\n",
        "        if _is_instance(f, L(getattr(g, 'run_before', None))): return False\n",
        "    return True\n",
        "\n",
        "def sort_by_run(fs):\n",
        "    end = L(fs).attrgot('toward_end')\n",
        "    inp,res = L(fs)[~end] + L(fs)[end], L()\n",
        "    while len(inp):\n",
        "        for i,o in enumerate(inp):\n",
        "            if _is_first(o, inp):\n",
        "                res.append(inp.pop(i))\n",
        "                break\n",
        "        else: raise Exception(\"Impossible to sort\")\n",
        "    # TRACE: print(f\"will invoke toward_end {res}\")\n",
        "    return res\n",
        "\n",
        "\n",
        "print(\"---------------------- PASSED LEARNER\")\n",
        "\n",
        "\n",
        "\n",
        "##################################################################33 torch_base.py\n",
        "\n",
        "\n",
        "\n",
        "print(\"---------------------- STARTED torch_base.py\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    if torch.cuda.current_device()==0:\n",
        "        def_gpu = int(os.environ.get('DEFAULT_GPU') or 0)\n",
        "        if torch.cuda.device_count()>=def_gpu: torch.cuda.set_device(def_gpu)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Cell\n",
        "@delegates(plt.subplots, keep=True)\n",
        "def subplots(nrows=1, ncols=1, figsize=None, imsize=3, add_vert=0, **kwargs):\n",
        "    if figsize is None: figsize=(ncols*imsize, nrows*imsize+add_vert)\n",
        "    fig,ax = plt.subplots(nrows, ncols, figsize=figsize, **kwargs)\n",
        "    if nrows*ncols==1: ax = array([ax])\n",
        "    return fig,ax\n",
        "\n",
        "# Cell\n",
        "def _fig_bounds(x):\n",
        "    r = x//32\n",
        "    return min(5, max(1,r))\n",
        "\n",
        "# Cell\n",
        "def show_image(im, ax=None, figsize=None, title=None, ctx=None, **kwargs):\n",
        "    \"Show a PIL or PyTorch image on `ax`.\"\n",
        "    # Handle pytorch axis order\n",
        "    if hasattrs(im, ('data','cpu','permute')):\n",
        "        im = im.data.cpu()\n",
        "        if im.shape[0]<5: im=im.permute(1,2,0)\n",
        "    elif not isinstance(im,np.ndarray): im=array(im)\n",
        "    # Handle 1-channel images\n",
        "    if im.shape[-1]==1: im=im[...,0]\n",
        "\n",
        "    ax = ifnone(ax,ctx)\n",
        "    if figsize is None: figsize = (_fig_bounds(im.shape[0]), _fig_bounds(im.shape[1]))\n",
        "    if ax is None: _,ax = plt.subplots(figsize=figsize)\n",
        "    ax.imshow(im, **kwargs)\n",
        "    if title is not None: ax.set_title(title)\n",
        "    ax.axis('off')\n",
        "    return ax\n",
        "\n",
        "# Cell\n",
        "def show_titled_image(o, **kwargs):\n",
        "    \"Call `show_image` destructuring `o` to `(img,title)`\"\n",
        "    show_image(o[0], title=str(o[1]), **kwargs)\n",
        "\n",
        "# Cell\n",
        "@delegates(subplots)\n",
        "def show_images(ims, nrows=1, ncols=None, titles=None, **kwargs):\n",
        "    \"Show all images `ims` as subplots with `rows` using `titles`\"\n",
        "    if ncols is None: ncols = int(math.ceil(len(ims)/nrows))\n",
        "    if titles is None: titles = [None]*len(ims)\n",
        "    axs = subplots(nrows, ncols, **kwargs)[1].flat\n",
        "    for im,t,ax in zip(ims, titles, axs): show_image(im, ax=ax, title=t)\n",
        "\n",
        "# Cell\n",
        "class ArrayBase(ndarray):\n",
        "    @classmethod\n",
        "    def _before_cast(cls, x): return x if isinstance(x,ndarray) else array(x)\n",
        "\n",
        "# Cell\n",
        "class ArrayImageBase(ArrayBase):\n",
        "    _show_args = {'cmap':'viridis'}\n",
        "    def show(self, ctx=None, **kwargs):\n",
        "        return show_image(self, ctx=ctx, **{**self._show_args, **kwargs})\n",
        "\n",
        "# Cell\n",
        "class ArrayImage(ArrayImageBase): pass\n",
        "\n",
        "# Cell\n",
        "class ArrayImageBW(ArrayImage): _show_args = {'cmap':'Greys'}\n",
        "\n",
        "# Cell\n",
        "class ArrayMask(ArrayImageBase): _show_args = {'alpha':0.5, 'cmap':'tab20', 'interpolation':'nearest'}\n",
        "\n",
        "# Cell\n",
        "@patch\n",
        "def __array_eq__(self:Tensor,b):\n",
        "    return torch.equal(self,b) if self.dim() else self==b\n",
        "\n",
        "# Cell\n",
        "def _array2tensor(x):\n",
        "    if x.dtype==np.uint16: x = x.astype(np.float32)\n",
        "    return torch.from_numpy(x)\n",
        "\n",
        "# Cell\n",
        "def tensor(x, *rest, **kwargs):\n",
        "    \"Like `torch.as_tensor`, but handle lists too, and can pass multiple vector elements directly.\"\n",
        "    if len(rest): x = (x,)+rest\n",
        "    # There was a Pytorch bug in dataloader using num_workers>0. Haven't confirmed if fixed\n",
        "    # if isinstance(x, (tuple,list)) and len(x)==0: return tensor(0)\n",
        "    res = (x if isinstance(x, Tensor)\n",
        "           else torch.tensor(x, **kwargs) if isinstance(x, (tuple,list))\n",
        "           else _array2tensor(x) if isinstance(x, ndarray)\n",
        "           else as_tensor(x.values, **kwargs) if isinstance(x, (pd.Series, pd.DataFrame))\n",
        "           else as_tensor(x, **kwargs) if hasattr(x, '__array__') or is_iter(x)\n",
        "           else _array2tensor(array(x), **kwargs))\n",
        "    if res.dtype is torch.float64: return res.float()\n",
        "    return res\n",
        "\n",
        "# Cell\n",
        "def set_seed(s):\n",
        "    \"Set random seed for `random`, `torch`, and `numpy` (where available)\"\n",
        "    try: torch.manual_seed(s)\n",
        "    except NameError: pass\n",
        "    try: np.random.seed(s%(2**32-1))\n",
        "    except NameError: pass\n",
        "    random.seed(s)\n",
        "\n",
        "# Cell\n",
        "def unsqueeze(x, dim=-1, n=1):\n",
        "    \"Same as `torch.unsqueeze` but can add `n` dims\"\n",
        "    for _ in range(n): x = x.unsqueeze(dim)\n",
        "    return x\n",
        "\n",
        "# Cell\n",
        "def unsqueeze_(x, dim=-1, n=1):\n",
        "    \"Same as `torch.unsqueeze_` but can add `n` dims\"\n",
        "    for _ in range(n): x.unsqueeze_(dim)\n",
        "    return x\n",
        "\n",
        "# Cell\n",
        "def _fa_rebuild_tensor (cls, *args, **kwargs): return cls(torch._utils._rebuild_tensor_v2(*args, **kwargs))\n",
        "def _fa_rebuild_qtensor(cls, *args, **kwargs): return cls(torch._utils._rebuild_qtensor  (*args, **kwargs))\n",
        "\n",
        "# Cell\n",
        "def apply(func, x, *args, **kwargs):\n",
        "    \"Apply `func` recursively to `x`, passing on args\"\n",
        "    if is_listy(x): return type(x)([apply(func, o, *args, **kwargs) for o in x])\n",
        "    if isinstance(x,dict):  return {k: apply(func, v, *args, **kwargs) for k,v in x.items()}\n",
        "    res = func(x, *args, **kwargs)\n",
        "    return res if x is None else retain_type(res, x)\n",
        "\n",
        "# Cell\n",
        "def maybe_gather(x, axis=0):\n",
        "    \"Gather copies of `x` on `axis` (if training is distributed)\"\n",
        "    if num_distrib()<=1: return x\n",
        "    ndim = x.ndim\n",
        "    res = [x.new_zeros(*x.shape if ndim > 0 else (1,)) for _ in range(num_distrib())]\n",
        "    torch.distributed.all_gather(res, x if ndim > 0 else x[None])\n",
        "    return torch.cat(res, dim=axis) if ndim > 0 else torch.cat(res, dim=axis).mean()\n",
        "\n",
        "# Cell\n",
        "def to_detach(b, cpu=True, gather=True):\n",
        "    \"Recursively detach lists of tensors in `b `; put them on the CPU if `cpu=True`.\"\n",
        "    def _inner(x, cpu=True, gather=True):\n",
        "        if not isinstance(x,Tensor): return x\n",
        "        x = x.detach()\n",
        "        if gather: x = maybe_gather(x)\n",
        "        return x.cpu() if cpu else x\n",
        "    return apply(_inner, b, cpu=cpu, gather=gather)\n",
        "\n",
        "# Cell\n",
        "def to_half(b):\n",
        "    \"Recursively map lists of tensors in `b ` to FP16.\"\n",
        "    return apply(lambda x: x.half() if torch.is_floating_point(x) else x, b)\n",
        "\n",
        "# Cell\n",
        "def to_float(b):\n",
        "    \"Recursively map lists of int tensors in `b ` to float.\"\n",
        "    return apply(lambda x: x.float() if torch.is_floating_point(x) else x, b)\n",
        "\n",
        "# Cell\n",
        "# None: True if available; True: error if not availabe; False: use CPU\n",
        "defaults.use_cuda = None\n",
        "\n",
        "# Cell\n",
        "def default_device(use_cuda=-1):\n",
        "    \"Return or set default device; `use_cuda`: None - CUDA if available; True - error if not availabe; False - CPU\"\n",
        "    if use_cuda != -1: defaults.use_cuda=use_cuda\n",
        "    use = defaults.use_cuda or (torch.cuda.is_available() and defaults.use_cuda is None)\n",
        "    assert torch.cuda.is_available() or not use\n",
        "    return torch.device(torch.cuda.current_device()) if use else torch.device('cpu')\n",
        "\n",
        "# Cell\n",
        "def to_device(b, device=None):\n",
        "    \"Recursively put `b` on `device`.\"\n",
        "    if defaults.use_cuda==False: device='cpu'\n",
        "    elif device is None: device=default_device()\n",
        "    def _inner(o): return o.to(device, non_blocking=True) if isinstance(o,Tensor) else o.to_device(device) if hasattr(o, \"to_device\") else o\n",
        "    return apply(_inner, b)\n",
        "\n",
        "# Cell\n",
        "def to_cpu(b):\n",
        "    \"Recursively map lists of tensors in `b ` to the cpu.\"\n",
        "    return to_device(b,'cpu')\n",
        "\n",
        "# Cell\n",
        "def to_np(x):\n",
        "    \"Convert a tensor to a numpy array.\"\n",
        "    return apply(lambda o: o.data.cpu().numpy(), x)\n",
        "\n",
        "# Cell\n",
        "def to_concat(xs, dim=0):\n",
        "    \"Concat the element in `xs` (recursively if they are tuples/lists of tensors)\"\n",
        "    if is_listy(xs[0]): return type(xs[0])([to_concat([x[i] for x in xs], dim=dim) for i in range_of(xs[0])])\n",
        "    if isinstance(xs[0],dict):  return {k: to_concat([x[k] for x in xs], dim=dim) for k in xs[0].keys()}\n",
        "    #We may receives xs that are not concatenatable (inputs of a text classifier for instance),\n",
        "    #   in this case we return a big list\n",
        "    try:    return retain_type(torch.cat(xs, dim=dim), xs[0])\n",
        "    except: return sum([L(retain_type(o_.index_select(dim, tensor(i)).squeeze(dim), xs[0])\n",
        "                          for i in range_of(o_)) for o_ in xs], L())\n",
        "\n",
        "# Cell\n",
        "@patch\n",
        "def set_meta(self:Tensor, x):\n",
        "    \"Set all metadata in `__dict__`\"\n",
        "    if hasattr(x,'__dict__'): self.__dict__ = x.__dict__\n",
        "\n",
        "\n",
        "# Cell\n",
        "@patch\n",
        "def get_meta(self:Tensor, n, d=None):\n",
        "    \"Set `n` from `self._meta` if it exists and returns default `d` otherwise\"\n",
        "    return getattr(self, '_meta', {}).get(n, d)\n",
        "\n",
        "# Cell\n",
        "@patch\n",
        "def as_subclass(self:Tensor, typ):\n",
        "    \"Cast to `typ` (should be in future PyTorch version, so remove this then)\"\n",
        "    res = torch.Tensor._make_subclass(typ, self)\n",
        "    return retain_meta(self, res)\n",
        "\n",
        "# Cell\n",
        "class TensorBase(Tensor):\n",
        "    def __new__(cls, x, **kwargs):\n",
        "        global printed_device2 \n",
        "        res = cast(tensor(x), cls)\n",
        "        if res.device != tpu_device:\n",
        "          print(f\"************** XXX ######################### EL DEVICE ES {tpu_device} no es {res.device} ni el de entrada {x.device} ######################### **************\") if printed_device < 10 else noop\n",
        "          printed_device2 += 1\n",
        "        res._meta = kwargs\n",
        "        return res\n",
        "\n",
        "    @classmethod\n",
        "    def _before_cast(cls, x): return x if isinstance(x,Tensor) else tensor(x)\n",
        "\n",
        "    def __reduce_ex__(self,proto):\n",
        "        torch.utils.hooks.warn_if_has_hooks(self)\n",
        "        args = (type(self), self.storage(), self.storage_offset(), tuple(self.size()), self.stride())\n",
        "        if self.is_quantized: args = args + (self.q_scale(), self.q_zero_point())\n",
        "        f = _fa_rebuild_qtensor if self.is_quantized else  _fa_rebuild_tensor\n",
        "        return (f, args + (self.requires_grad, OrderedDict()))\n",
        "\n",
        "    def gi(self, i):\n",
        "        res = self[i]\n",
        "        return res.as_subclass(type(self)) if isinstance(res,Tensor) else res\n",
        "\n",
        "    def __repr__(self):\n",
        "        return re.sub('tensor', self.__class__.__name__, super().__repr__())\n",
        "\n",
        "# Cell\n",
        "def _patch_tb():\n",
        "    # TRACE: print(\"*********################### ######################## patch_tb\")\n",
        "    if getattr(TensorBase,'_patched',False): return\n",
        "    TensorBase._patched = True\n",
        "\n",
        "    def get_f(fn):\n",
        "        def _f(self, *args, **kwargs):\n",
        "            cls = self.__class__\n",
        "            res = getattr(super(TensorBase, self), fn)(*args, **kwargs)\n",
        "            return retain_type(res, self)\n",
        "        return _f\n",
        "\n",
        "    t = tensor([1])\n",
        "    skips = 'as_subclass imag real __getitem__ __class__ __deepcopy__ __delattr__ __dir__ __doc__ __getattribute__ __hash__ __init__ \\\n",
        "        __init_subclass__ __new__ __reduce__ __reduce_ex__ __repr__ __module__ __setstate__'.split()\n",
        "\n",
        "    for fn in dir(t):\n",
        "        if fn in skips: continue\n",
        "        f = getattr(t, fn)\n",
        "        if isinstance(f, (MethodWrapperType, BuiltinFunctionType, BuiltinMethodType, MethodType, FunctionType)):\n",
        "            setattr(TensorBase, fn, get_f(fn))\n",
        "\n",
        "_patch_tb()\n",
        "\n",
        "# Cell\n",
        "class TensorCategory(TensorBase): pass\n",
        "\n",
        "# Cell\n",
        "class TensorMultiCategory(TensorCategory): pass\n",
        "\n",
        "# Cell\n",
        "class TensorImageBase(TensorBase):\n",
        "    _show_args = ArrayImageBase._show_args\n",
        "    def show(self, ctx=None, **kwargs):\n",
        "        return show_image(self, ctx=ctx, **{**self._show_args, **kwargs})\n",
        "\n",
        "# Cell\n",
        "class TensorImage(TensorImageBase): pass\n",
        "\n",
        "# Cell\n",
        "class TensorImageBW(TensorImage): _show_args = ArrayImageBW._show_args\n",
        "\n",
        "# Cell\n",
        "class TensorMask(TensorImageBase):\n",
        "    _show_args = ArrayMask._show_args\n",
        "\n",
        "    def show(self, ctx=None, **kwargs):\n",
        "        codes = self.get_meta('codes')\n",
        "        if codes is not None: kwargs = merge({'vmin': 1, 'vmax': len(codes)}, kwargs)\n",
        "        return super().show(ctx=ctx, **kwargs)\n",
        "\n",
        "# Cell\n",
        "class TitledTensorScalar(TensorBase):\n",
        "    \"A tensor containing a scalar that has a `show` method\"\n",
        "    def show(self, **kwargs): show_title(self.item(), **kwargs)\n",
        "\n",
        "# Cell\n",
        "@patch\n",
        "def tensored(self:L):\n",
        "    \"`mapped(tensor)`\"\n",
        "    return self.map(tensor)\n",
        "@patch\n",
        "def stack(self:L, dim=0):\n",
        "    \"Same as `torch.stack`\"\n",
        "    return torch.stack(list(self.tensored()), dim=dim)\n",
        "@patch\n",
        "def cat  (self:L, dim=0):\n",
        "    \"Same as `torch.cat`\"\n",
        "    return torch.cat  (list(self.tensored()), dim=dim)\n",
        "\n",
        "# Cell\n",
        "def concat(*ls):\n",
        "    \"Concatenate tensors, arrays, lists, or tuples\"\n",
        "    if not len(ls): return []\n",
        "    it = ls[0]\n",
        "    if isinstance(it,torch.Tensor): res = torch.cat(ls)\n",
        "    elif isinstance(it,ndarray): res = np.concatenate(ls)\n",
        "    else:\n",
        "        res = itertools.chain.from_iterable(map(L,ls))\n",
        "        if isinstance(it,(tuple,list)): res = type(it)(res)\n",
        "        else: res = L(res)\n",
        "    return retain_type(res, it)\n",
        "\n",
        "# Cell\n",
        "class Chunks:\n",
        "    \"Slice and int indexing into a list of lists\"\n",
        "    def __init__(self, chunks, lens=None):\n",
        "        self.chunks = chunks\n",
        "        self.lens = L(map(len,self.chunks) if lens is None else lens)\n",
        "        self.cumlens = np.cumsum(0+self.lens)\n",
        "        self.totlen = self.cumlens[-1]\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        if isinstance(i,slice): return retain_type(self.getslice(i), old=self.chunks[0])\n",
        "        di,idx = self.doc_idx(i)\n",
        "        return retain_type(self.chunks[di][idx], old=self.chunks[0])\n",
        "\n",
        "    def getslice(self, i):\n",
        "        st_d,st_i = self.doc_idx(ifnone(i.start,0))\n",
        "        en_d,en_i = self.doc_idx(ifnone(i.stop,self.totlen+1))\n",
        "        res = [self.chunks[st_d][st_i:(en_i if st_d==en_d else sys.maxsize)]]\n",
        "        for b in range(st_d+1,en_d): res.append(self.chunks[b])\n",
        "        if st_d!=en_d and en_d<len(self.chunks): res.append(self.chunks[en_d][:en_i])\n",
        "        return concat(*res)\n",
        "\n",
        "    def doc_idx(self, i):\n",
        "        if i<0: i=self.totlen+i # count from end\n",
        "        docidx = np.searchsorted(self.cumlens, i+1)-1\n",
        "        cl = self.cumlens[docidx]\n",
        "        return docidx,i-cl\n",
        "\n",
        "# Cell\n",
        "def show_title(o, ax=None, ctx=None, label=None, color='black', **kwargs):\n",
        "    \"Set title of `ax` to `o`, or print `o` if `ax` is `None`\"\n",
        "    ax = ifnone(ax,ctx)\n",
        "    if ax is None: print(o)\n",
        "    elif hasattr(ax, 'set_title'):\n",
        "        t = ax.title.get_text()\n",
        "        if len(t) > 0: o = t+'\\n'+str(o)\n",
        "        ax.set_title(o, color=color)\n",
        "    elif isinstance(ax, pd.Series):\n",
        "        while label in ax: label += '_'\n",
        "        ax = ax.append(pd.Series({label: o}))\n",
        "    return ax\n",
        "\n",
        "# Cell\n",
        "class ShowTitle:\n",
        "    \"Base class that adds a simple `show`\"\n",
        "    _show_args = {'label': 'text'}\n",
        "    def show(self, ctx=None, **kwargs):\n",
        "        \"Show self\"\n",
        "        return show_title(str(self), ctx=ctx, **merge(self._show_args, kwargs))\n",
        "\n",
        "class TitledInt(Int, ShowTitle):\n",
        "    _show_args = {'label': 'text'}\n",
        "    def show(self, ctx=None, **kwargs):\n",
        "        \"Show self\"\n",
        "        return show_title(str(self), ctx=ctx, **merge(self._show_args, kwargs))\n",
        "\n",
        "class TitledFloat(Float, ShowTitle):\n",
        "    _show_args = {'label': 'text'}\n",
        "    def show(self, ctx=None, **kwargs):\n",
        "        \"Show self\"\n",
        "        return show_title(str(self), ctx=ctx, **merge(self._show_args, kwargs))\n",
        "\n",
        "class TitledStr(Str, ShowTitle):\n",
        "    _show_args = {'label': 'text'}\n",
        "    def show(self, ctx=None, **kwargs):\n",
        "        \"Show self\"\n",
        "        return show_title(str(self), ctx=ctx, **merge(self._show_args, kwargs))\n",
        "\n",
        "class TitledTuple(Tuple, ShowTitle):\n",
        "    _show_args = {'label': 'text'}\n",
        "    def show(self, ctx=None, **kwargs):\n",
        "        \"Show self\"\n",
        "        return show_title(str(self), ctx=ctx, **merge(self._show_args, kwargs))\n",
        "\n",
        "add_docs(TitledInt, \"An `int` with `show`\"); add_docs(TitledStr, \"An `str` with `show`\");\n",
        "add_docs(TitledFloat, \"A `float` with `show`\"); add_docs(TitledTuple, \"A `Tuple` with `show`\")\n",
        "\n",
        "# Cell\n",
        "@patch\n",
        "def truncate(self:TitledStr, n):\n",
        "    \"Truncate self to `n`\"\n",
        "    words = self.split(' ')[:n]\n",
        "    return TitledStr(' '.join(words))\n",
        "\n",
        "# Cell\n",
        "if not hasattr(pd.DataFrame,'_old_init'): pd.DataFrame._old_init = pd.DataFrame.__init__\n",
        "\n",
        "# Cell\n",
        "@patch\n",
        "def __init__(self:pd.DataFrame, data=None, index=None, columns=None, dtype=None, copy=False):\n",
        "    if data is not None and isinstance(data, Tensor): data = to_np(data)\n",
        "    self._old_init(data, index=index, columns=columns, dtype=dtype, copy=copy)\n",
        "\n",
        "# Cell\n",
        "def get_empty_df(n):\n",
        "    \"Return `n` empty rows of a dataframe\"\n",
        "    df = pd.DataFrame(index = range(n))\n",
        "    return [df.iloc[i] for i in range(n)]\n",
        "\n",
        "# Cell\n",
        "def display_df(df):\n",
        "    \"Display `df` in a notebook or defaults to print\"\n",
        "    try: from IPython.display import display, HTML\n",
        "    except: return print(df)\n",
        "    display(HTML(df.to_html()))\n",
        "\n",
        "# Cell\n",
        "def get_first(c):\n",
        "    \"Get the first element of c, even if c is a dataframe\"\n",
        "    return getattr(c, 'iloc', c)[0]\n",
        "\n",
        "# Cell\n",
        "def one_param(m):\n",
        "    \"First parameter in `m`\"\n",
        "    return first(m.parameters())\n",
        "\n",
        "# Cell\n",
        "def item_find(x, idx=0):\n",
        "    \"Recursively takes the `idx`-th element of `x`\"\n",
        "    if is_listy(x): return item_find(x[idx])\n",
        "    if isinstance(x,dict):\n",
        "        key = list(x.keys())[idx] if isinstance(idx, int) else idx\n",
        "        return item_find(x[key])\n",
        "    return x\n",
        "\n",
        "# Cell\n",
        "def find_device(b):\n",
        "    \"Recursively search the device of `b`.\"\n",
        "    return item_find(b).device\n",
        "\n",
        "# Cell\n",
        "def find_bs(b):\n",
        "    \"Recursively search the batch size of `b`.\"\n",
        "    return item_find(b).shape[0]\n",
        "\n",
        "# Cell\n",
        "class Module(nn.Module, metaclass=PrePostInitMeta):\n",
        "    \"Same as `nn.Module`, but no need for subclasses to call `super().__init__`\"\n",
        "    def __pre_init__(self, *args, **kwargs): super().__init__()\n",
        "    def __init__(self): pass\n",
        "\n",
        "# Cell\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "def get_model(model):\n",
        "    \"Return the model maybe wrapped inside `model`.\"\n",
        "    return model.module if isinstance(model, (DistributedDataParallel, nn.DataParallel)) else model\n",
        "\n",
        "# Cell\n",
        "def one_hot(x, c):\n",
        "    \"One-hot encode `x` with `c` classes.\"\n",
        "    res = torch.zeros(c, dtype=torch.uint8)\n",
        "    if isinstance(x, Tensor) and x.numel()>0: res[x] = 1.\n",
        "    else: res[list(L(x, use_list=None))] = 1.\n",
        "    return res\n",
        "\n",
        "# Cell\n",
        "def one_hot_decode(x, vocab=None):\n",
        "    return L(vocab[i] if vocab else i for i,x_ in enumerate(x) if x_==1)\n",
        "\n",
        "# Cell\n",
        "def params(m):\n",
        "    \"Return all parameters of `m`\"\n",
        "    return [p for p in m.parameters()]\n",
        "\n",
        "# Cell\n",
        "def trainable_params(m):\n",
        "    \"Return all trainable parameters of `m`\"\n",
        "    return [p for p in m.parameters() if p.requires_grad]\n",
        "\n",
        "# Cell\n",
        "norm_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d)\n",
        "\n",
        "# Cell\n",
        "def bn_bias_params(m, with_bias=True): # TODO: Rename to `norm_bias_params`\n",
        "    \"Return all bias and BatchNorm parameters\"\n",
        "    if isinstance(m, norm_types): return L(m.parameters())\n",
        "    res = L(m.children()).map(bn_bias_params, with_bias=with_bias).concat()\n",
        "    if with_bias and getattr(m, 'bias', None) is not None: res.append(m.bias)\n",
        "    return res\n",
        "\n",
        "# Cell\n",
        "def batch_to_samples(b, max_n=10):\n",
        "    \"'Transposes' a batch to (at most `max_n`) samples\"\n",
        "    if isinstance(b, Tensor): return retain_types(list(b[:max_n]), [b])\n",
        "    else:\n",
        "        res = L(b).map(partial(batch_to_samples,max_n=max_n))\n",
        "        return retain_types(res.zip(), [b])\n",
        "\n",
        "# Cell\n",
        "@patch\n",
        "def interp_1d(x:Tensor, xp, fp):\n",
        "    \"Same as `np.interp`\"\n",
        "    slopes = (fp[1:]-fp[:-1])/(xp[1:]-xp[:-1])\n",
        "    incx = fp[:-1] - (slopes*xp[:-1])\n",
        "    locs = (x[:,None]>=xp[None,:]).long().sum(1)-1\n",
        "    locs = locs.clamp(0,len(slopes)-1)\n",
        "    return slopes[locs]*x + incx[locs]\n",
        "\n",
        "# Cell\n",
        "@patch\n",
        "def pca(x:Tensor, k=2):\n",
        "    \"Compute PCA of `x` with `k` dimensions.\"\n",
        "    x = x-torch.mean(x,0)\n",
        "    U,S,V = torch.svd(x.t())\n",
        "    return torch.mm(x,U[:,:k])\n",
        "\n",
        "# Cell\n",
        "def logit(x):\n",
        "    \"Logit of `x`, clamped to avoid inf.\"\n",
        "    x = x.clamp(1e-7, 1-1e-7)\n",
        "    return -(1/x-1).log()\n",
        "\n",
        "# Cell\n",
        "def num_distrib():\n",
        "    \"Return the number of processes in distributed training (if applicable).\"\n",
        "    return int(os.environ.get('WORLD_SIZE', 0))\n",
        "\n",
        "# Cell\n",
        "def rank_distrib():\n",
        "    \"Return the distributed rank of this process (if applicable).\"\n",
        "    return int(os.environ.get('RANK', 0))\n",
        "\n",
        "# Cell\n",
        "def distrib_barrier():\n",
        "    \"Place a synchronization barrier in distributed training so that ALL sub-processes in the pytorch process group must arrive here before proceeding.\"\n",
        "    if num_distrib() > 1 and torch.distributed.is_initialized(): torch.distributed.barrier()\n",
        "\n",
        "# Cell\n",
        "# Saving arrays requires pytables - optional dependency\n",
        "try: import tables\n",
        "except: pass\n",
        "\n",
        "# Cell\n",
        "def _comp_filter(lib='lz4',lvl=3): return tables.Filters(complib=f'blosc:{lib}', complevel=lvl)\n",
        "\n",
        "# Cell\n",
        "@patch\n",
        "def save_array(p:Path, o, complib='lz4', lvl=3):\n",
        "    \"Save numpy array to a compressed `pytables` file, using compression level `lvl`\"\n",
        "    if isinstance(o,Tensor): o = to_np(o)\n",
        "    with tables.open_file(p, mode='w', filters=_comp_filter(lib=complib,lvl=lvl)) as f: f.create_carray('/', 'data', obj=o)\n",
        "\n",
        "# Cell\n",
        "@patch\n",
        "def load_array(p:Path):\n",
        "    \"Save numpy array to a `pytables` file\"\n",
        "    with tables.open_file(p, 'r') as f: return f.root.data.read()\n",
        "\n",
        "# Cell\n",
        "def base_doc(elt):\n",
        "    \"Print a base documentation of `elt`\"\n",
        "    name = getattr(elt, '__qualname__', getattr(elt, '__name__', ''))\n",
        "    print(f'{name}{inspect.signature(elt)}\\n{inspect.getdoc(elt)}\\n')\n",
        "    print('To get a prettier result with hyperlinks to source code and documentation, install nbdev: pip install nbdev')\n",
        "\n",
        "# Cell\n",
        "def doc(elt):\n",
        "    \"Try to use doc form nbdev and fall back to `base_doc`\"\n",
        "    try:\n",
        "        from nbdev.showdoc import doc\n",
        "        doc(elt)\n",
        "    except: base_doc(elt)\n",
        "\n",
        "# Cell\n",
        "def nested_reorder(t, idxs):\n",
        "    \"Reorder all tensors in `t` using `idxs`\"\n",
        "    if isinstance(t, (Tensor,L)): return t[idxs]\n",
        "    elif is_listy(t): return type(t)(nested_reorder(t_, idxs) for t_ in t)\n",
        "    if t is None: return t\n",
        "    raise TypeError(f\"Expected tensor, tuple, list or L but got {type(t)}\")\n",
        "\n",
        "# Cell\n",
        "def to_image(x):\n",
        "    if isinstance(x,Image.Image): return x\n",
        "    if isinstance(x,Tensor): x = to_np(x.permute((1,2,0)))\n",
        "    if x.dtype==np.float32: x = (x*255).astype(np.uint8)\n",
        "    return Image.fromarray(x, mode=['RGB','CMYK'][x.shape[0]==4])\n",
        "\n",
        "# Cell\n",
        "def make_cross_image(bw=True):\n",
        "    \"Create a tensor containing a cross image, either `bw` (True) or color\"\n",
        "    if bw:\n",
        "        im = torch.zeros(5,5)\n",
        "        im[2,:] = 1.\n",
        "        im[:,2] = 1.\n",
        "    else:\n",
        "        im = torch.zeros(3,5,5)\n",
        "        im[0,2,:] = 1.\n",
        "        im[1,:,2] = 1.\n",
        "    return im\n",
        "\n",
        "# Cell\n",
        "def show_image_batch(b, show=show_titled_image, items=9, cols=3, figsize=None, **kwargs):\n",
        "    \"Display batch `b` in a grid of size `items` with `cols` width\"\n",
        "    if items<cols: cols=items\n",
        "    rows = (items+cols-1) // cols\n",
        "    if figsize is None: figsize = (cols*3, rows*3)\n",
        "    fig,axs = plt.subplots(rows, cols, figsize=figsize)\n",
        "    for *o,ax in zip(*to_cpu(b), axs.flatten()): show(o, ax=ax, **kwargs)\n",
        "\n",
        "# Cell\n",
        "def requires_grad(m):\n",
        "    \"Check if the first parameter of `m` requires grad or not\"\n",
        "    ps = list(m.parameters())\n",
        "    return ps[0].requires_grad if len(ps)>0 else False\n",
        "\n",
        "# Cell\n",
        "def init_default(m, func=nn.init.kaiming_normal_):\n",
        "    \"Initialize `m` weights with `func` and set `bias` to 0.\"\n",
        "    if func:\n",
        "        if hasattr(m, 'weight'): func(m.weight)\n",
        "        if hasattr(m, 'bias') and hasattr(m.bias, 'data'): m.bias.data.fill_(0.)\n",
        "    return m\n",
        "\n",
        "# Cell\n",
        "def cond_init(m, func):\n",
        "    \"Apply `init_default` to `m` unless it's a batchnorm module\"\n",
        "    if (not isinstance(m, norm_types)) and requires_grad(m): init_default(m, func)\n",
        "\n",
        "# Cell\n",
        "def apply_leaf(m, f):\n",
        "    \"Apply `f` to children of `m`.\"\n",
        "    c = m.children()\n",
        "    if isinstance(m, nn.Module): f(m)\n",
        "    for l in c: apply_leaf(l,f)\n",
        "\n",
        "# Cell\n",
        "def apply_init(m, func=nn.init.kaiming_normal_):\n",
        "    \"Initialize all non-batchnorm layers of `m` with `func`.\"\n",
        "    apply_leaf(m, partial(cond_init, func=func))\n",
        "\n",
        "# Cell\n",
        "from multiprocessing import Process, Queue\n",
        "\n",
        "# Cell\n",
        "def set_num_threads(nt):\n",
        "    \"Get numpy (and others) to use `nt` threads\"\n",
        "    try: import mkl; mkl.set_num_threads(nt)\n",
        "    except: pass\n",
        "    torch.set_num_threads(1)\n",
        "    os.environ['IPC_ENABLE']='1'\n",
        "    for o in ['OPENBLAS_NUM_THREADS','NUMEXPR_NUM_THREADS','OMP_NUM_THREADS','MKL_NUM_THREADS']:\n",
        "        os.environ[o] = str(nt)\n",
        "\n",
        "# Cell\n",
        "@delegates(concurrent.futures.ProcessPoolExecutor)\n",
        "class ProcessPoolExecutor(concurrent.futures.ProcessPoolExecutor):\n",
        "    def __init__(self, max_workers=None, on_exc=print, **kwargs):\n",
        "        self.not_parallel = max_workers==0\n",
        "        self.on_exc = on_exc\n",
        "        if self.not_parallel: max_workers=1\n",
        "        super().__init__(max_workers, **kwargs)\n",
        "\n",
        "    def map(self, f, items, *args, **kwargs):\n",
        "        g = partial(f, *args, **kwargs)\n",
        "        if self.not_parallel: return map(g, items)\n",
        "        try: return super().map(g, items)\n",
        "        except Exception as e: self.on_exc(e)\n",
        "\n",
        "# Cell\n",
        "def parallel(f, items, *args, n_workers=defaults.cpus, total=None, progress=True, **kwargs):\n",
        "    \"Applies `func` in parallel to `items`, using `n_workers`\"\n",
        "    with ProcessPoolExecutor(n_workers) as ex:\n",
        "        r = ex.map(f,items, *args, **kwargs)\n",
        "        if progress:\n",
        "            if total is None: total = len(items)\n",
        "            r = progress_bar(r, total=total, leave=False)\n",
        "        return L(r)\n",
        "\n",
        "# Cell\n",
        "def run_procs(f, f_done, args):\n",
        "    \"Call `f` for each item in `args` in parallel, yielding `f_done`\"\n",
        "    processes = L(args).map(Process, args=arg0, target=f)\n",
        "    for o in processes: o.start()\n",
        "    try: yield from f_done()\n",
        "    except Exception as e: print(e)\n",
        "    finally: processes.map(Self.join())\n",
        "\n",
        "# Cell\n",
        "def parallel_gen(cls, items, n_workers=defaults.cpus, as_gen=False, **kwargs):\n",
        "    \"Instantiate `cls` in `n_workers` procs & call each on a subset of `items` in parallel.\"\n",
        "    batches = np.array_split(items, n_workers)\n",
        "    idx = np.cumsum(0 + L(batches).map(len))\n",
        "    queue = Queue()\n",
        "    def f(batch, start_idx):\n",
        "        for i,b in enumerate(cls(**kwargs)(batch)): queue.put((start_idx+i,b))\n",
        "    def done(): return (queue.get() for _ in progress_bar(items, leave=False))\n",
        "    yield from run_procs(f, done, L(batches,idx).zip())\n",
        "\n",
        "# Cell\n",
        "def script_use_ctx(f):\n",
        "    \"Decorator: create jit script and pass everything in `ctx.saved_variables to `f`, after `*args`\"\n",
        "    sf = torch.jit.script(f)\n",
        "    def _f(ctx, *args, **kwargs): return sf(*args, *ctx.saved_variables, **kwargs)\n",
        "    return update_wrapper(_f,f)\n",
        "\n",
        "# Cell\n",
        "def script_save_ctx(static, *argidx):\n",
        "    \"Decorator: create jit script and save args with indices `argidx` using `ctx.save_for_backward`\"\n",
        "    def _dec(f):\n",
        "        sf = torch.jit.script(f)\n",
        "        def _f(ctx, *args, **kwargs):\n",
        "            if argidx:\n",
        "                save = [args[o] for o in argidx]\n",
        "                ctx.save_for_backward(*save)\n",
        "            if not argidx: args = [ctx]+args\n",
        "            return sf(*args, **kwargs)\n",
        "        if static: _f = staticmethod(_f)\n",
        "        return update_wrapper(_f,f)\n",
        "    return _dec\n",
        "\n",
        "# Cell\n",
        "def script_fwd(*argidx):\n",
        "    \"Decorator: create static jit script and save args with indices `argidx` using `ctx.save_for_backward`\"\n",
        "    return script_save_ctx(True, *argidx)\n",
        "\n",
        "# Cell\n",
        "def script_bwd(f):\n",
        "    \"Decorator: create static jit script and pass everything in `ctx.saved_variables to `f`, after `*args`\"\n",
        "    return staticmethod(script_use_ctx(f))\n",
        "\n",
        "# Cell\n",
        "def grad_module(cls):\n",
        "    \"Decorator: convert `cls` into an autograd function\"\n",
        "    class _c(nn.Module):\n",
        "        def forward(self, *args, **kwargs): return cls.apply(*args, **kwargs)\n",
        "    return _c\n",
        "\n",
        "# Comes from 13b_metrics.ipynb, cell\n",
        "def flatten_check(inp, targ):\n",
        "    \"Check that `out` and `targ` have the same number of elements and flatten them.\"\n",
        "    inp,targ = inp.contiguous().view(-1),targ.contiguous().view(-1)\n",
        "    test_eq(len(inp), len(targ))\n",
        "    return inp,targ\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"---------------------- PASSED torch_base.py\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"---------------------------------------------------------- ALMOOOOST END\")\n",
        "\n",
        "\n",
        "################################################################33\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "@patch_to(Learner)\n",
        "def one_batch(self, i, b):\n",
        "        print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}                   Learner#one_batch@BATCH {i}\")\n",
        "        self.iter = i\n",
        "        try:\n",
        "            print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}                   Learner#one_batch@SPLIT {i}\")\n",
        "            self._split(b);                                  self('begin_batch')\n",
        "            print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}                 Learner#one_batch@ {i} called begin_batch\")\n",
        "            self.pred = self.model(*self.xb);                self('after_pred')\n",
        "            print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}                 Learner#one_batch@ {i} called after_pred\")\n",
        "            if len(self.yb) == 0: return\n",
        "            print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}                 Learner#one_batch@ {i} did not return becasue yb.length == 0\")\n",
        "            self.loss = self.loss_func(self.pred, *self.yb); self('after_loss')\n",
        "            print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}                 Learner#one_batch@ {i} called after_loss\")\n",
        "            if not self.training: return\n",
        "            print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}                 Learner#one_batch@ {i} did not return because we are training\")\n",
        "            self.loss.backward();                            self('after_backward')\n",
        "            print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}                 Learner#one_batch@ {i} called after_backward\")\n",
        "            #self.opt.step();                                 self('after_step')\n",
        "            xm.optimizer_step(self.opt,barrier=True);        self('after_step')\n",
        "            print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}                 Learner#one_batch@ {i} called after_step\")\n",
        "            self.opt.zero_grad()\n",
        "            print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}                 Learner#one_batch@ {i} called zero_grad!!!!!!!\")\n",
        "        except CancelBatchException:\n",
        "            self('after_cancel_batch')\n",
        "            print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}                 Learner#one_batch@ {i} called after_cancel_batch\")\n",
        "        finally:\n",
        "            if False:\n",
        "                # TRACE: print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}                 {i} AFTER BATCH NOT CALLED\")\n",
        "                # TRACE: print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}                 {i} AFTER BATCH NOT CALLED\")\n",
        "                # TRACE: print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}                 {i} AFTER BATCH NOT CALLED\")\n",
        "                # TRACE: print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}                 {i} AFTER BATCH NOT CALLED\")\n",
        "                print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}                 {i} AFTER BATCH NOT CALLED\")\n",
        "            else:\n",
        "                print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}                 {i} WILL CALL after_batch\")\n",
        "                self('after_batch')\n",
        "                print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}                 {i} called after_batch\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data.dataloader import _MultiProcessingDataLoaderIter,_SingleProcessDataLoaderIter,_DatasetKind\n",
        "_loaders = (_MultiProcessingDataLoaderIter,_SingleProcessDataLoaderIter)\n",
        "\n",
        "\n",
        "@patch_to(DataLoader)\n",
        "def __iter__(self):\n",
        "        print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}  DataLoader#DataLoader#DataLoader#__iter__                         0\")\n",
        "        self.randomize()\n",
        "        self.before_iter()\n",
        "        print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}  DataLoader#DataLoader#DataLoader#__iter__ START FOR               1\")\n",
        "        xxxx = _loaders[self.fake_l.num_workers==0](self.fake_l)\n",
        "        print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}  DataLoader#DataLoader#DataLoader#__iter__ ----------------------- 1\")\n",
        "        for b in xxxx:\n",
        "            print(\"======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
        "            print(\"======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
        "            print(\"======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
        "            print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\")\n",
        "            if self.device is not None:\n",
        "                print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}  DataLoader#DataLoader#DataLoader#iterator to device from {b[0].device} y {b[1].device} to {self.device}\")\n",
        "                b = to_device(b, self.device)\n",
        "                print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}  DataLoader#DataLoader#DataLoader#iterator to done!!!!\")\n",
        "            print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch({b[0].device} y {b[1].device}) len of b is {len(b)}\")\n",
        "            yield self.after_batch(b)\n",
        "            print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}  DataLoader#DataLoader#DataLoader#yielding                    4!!!!\")\n",
        "        print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}  DataLoader#DataLoader#DataLoader#__iter__ END FOR                 2\")\n",
        "        self.after_iter()\n",
        "        print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}   DataLoader#DataLoader#DataLoader#__iter__ after ITER\")\n",
        "        if hasattr(self, 'it'): delattr(self, 'it')\n",
        "        print(f\"{datetime.now().strftime(' (%H:%M:%S.%f)')}     DataLoader#DataLoader#DataLoader#END __iter__\")\n",
        "\n",
        "\n",
        "\n",
        "@patch_to(Learner)\n",
        "def all_batches(self):\n",
        "        print(\"                                          Learner#ALL_BATCHES\")\n",
        "        self.n_iter = len(self.dl)\n",
        "        print(\"                                          Learner#ALL_BATCHES GEN enumerator!!!!\")\n",
        "        e = enumerate(self.dl)\n",
        "        print(\"                                          Learner#ALL_BATCHES generate enumerator!!!!\")\n",
        "        for o in e:\n",
        "            print(\">>>> =======================================================================\")\n",
        "            print(\">>>> =======================================================================\")\n",
        "            print(f\">>>>                                           Learner#ALL_BATCHES CALL ENTER [{o[0]}]\")\n",
        "            print(\">>>> =======================================================================\")\n",
        "            print(\">>>> ======================================================================= OOOOOOOOOOOOOOOO call self.one_batch\")\n",
        "            self.one_batch(*o)\n",
        "            print(\"<<<< ======================================================================= OOOOOOOOOOOOOOOO\")\n",
        "            print(\"<<<< =======================================================================\")\n",
        "            print(\"<<<< =======================================================================\")\n",
        "            print(\"<<<<                                           Learner#ALL_BATCHES CALL EXIT\")\n",
        "            print(\"<<<< ====================================================================***\")\n",
        "            print(\"<<<< ========================================*******************************\")\n",
        "            print(\"<<<< ===========================********************************************\")\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@patch_to(Learner)\n",
        "def create_opt(self):\n",
        "        # TRACE: print_local(\"create_opt!!!\")\n",
        "        ooo = self.opt_func(self.splitter(self.model), lr=self.lr)\n",
        "        prox = XLAOptimProxy(ooo)\n",
        "        self.opt = prox\n",
        "        if not self.wd_bn_bias:\n",
        "            for p in self._bn_bias_state(True ): p['do_wd'] = False\n",
        "        if self.train_bn:\n",
        "            for p in self._bn_bias_state(False): p['force_train'] = True\n",
        "\n",
        "#proxyLearn = Learner(dls_tpu, Lenet2(), metrics=accuracy, opt_func=Adam)#, cbs=CallbackXLA)\n",
        "\n",
        "\n",
        "\n",
        "#print(\"---------------------------------------------------------- Before fit\")\n",
        "#proxyLearn.fit(1, 10e-3) # 0.05) NOTE: Im not sure if this works...!!! it is now 96!\n",
        "#print(\"---------------------------------------------------------- After fit\")\n",
        "\n",
        "\n",
        "\n",
        "print(\".............................................\\n\"*20)\n",
        "\n",
        "@patch\n",
        "@log_args(but_as=Learner.fit)\n",
        "def fit_one_cycle(self:Learner, n_epoch, lr_max=None, div=25., div_final=1e5, pct_start=0.25, wd=None,\n",
        "                  moms=None, cbs=None, reset_opt=False):\n",
        "    \"Fit `self.model` for `n_epoch` using the 1cycle policy.\"\n",
        "    if self.opt is None: self.create_opt()\n",
        "    self.opt.set_hyper('lr', self.lr if lr_max is None else lr_max)\n",
        "    lr_max = np.array([h['lr'] for h in self.opt.hypers])\n",
        "    scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),\n",
        "              'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}\n",
        "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd)\n",
        "\n",
        "\n",
        "@patch\n",
        "@log_args(but_as=Learner.fit)\n",
        "@delegates(Learner.fit_one_cycle)\n",
        "def fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n",
        "              pct_start=0.3, div=5.0, **kwargs):\n",
        "    \"Fine tune with `freeze` for `freeze_epochs` then with `unfreeze` from `epochs` using discriminative LR\"\n",
        "    self.freeze()\n",
        "    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n",
        "    base_lr /= 2\n",
        "    self.unfreeze()\n",
        "    self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\n",
        "\n",
        "\n",
        "@patch\n",
        "def freeze_to(self:Learner, n):\n",
        "    if self.opt is None: self.create_opt()\n",
        "    self.opt.freeze_to(n)\n",
        "    self.opt.clear_state()\n",
        "\n",
        "@patch\n",
        "def freeze(self:Learner): self.freeze_to(-1)\n",
        "\n",
        "@patch\n",
        "def unfreeze(self:Learner): self.freeze_to(0)\n",
        "\n",
        "\n",
        "def _add_norm(dls, meta, pretrained):\n",
        "    if not pretrained: return\n",
        "    after_batch = dls.after_batch\n",
        "    if first(o for o in after_batch.fs if isinstance(o,Normalize)): return\n",
        "    stats = meta.get('stats')\n",
        "    if stats is None: return\n",
        "    after_batch.add(Normalize.from_stats(*stats))\n",
        "\n",
        "\n",
        "def default_split(m):\n",
        "    \"Default split of a model between body and head\"\n",
        "    return L(m[0], m[1:]).map(params)\n",
        "_default_meta    = {'cut':None, 'split':default_split}\n",
        "\n",
        "\n",
        "\n",
        "@log_args(to_return=True, but_as=Learner.__init__)\n",
        "@delegates(Learner.__init__)\n",
        "def cnn_learner(dls, arch, loss_func=None, pretrained=True, cut=None, splitter=None,\n",
        "                y_range=None, config=None, n_out=None, normalize=True, **kwargs):\n",
        "    \"Build a convnet style learner from `dls` and `arch`\"\n",
        "    print(f\"cnn dls.device {dls.device}\")\n",
        "    if config is None: config = {}\n",
        "    meta = model_meta.get(arch, _default_meta)\n",
        "    if n_out is None: n_out = get_c(dls)\n",
        "    assert n_out, \"`n_out` is not defined, and could not be infered from data, set `dls.c` or pass `n_out`\"\n",
        "    if normalize: _add_norm(dls, meta, pretrained)\n",
        "    if y_range is None and 'y_range' in config: y_range = config.pop('y_range')\n",
        "    print(f\"cnn dls.device MIDDLE\")\n",
        "    model = create_cnn_model(arch, n_out, ifnone(cut, meta['cut']), pretrained, y_range=y_range, **config)\n",
        "    learn = Learner(dls, model, loss_func=loss_func, splitter=ifnone(splitter, meta['split']), **kwargs)\n",
        "    if pretrained: learn.freeze()\n",
        "    print(f\"cnn dls.device END\")\n",
        "    return learn\n",
        "\n",
        "path = untar_data(URLs.PETS)/'images'\n",
        "pat = r'(.+)_\\d+.jpg$'\n",
        "datablock = DataBlock(\n",
        "    blocks=(ImageBlock,CategoryBlock),\n",
        "    get_items=get_image_files,\n",
        "    splitter=RandomSplitter(seed=42),\n",
        "    get_y=using_attr(RegexLabeller(pat),'name'),\n",
        "    item_tfms=Resize(224),\n",
        "    # batch_tfms=[]\n",
        "    batch_tfms=aug_transforms(size=224,min_scale=0.75)\n",
        ")\n",
        "datablock.summary(path)\n",
        "dls = datablock.dataloaders(path,bs=256, device=tpu_device)\n",
        "print(\"CNN Learner\")\n",
        "learner = cnn_learner(dls, resnet34, metrics=accuracy)\n",
        "print(\"FINE TUNE\")\n",
        "learner.fine_tune(1,base_lr=4e-3,freeze_epochs=2)\n",
        "print(\"end FINE TUNE\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------- START\n",
            "device is xla:1\n",
            "---------------------------------------------------------- START:1\n",
            "---------------------------------------------------------- START:2\n",
            "---------------------------------------------------------- START:3\n",
            " (07:15:02.164228)  DataLoader#DataLoader#DataLoader#__iter__                         0\n",
            " (07:15:02.165292)  DataLoader#DataLoader#DataLoader#__iter__ START FOR               1\n",
            " (07:15:02.169795)  DataLoader#DataLoader#DataLoader#__iter__ ----------------------- 1\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:15:02.193488)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:15:02.193909)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n",
            " (07:15:02.201670)  DataLoader#DataLoader#DataLoader#iterator to done!!!!\n",
            " (07:15:02.202112)  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch(xla:1 y xla:1) len of b is 2\n",
            "---------------------------------------------------------- START:4\n",
            "---------------------------------------------------------- START:5\n",
            "---------------------------------------------------------- START:6\n",
            "---------------------------------------------------------- START:7\n",
            "---------------------------------------------------------- START:8 enbd STARET\n",
            "---------------------- STARTED class Learner\n",
            "---------------------- PASSED LEARNER\n",
            "---------------------- STARTED torch_base.py\n",
            "---------------------- PASSED torch_base.py\n",
            "---------------------------------------------------------- ALMOOOOST END\n",
            ".............................................\n",
            ".............................................\n",
            ".............................................\n",
            ".............................................\n",
            ".............................................\n",
            ".............................................\n",
            ".............................................\n",
            ".............................................\n",
            ".............................................\n",
            ".............................................\n",
            ".............................................\n",
            ".............................................\n",
            ".............................................\n",
            ".............................................\n",
            ".............................................\n",
            ".............................................\n",
            ".............................................\n",
            ".............................................\n",
            ".............................................\n",
            ".............................................\n",
            "\n",
            "Setting-up type transforms pipelines\n",
            "Collecting items from /root/.fastai/data/oxford-iiit-pet/images\n",
            "Found 7390 items\n",
            "2 datasets of sizes 5912,1478\n",
            "Setting up Pipeline: PILBase.create\n",
            "Setting up Pipeline: partial -> Categorize\n",
            "\n",
            "Building one sample\n",
            "  Pipeline: PILBase.create\n",
            "    starting from\n",
            "      /root/.fastai/data/oxford-iiit-pet/images/scottish_terrier_127.jpg\n",
            "    applying PILBase.create gives\n",
            "      PILImage mode=RGB size=375x500\n",
            "  Pipeline: partial -> Categorize\n",
            "    starting from\n",
            "      /root/.fastai/data/oxford-iiit-pet/images/scottish_terrier_127.jpg\n",
            "    applying partial gives\n",
            "      scottish_terrier\n",
            "    applying Categorize gives\n",
            "      TensorCategory(32)\n",
            "\n",
            "Final sample: (PILImage mode=RGB size=375x500, TensorCategory(32))\n",
            "\n",
            "\n",
            "Setting up after_item: Pipeline: Resize -> ToTensor\n",
            "Setting up before_batch: Pipeline: \n",
            "Setting up after_batch: Pipeline: IntToFloatTensor -> AffineCoordTfm -> RandomResizedCropGPU -> LightingTfm\n",
            "\n",
            "Building one batch\n",
            "Applying item_tfms to the first sample:\n",
            "  Pipeline: Resize -> ToTensor\n",
            "    starting from\n",
            "      (PILImage mode=RGB size=375x500, TensorCategory(32))\n",
            "    applying Resize gives\n",
            "      (PILImage mode=RGB size=224x224, TensorCategory(32))\n",
            "    applying ToTensor gives\n",
            "      (TensorImage of size 3x224x224, TensorCategory(32))\n",
            "\n",
            "Adding the next 3 samples\n",
            "\n",
            "No before_batch transform to apply\n",
            "\n",
            "Collating items in a batch\n",
            "\n",
            "Applying batch_tfms to the batch built\n",
            "  Pipeline: IntToFloatTensor -> AffineCoordTfm -> RandomResizedCropGPU -> LightingTfm\n",
            "    starting from\n",
            "      (TensorImage of size 4x3x224x224, TensorCategory([32, 20, 29, 35]))\n",
            "    applying IntToFloatTensor gives\n",
            "      (TensorImage of size 4x3x224x224, TensorCategory([32, 20, 29, 35]))\n",
            "    applying AffineCoordTfm gives\n",
            "      (TensorImage of size 4x3x224x224, TensorCategory([32, 20, 29, 35]))\n",
            "    applying RandomResizedCropGPU gives\n",
            "      (TensorImage of size 4x3x224x224, TensorCategory([32, 20, 29, 35]))\n",
            "    applying LightingTfm gives\n",
            "      (TensorImage of size 4x3x224x224, TensorCategory([32, 20, 29, 35]))\n",
            "CNN Learner\n",
            "cnn dls.device xla:1\n",
            "cnn dls.device MIDDLE\n",
            "cnn dls.device END\n",
            "FINE TUNE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='0' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/2 00:00<00:00]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='17' class='' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      73.91% [17/23 04:18<01:31 3.8557]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "                                          Learner#ALL_BATCHES\n",
            "                                          Learner#ALL_BATCHES GEN enumerator!!!!\n",
            "                                          Learner#ALL_BATCHES generate enumerator!!!!\n",
            " (07:15:25.013256)  DataLoader#DataLoader#DataLoader#__iter__                         0\n",
            " (07:15:25.014005)  DataLoader#DataLoader#DataLoader#__iter__ START FOR               1\n",
            " (07:15:25.097918)  DataLoader#DataLoader#DataLoader#__iter__ ----------------------- 1\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:15:37.173562)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:15:37.176881)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n",
            " (07:15:39.316684)  DataLoader#DataLoader#DataLoader#iterator to done!!!!\n",
            " (07:15:39.319828)  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch(xla:1 y xla:1) len of b is 2\n",
            ">>>> =======================================================================\n",
            ">>>> =======================================================================\n",
            ">>>>                                           Learner#ALL_BATCHES CALL ENTER [0]\n",
            ">>>> =======================================================================\n",
            ">>>> ======================================================================= OOOOOOOOOOOOOOOO call self.one_batch\n",
            " (07:15:54.172144)                   Learner#one_batch@BATCH 0\n",
            " (07:15:54.172765)                   Learner#one_batch@SPLIT 0\n",
            " (07:15:54.188308)                 Learner#one_batch@ 0 called begin_batch\n",
            " (07:16:02.418419)                 Learner#one_batch@ 0 called after_pred\n",
            " (07:16:02.419087)                 Learner#one_batch@ 0 did not return becasue yb.length == 0\n",
            " (07:16:02.429082)                 Learner#one_batch@ 0 called after_loss\n",
            " (07:16:02.429621)                 Learner#one_batch@ 0 did not return because we are training\n",
            " (07:16:11.455029)                 Learner#one_batch@ 0 called after_backward\n",
            " (07:16:27.284186)                 Learner#one_batch@ 0 called after_step\n",
            " (07:16:27.292841)                 Learner#one_batch@ 0 called zero_grad!!!!!!!\n",
            " (07:16:27.293264)                 0 WILL CALL after_batch\n",
            " (07:16:27.639723)                 0 called after_batch\n",
            "<<<< ======================================================================= OOOOOOOOOOOOOOOO\n",
            "<<<< =======================================================================\n",
            "<<<< =======================================================================\n",
            "<<<<                                           Learner#ALL_BATCHES CALL EXIT\n",
            "<<<< ====================================================================***\n",
            "<<<< ========================================*******************************\n",
            "<<<< ===========================********************************************\n",
            " (07:16:27.642913)  DataLoader#DataLoader#DataLoader#yielding                    4!!!!\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:16:27.644895)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:16:27.645288)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n",
            " (07:16:29.304716)  DataLoader#DataLoader#DataLoader#iterator to done!!!!\n",
            " (07:16:29.305916)  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch(xla:1 y xla:1) len of b is 2\n",
            ">>>> =======================================================================\n",
            ">>>> =======================================================================\n",
            ">>>>                                           Learner#ALL_BATCHES CALL ENTER [1]\n",
            ">>>> =======================================================================\n",
            ">>>> ======================================================================= OOOOOOOOOOOOOOOO call self.one_batch\n",
            " (07:16:38.629329)                   Learner#one_batch@BATCH 1\n",
            " (07:16:38.629725)                   Learner#one_batch@SPLIT 1\n",
            " (07:16:38.638068)                 Learner#one_batch@ 1 called begin_batch\n",
            " (07:16:38.935730)                 Learner#one_batch@ 1 called after_pred\n",
            " (07:16:38.936368)                 Learner#one_batch@ 1 did not return becasue yb.length == 0\n",
            " (07:16:38.945960)                 Learner#one_batch@ 1 called after_loss\n",
            " (07:16:38.946394)                 Learner#one_batch@ 1 did not return because we are training\n",
            " (07:16:39.479736)                 Learner#one_batch@ 1 called after_backward\n",
            " (07:16:39.527306)                 Learner#one_batch@ 1 called after_step\n",
            " (07:16:39.535856)                 Learner#one_batch@ 1 called zero_grad!!!!!!!\n",
            " (07:16:39.536288)                 1 WILL CALL after_batch\n",
            " (07:16:39.836072)                 1 called after_batch\n",
            "<<<< ======================================================================= OOOOOOOOOOOOOOOO\n",
            "<<<< =======================================================================\n",
            "<<<< =======================================================================\n",
            "<<<<                                           Learner#ALL_BATCHES CALL EXIT\n",
            "<<<< ====================================================================***\n",
            "<<<< ========================================*******************************\n",
            "<<<< ===========================********************************************\n",
            " (07:16:39.839167)  DataLoader#DataLoader#DataLoader#yielding                    4!!!!\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:16:39.860763)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:16:39.861283)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n",
            " (07:16:41.507236)  DataLoader#DataLoader#DataLoader#iterator to done!!!!\n",
            " (07:16:41.508168)  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch(xla:1 y xla:1) len of b is 2\n",
            ">>>> =======================================================================\n",
            ">>>> =======================================================================\n",
            ">>>>                                           Learner#ALL_BATCHES CALL ENTER [2]\n",
            ">>>> =======================================================================\n",
            ">>>> ======================================================================= OOOOOOOOOOOOOOOO call self.one_batch\n",
            " (07:16:50.692456)                   Learner#one_batch@BATCH 2\n",
            " (07:16:50.692858)                   Learner#one_batch@SPLIT 2\n",
            " (07:16:50.701309)                 Learner#one_batch@ 2 called begin_batch\n",
            " (07:16:50.992070)                 Learner#one_batch@ 2 called after_pred\n",
            " (07:16:50.992703)                 Learner#one_batch@ 2 did not return becasue yb.length == 0\n",
            " (07:16:51.002458)                 Learner#one_batch@ 2 called after_loss\n",
            " (07:16:51.002877)                 Learner#one_batch@ 2 did not return because we are training\n",
            " (07:16:51.503525)                 Learner#one_batch@ 2 called after_backward\n",
            " (07:16:51.551250)                 Learner#one_batch@ 2 called after_step\n",
            " (07:16:51.559538)                 Learner#one_batch@ 2 called zero_grad!!!!!!!\n",
            " (07:16:51.560124)                 2 WILL CALL after_batch\n",
            " (07:16:51.861375)                 2 called after_batch\n",
            "<<<< ======================================================================= OOOOOOOOOOOOOOOO\n",
            "<<<< =======================================================================\n",
            "<<<< =======================================================================\n",
            "<<<<                                           Learner#ALL_BATCHES CALL EXIT\n",
            "<<<< ====================================================================***\n",
            "<<<< ========================================*******************************\n",
            "<<<< ===========================********************************************\n",
            " (07:16:51.864756)  DataLoader#DataLoader#DataLoader#yielding                    4!!!!\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:16:51.866781)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:16:51.867227)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n",
            " (07:16:53.458107)  DataLoader#DataLoader#DataLoader#iterator to done!!!!\n",
            " (07:16:53.458968)  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch(xla:1 y xla:1) len of b is 2\n",
            ">>>> =======================================================================\n",
            ">>>> =======================================================================\n",
            ">>>>                                           Learner#ALL_BATCHES CALL ENTER [3]\n",
            ">>>> =======================================================================\n",
            ">>>> ======================================================================= OOOOOOOOOOOOOOOO call self.one_batch\n",
            " (07:17:02.871614)                   Learner#one_batch@BATCH 3\n",
            " (07:17:02.872013)                   Learner#one_batch@SPLIT 3\n",
            " (07:17:02.880486)                 Learner#one_batch@ 3 called begin_batch\n",
            " (07:17:03.176678)                 Learner#one_batch@ 3 called after_pred\n",
            " (07:17:03.177407)                 Learner#one_batch@ 3 did not return becasue yb.length == 0\n",
            " (07:17:03.186884)                 Learner#one_batch@ 3 called after_loss\n",
            " (07:17:03.187308)                 Learner#one_batch@ 3 did not return because we are training\n",
            " (07:17:03.692511)                 Learner#one_batch@ 3 called after_backward\n",
            " (07:17:03.736704)                 Learner#one_batch@ 3 called after_step\n",
            " (07:17:03.744625)                 Learner#one_batch@ 3 called zero_grad!!!!!!!\n",
            " (07:17:03.745025)                 3 WILL CALL after_batch\n",
            " (07:17:04.045452)                 3 called after_batch\n",
            "<<<< ======================================================================= OOOOOOOOOOOOOOOO\n",
            "<<<< =======================================================================\n",
            "<<<< =======================================================================\n",
            "<<<<                                           Learner#ALL_BATCHES CALL EXIT\n",
            "<<<< ====================================================================***\n",
            "<<<< ========================================*******************************\n",
            "<<<< ===========================********************************************\n",
            " (07:17:04.048594)  DataLoader#DataLoader#DataLoader#yielding                    4!!!!\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:17:04.062058)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:17:04.062474)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n",
            " (07:17:05.695723)  DataLoader#DataLoader#DataLoader#iterator to done!!!!\n",
            " (07:17:05.696649)  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch(xla:1 y xla:1) len of b is 2\n",
            ">>>> =======================================================================\n",
            ">>>> =======================================================================\n",
            ">>>>                                           Learner#ALL_BATCHES CALL ENTER [4]\n",
            ">>>> =======================================================================\n",
            ">>>> ======================================================================= OOOOOOOOOOOOOOOO call self.one_batch\n",
            " (07:17:15.173362)                   Learner#one_batch@BATCH 4\n",
            " (07:17:15.173872)                   Learner#one_batch@SPLIT 4\n",
            " (07:17:15.182551)                 Learner#one_batch@ 4 called begin_batch\n",
            " (07:17:15.475314)                 Learner#one_batch@ 4 called after_pred\n",
            " (07:17:15.475943)                 Learner#one_batch@ 4 did not return becasue yb.length == 0\n",
            " (07:17:15.485822)                 Learner#one_batch@ 4 called after_loss\n",
            " (07:17:15.486259)                 Learner#one_batch@ 4 did not return because we are training\n",
            " (07:17:15.994845)                 Learner#one_batch@ 4 called after_backward\n",
            " (07:17:16.060033)                 Learner#one_batch@ 4 called after_step\n",
            " (07:17:16.069190)                 Learner#one_batch@ 4 called zero_grad!!!!!!!\n",
            " (07:17:16.069716)                 4 WILL CALL after_batch\n",
            " (07:17:16.368443)                 4 called after_batch\n",
            "<<<< ======================================================================= OOOOOOOOOOOOOOOO\n",
            "<<<< =======================================================================\n",
            "<<<< =======================================================================\n",
            "<<<<                                           Learner#ALL_BATCHES CALL EXIT\n",
            "<<<< ====================================================================***\n",
            "<<<< ========================================*******************************\n",
            "<<<< ===========================********************************************\n",
            " (07:17:16.371542)  DataLoader#DataLoader#DataLoader#yielding                    4!!!!\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:17:16.390493)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:17:16.390998)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n",
            " (07:17:18.015838)  DataLoader#DataLoader#DataLoader#iterator to done!!!!\n",
            " (07:17:18.017007)  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch(xla:1 y xla:1) len of b is 2\n",
            ">>>> =======================================================================\n",
            ">>>> =======================================================================\n",
            ">>>>                                           Learner#ALL_BATCHES CALL ENTER [5]\n",
            ">>>> =======================================================================\n",
            ">>>> ======================================================================= OOOOOOOOOOOOOOOO call self.one_batch\n",
            " (07:17:27.511329)                   Learner#one_batch@BATCH 5\n",
            " (07:17:27.511716)                   Learner#one_batch@SPLIT 5\n",
            " (07:17:27.519869)                 Learner#one_batch@ 5 called begin_batch\n",
            " (07:17:27.806226)                 Learner#one_batch@ 5 called after_pred\n",
            " (07:17:27.806851)                 Learner#one_batch@ 5 did not return becasue yb.length == 0\n",
            " (07:17:27.816301)                 Learner#one_batch@ 5 called after_loss\n",
            " (07:17:27.816708)                 Learner#one_batch@ 5 did not return because we are training\n",
            " (07:17:28.316518)                 Learner#one_batch@ 5 called after_backward\n",
            " (07:17:28.371950)                 Learner#one_batch@ 5 called after_step\n",
            " (07:17:28.379936)                 Learner#one_batch@ 5 called zero_grad!!!!!!!\n",
            " (07:17:28.380358)                 5 WILL CALL after_batch\n",
            " (07:17:28.679914)                 5 called after_batch\n",
            "<<<< ======================================================================= OOOOOOOOOOOOOOOO\n",
            "<<<< =======================================================================\n",
            "<<<< =======================================================================\n",
            "<<<<                                           Learner#ALL_BATCHES CALL EXIT\n",
            "<<<< ====================================================================***\n",
            "<<<< ========================================*******************************\n",
            "<<<< ===========================********************************************\n",
            " (07:17:28.683002)  DataLoader#DataLoader#DataLoader#yielding                    4!!!!\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:17:28.699103)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:17:28.699491)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n",
            " (07:17:30.329500)  DataLoader#DataLoader#DataLoader#iterator to done!!!!\n",
            " (07:17:30.330374)  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch(xla:1 y xla:1) len of b is 2\n",
            ">>>> =======================================================================\n",
            ">>>> =======================================================================\n",
            ">>>>                                           Learner#ALL_BATCHES CALL ENTER [6]\n",
            ">>>> =======================================================================\n",
            ">>>> ======================================================================= OOOOOOOOOOOOOOOO call self.one_batch\n",
            " (07:17:39.871819)                   Learner#one_batch@BATCH 6\n",
            " (07:17:39.872232)                   Learner#one_batch@SPLIT 6\n",
            " (07:17:39.881148)                 Learner#one_batch@ 6 called begin_batch\n",
            " (07:17:40.168253)                 Learner#one_batch@ 6 called after_pred\n",
            " (07:17:40.168862)                 Learner#one_batch@ 6 did not return becasue yb.length == 0\n",
            " (07:17:40.178524)                 Learner#one_batch@ 6 called after_loss\n",
            " (07:17:40.178925)                 Learner#one_batch@ 6 did not return because we are training\n",
            " (07:17:40.679352)                 Learner#one_batch@ 6 called after_backward\n",
            " (07:17:40.741932)                 Learner#one_batch@ 6 called after_step\n",
            " (07:17:40.750410)                 Learner#one_batch@ 6 called zero_grad!!!!!!!\n",
            " (07:17:40.750818)                 6 WILL CALL after_batch\n",
            " (07:17:41.049912)                 6 called after_batch\n",
            "<<<< ======================================================================= OOOOOOOOOOOOOOOO\n",
            "<<<< =======================================================================\n",
            "<<<< =======================================================================\n",
            "<<<<                                           Learner#ALL_BATCHES CALL EXIT\n",
            "<<<< ====================================================================***\n",
            "<<<< ========================================*******************************\n",
            "<<<< ===========================********************************************\n",
            " (07:17:41.053013)  DataLoader#DataLoader#DataLoader#yielding                    4!!!!\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:17:41.067639)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:17:41.068008)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n",
            " (07:17:42.706829)  DataLoader#DataLoader#DataLoader#iterator to done!!!!\n",
            " (07:17:42.707724)  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch(xla:1 y xla:1) len of b is 2\n",
            ">>>> =======================================================================\n",
            ">>>> =======================================================================\n",
            ">>>>                                           Learner#ALL_BATCHES CALL ENTER [7]\n",
            ">>>> =======================================================================\n",
            ">>>> ======================================================================= OOOOOOOOOOOOOOOO call self.one_batch\n",
            " (07:17:52.108578)                   Learner#one_batch@BATCH 7\n",
            " (07:17:52.108973)                   Learner#one_batch@SPLIT 7\n",
            " (07:17:52.117170)                 Learner#one_batch@ 7 called begin_batch\n",
            " (07:17:52.407579)                 Learner#one_batch@ 7 called after_pred\n",
            " (07:17:52.408273)                 Learner#one_batch@ 7 did not return becasue yb.length == 0\n",
            " (07:17:52.418342)                 Learner#one_batch@ 7 called after_loss\n",
            " (07:17:52.418863)                 Learner#one_batch@ 7 did not return because we are training\n",
            " (07:17:52.929617)                 Learner#one_batch@ 7 called after_backward\n",
            " (07:17:52.985071)                 Learner#one_batch@ 7 called after_step\n",
            " (07:17:52.993915)                 Learner#one_batch@ 7 called zero_grad!!!!!!!\n",
            " (07:17:52.994445)                 7 WILL CALL after_batch\n",
            " (07:17:53.294173)                 7 called after_batch\n",
            "<<<< ======================================================================= OOOOOOOOOOOOOOOO\n",
            "<<<< =======================================================================\n",
            "<<<< =======================================================================\n",
            "<<<<                                           Learner#ALL_BATCHES CALL EXIT\n",
            "<<<< ====================================================================***\n",
            "<<<< ========================================*******************************\n",
            "<<<< ===========================********************************************\n",
            " (07:17:53.297280)  DataLoader#DataLoader#DataLoader#yielding                    4!!!!\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:17:53.312112)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:17:53.312535)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n",
            " (07:17:54.952427)  DataLoader#DataLoader#DataLoader#iterator to done!!!!\n",
            " (07:17:54.953320)  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch(xla:1 y xla:1) len of b is 2\n",
            ">>>> =======================================================================\n",
            ">>>> =======================================================================\n",
            ">>>>                                           Learner#ALL_BATCHES CALL ENTER [8]\n",
            ">>>> =======================================================================\n",
            ">>>> ======================================================================= OOOOOOOOOOOOOOOO call self.one_batch\n",
            " (07:18:04.588038)                   Learner#one_batch@BATCH 8\n",
            " (07:18:04.588458)                   Learner#one_batch@SPLIT 8\n",
            " (07:18:04.597155)                 Learner#one_batch@ 8 called begin_batch\n",
            " (07:18:04.880335)                 Learner#one_batch@ 8 called after_pred\n",
            " (07:18:04.880982)                 Learner#one_batch@ 8 did not return becasue yb.length == 0\n",
            " (07:18:04.890976)                 Learner#one_batch@ 8 called after_loss\n",
            " (07:18:04.891410)                 Learner#one_batch@ 8 did not return because we are training\n",
            " (07:18:05.389839)                 Learner#one_batch@ 8 called after_backward\n",
            " (07:18:05.444373)                 Learner#one_batch@ 8 called after_step\n",
            " (07:18:05.452203)                 Learner#one_batch@ 8 called zero_grad!!!!!!!\n",
            " (07:18:05.452610)                 8 WILL CALL after_batch\n",
            " (07:18:05.752579)                 8 called after_batch\n",
            "<<<< ======================================================================= OOOOOOOOOOOOOOOO\n",
            "<<<< =======================================================================\n",
            "<<<< =======================================================================\n",
            "<<<<                                           Learner#ALL_BATCHES CALL EXIT\n",
            "<<<< ====================================================================***\n",
            "<<<< ========================================*******************************\n",
            "<<<< ===========================********************************************\n",
            " (07:18:05.755678)  DataLoader#DataLoader#DataLoader#yielding                    4!!!!\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:18:05.769239)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:18:05.769638)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n",
            " (07:18:07.387748)  DataLoader#DataLoader#DataLoader#iterator to done!!!!\n",
            " (07:18:07.389016)  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch(xla:1 y xla:1) len of b is 2\n",
            ">>>> =======================================================================\n",
            ">>>> =======================================================================\n",
            ">>>>                                           Learner#ALL_BATCHES CALL ENTER [9]\n",
            ">>>> =======================================================================\n",
            ">>>> ======================================================================= OOOOOOOOOOOOOOOO call self.one_batch\n",
            " (07:18:16.712972)                   Learner#one_batch@BATCH 9\n",
            " (07:18:16.713383)                   Learner#one_batch@SPLIT 9\n",
            " (07:18:16.721808)                 Learner#one_batch@ 9 called begin_batch\n",
            " (07:18:17.009228)                 Learner#one_batch@ 9 called after_pred\n",
            " (07:18:17.009853)                 Learner#one_batch@ 9 did not return becasue yb.length == 0\n",
            " (07:18:17.019329)                 Learner#one_batch@ 9 called after_loss\n",
            " (07:18:17.019736)                 Learner#one_batch@ 9 did not return because we are training\n",
            " (07:18:17.525829)                 Learner#one_batch@ 9 called after_backward\n",
            " (07:18:17.579974)                 Learner#one_batch@ 9 called after_step\n",
            " (07:18:17.588367)                 Learner#one_batch@ 9 called zero_grad!!!!!!!\n",
            " (07:18:17.588795)                 9 WILL CALL after_batch\n",
            " (07:18:17.889073)                 9 called after_batch\n",
            "<<<< ======================================================================= OOOOOOOOOOOOOOOO\n",
            "<<<< =======================================================================\n",
            "<<<< =======================================================================\n",
            "<<<<                                           Learner#ALL_BATCHES CALL EXIT\n",
            "<<<< ====================================================================***\n",
            "<<<< ========================================*******************************\n",
            "<<<< ===========================********************************************\n",
            " (07:18:17.892220)  DataLoader#DataLoader#DataLoader#yielding                    4!!!!\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:18:17.911748)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:18:17.912169)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n",
            " (07:18:19.543314)  DataLoader#DataLoader#DataLoader#iterator to done!!!!\n",
            " (07:18:19.544552)  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch(xla:1 y xla:1) len of b is 2\n",
            ">>>> =======================================================================\n",
            ">>>> =======================================================================\n",
            ">>>>                                           Learner#ALL_BATCHES CALL ENTER [10]\n",
            ">>>> =======================================================================\n",
            ">>>> ======================================================================= OOOOOOOOOOOOOOOO call self.one_batch\n",
            " (07:18:28.829445)                   Learner#one_batch@BATCH 10\n",
            " (07:18:28.829846)                   Learner#one_batch@SPLIT 10\n",
            " (07:18:28.838416)                 Learner#one_batch@ 10 called begin_batch\n",
            " (07:18:29.122685)                 Learner#one_batch@ 10 called after_pred\n",
            " (07:18:29.123335)                 Learner#one_batch@ 10 did not return becasue yb.length == 0\n",
            " (07:18:29.133094)                 Learner#one_batch@ 10 called after_loss\n",
            " (07:18:29.133508)                 Learner#one_batch@ 10 did not return because we are training\n",
            " (07:18:29.643633)                 Learner#one_batch@ 10 called after_backward\n",
            " (07:18:29.701338)                 Learner#one_batch@ 10 called after_step\n",
            " (07:18:29.709617)                 Learner#one_batch@ 10 called zero_grad!!!!!!!\n",
            " (07:18:29.710074)                 10 WILL CALL after_batch\n",
            " (07:18:30.008983)                 10 called after_batch\n",
            "<<<< ======================================================================= OOOOOOOOOOOOOOOO\n",
            "<<<< =======================================================================\n",
            "<<<< =======================================================================\n",
            "<<<<                                           Learner#ALL_BATCHES CALL EXIT\n",
            "<<<< ====================================================================***\n",
            "<<<< ========================================*******************************\n",
            "<<<< ===========================********************************************\n",
            " (07:18:30.012149)  DataLoader#DataLoader#DataLoader#yielding                    4!!!!\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:18:30.027987)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:18:30.028515)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n",
            " (07:18:31.662151)  DataLoader#DataLoader#DataLoader#iterator to done!!!!\n",
            " (07:18:31.663082)  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch(xla:1 y xla:1) len of b is 2\n",
            ">>>> =======================================================================\n",
            ">>>> =======================================================================\n",
            ">>>>                                           Learner#ALL_BATCHES CALL ENTER [11]\n",
            ">>>> =======================================================================\n",
            ">>>> ======================================================================= OOOOOOOOOOOOOOOO call self.one_batch\n",
            " (07:18:41.035114)                   Learner#one_batch@BATCH 11\n",
            " (07:18:41.035518)                   Learner#one_batch@SPLIT 11\n",
            " (07:18:41.044067)                 Learner#one_batch@ 11 called begin_batch\n",
            " (07:18:41.329021)                 Learner#one_batch@ 11 called after_pred\n",
            " (07:18:41.329682)                 Learner#one_batch@ 11 did not return becasue yb.length == 0\n",
            " (07:18:41.339736)                 Learner#one_batch@ 11 called after_loss\n",
            " (07:18:41.340241)                 Learner#one_batch@ 11 did not return because we are training\n",
            " (07:18:41.852346)                 Learner#one_batch@ 11 called after_backward\n",
            " (07:18:41.908890)                 Learner#one_batch@ 11 called after_step\n",
            " (07:18:41.917224)                 Learner#one_batch@ 11 called zero_grad!!!!!!!\n",
            " (07:18:41.917640)                 11 WILL CALL after_batch\n",
            " (07:18:42.218283)                 11 called after_batch\n",
            "<<<< ======================================================================= OOOOOOOOOOOOOOOO\n",
            "<<<< =======================================================================\n",
            "<<<< =======================================================================\n",
            "<<<<                                           Learner#ALL_BATCHES CALL EXIT\n",
            "<<<< ====================================================================***\n",
            "<<<< ========================================*******************************\n",
            "<<<< ===========================********************************************\n",
            " (07:18:42.221410)  DataLoader#DataLoader#DataLoader#yielding                    4!!!!\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:18:42.237679)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:18:42.238223)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n",
            " (07:18:43.871999)  DataLoader#DataLoader#DataLoader#iterator to done!!!!\n",
            " (07:18:43.872796)  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch(xla:1 y xla:1) len of b is 2\n",
            ">>>> =======================================================================\n",
            ">>>> =======================================================================\n",
            ">>>>                                           Learner#ALL_BATCHES CALL ENTER [12]\n",
            ">>>> =======================================================================\n",
            ">>>> ======================================================================= OOOOOOOOOOOOOOOO call self.one_batch\n",
            " (07:18:53.458787)                   Learner#one_batch@BATCH 12\n",
            " (07:18:53.459203)                   Learner#one_batch@SPLIT 12\n",
            " (07:18:53.467751)                 Learner#one_batch@ 12 called begin_batch\n",
            " (07:18:53.759692)                 Learner#one_batch@ 12 called after_pred\n",
            " (07:18:53.760358)                 Learner#one_batch@ 12 did not return becasue yb.length == 0\n",
            " (07:18:53.770142)                 Learner#one_batch@ 12 called after_loss\n",
            " (07:18:53.770550)                 Learner#one_batch@ 12 did not return because we are training\n",
            " (07:18:54.281080)                 Learner#one_batch@ 12 called after_backward\n",
            " (07:18:54.336410)                 Learner#one_batch@ 12 called after_step\n",
            " (07:18:54.344573)                 Learner#one_batch@ 12 called zero_grad!!!!!!!\n",
            " (07:18:54.344983)                 12 WILL CALL after_batch\n",
            " (07:18:54.644160)                 12 called after_batch\n",
            "<<<< ======================================================================= OOOOOOOOOOOOOOOO\n",
            "<<<< =======================================================================\n",
            "<<<< =======================================================================\n",
            "<<<<                                           Learner#ALL_BATCHES CALL EXIT\n",
            "<<<< ====================================================================***\n",
            "<<<< ========================================*******************************\n",
            "<<<< ===========================********************************************\n",
            " (07:18:54.647248)  DataLoader#DataLoader#DataLoader#yielding                    4!!!!\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:18:54.667883)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:18:54.668547)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n",
            " (07:18:56.300111)  DataLoader#DataLoader#DataLoader#iterator to done!!!!\n",
            " (07:18:56.301397)  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch(xla:1 y xla:1) len of b is 2\n",
            ">>>> =======================================================================\n",
            ">>>> =======================================================================\n",
            ">>>>                                           Learner#ALL_BATCHES CALL ENTER [13]\n",
            ">>>> =======================================================================\n",
            ">>>> ======================================================================= OOOOOOOOOOOOOOOO call self.one_batch\n",
            " (07:19:05.417989)                   Learner#one_batch@BATCH 13\n",
            " (07:19:05.418402)                   Learner#one_batch@SPLIT 13\n",
            " (07:19:05.426375)                 Learner#one_batch@ 13 called begin_batch\n",
            " (07:19:05.718329)                 Learner#one_batch@ 13 called after_pred\n",
            " (07:19:05.718960)                 Learner#one_batch@ 13 did not return becasue yb.length == 0\n",
            " (07:19:05.728667)                 Learner#one_batch@ 13 called after_loss\n",
            " (07:19:05.729084)                 Learner#one_batch@ 13 did not return because we are training\n",
            " (07:19:06.230183)                 Learner#one_batch@ 13 called after_backward\n",
            " (07:19:06.286636)                 Learner#one_batch@ 13 called after_step\n",
            " (07:19:06.294545)                 Learner#one_batch@ 13 called zero_grad!!!!!!!\n",
            " (07:19:06.294932)                 13 WILL CALL after_batch\n",
            " (07:19:06.594544)                 13 called after_batch\n",
            "<<<< ======================================================================= OOOOOOOOOOOOOOOO\n",
            "<<<< =======================================================================\n",
            "<<<< =======================================================================\n",
            "<<<<                                           Learner#ALL_BATCHES CALL EXIT\n",
            "<<<< ====================================================================***\n",
            "<<<< ========================================*******************************\n",
            "<<<< ===========================********************************************\n",
            " (07:19:06.597763)  DataLoader#DataLoader#DataLoader#yielding                    4!!!!\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:19:06.613474)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:19:06.613864)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n",
            " (07:19:08.192406)  DataLoader#DataLoader#DataLoader#iterator to done!!!!\n",
            " (07:19:08.193634)  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch(xla:1 y xla:1) len of b is 2\n",
            ">>>> =======================================================================\n",
            ">>>> =======================================================================\n",
            ">>>>                                           Learner#ALL_BATCHES CALL ENTER [14]\n",
            ">>>> =======================================================================\n",
            ">>>> ======================================================================= OOOOOOOOOOOOOOOO call self.one_batch\n",
            " (07:19:17.507480)                   Learner#one_batch@BATCH 14\n",
            " (07:19:17.507878)                   Learner#one_batch@SPLIT 14\n",
            " (07:19:17.516244)                 Learner#one_batch@ 14 called begin_batch\n",
            " (07:19:17.807357)                 Learner#one_batch@ 14 called after_pred\n",
            " (07:19:17.807992)                 Learner#one_batch@ 14 did not return becasue yb.length == 0\n",
            " (07:19:17.817977)                 Learner#one_batch@ 14 called after_loss\n",
            " (07:19:17.818410)                 Learner#one_batch@ 14 did not return because we are training\n",
            " (07:19:18.335078)                 Learner#one_batch@ 14 called after_backward\n",
            " (07:19:18.393615)                 Learner#one_batch@ 14 called after_step\n",
            " (07:19:18.401612)                 Learner#one_batch@ 14 called zero_grad!!!!!!!\n",
            " (07:19:18.402011)                 14 WILL CALL after_batch\n",
            " (07:19:18.702800)                 14 called after_batch\n",
            "<<<< ======================================================================= OOOOOOOOOOOOOOOO\n",
            "<<<< =======================================================================\n",
            "<<<< =======================================================================\n",
            "<<<<                                           Learner#ALL_BATCHES CALL EXIT\n",
            "<<<< ====================================================================***\n",
            "<<<< ========================================*******************************\n",
            "<<<< ===========================********************************************\n",
            " (07:19:18.705930)  DataLoader#DataLoader#DataLoader#yielding                    4!!!!\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:19:18.725490)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:19:18.726006)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n",
            " (07:19:20.354525)  DataLoader#DataLoader#DataLoader#iterator to done!!!!\n",
            " (07:19:20.355775)  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch(xla:1 y xla:1) len of b is 2\n",
            ">>>> =======================================================================\n",
            ">>>> =======================================================================\n",
            ">>>>                                           Learner#ALL_BATCHES CALL ENTER [15]\n",
            ">>>> =======================================================================\n",
            ">>>> ======================================================================= OOOOOOOOOOOOOOOO call self.one_batch\n",
            " (07:19:29.840756)                   Learner#one_batch@BATCH 15\n",
            " (07:19:29.841477)                   Learner#one_batch@SPLIT 15\n",
            " (07:19:29.849961)                 Learner#one_batch@ 15 called begin_batch\n",
            " (07:19:30.138806)                 Learner#one_batch@ 15 called after_pred\n",
            " (07:19:30.139467)                 Learner#one_batch@ 15 did not return becasue yb.length == 0\n",
            " (07:19:30.149060)                 Learner#one_batch@ 15 called after_loss\n",
            " (07:19:30.149466)                 Learner#one_batch@ 15 did not return because we are training\n",
            " (07:19:30.665098)                 Learner#one_batch@ 15 called after_backward\n",
            " (07:19:30.721612)                 Learner#one_batch@ 15 called after_step\n",
            " (07:19:30.729836)                 Learner#one_batch@ 15 called zero_grad!!!!!!!\n",
            " (07:19:30.730274)                 15 WILL CALL after_batch\n",
            " (07:19:31.030333)                 15 called after_batch\n",
            "<<<< ======================================================================= OOOOOOOOOOOOOOOO\n",
            "<<<< =======================================================================\n",
            "<<<< =======================================================================\n",
            "<<<<                                           Learner#ALL_BATCHES CALL EXIT\n",
            "<<<< ====================================================================***\n",
            "<<<< ========================================*******************************\n",
            "<<<< ===========================********************************************\n",
            " (07:19:31.033464)  DataLoader#DataLoader#DataLoader#yielding                    4!!!!\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:19:31.048404)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:19:31.051620)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n",
            " (07:19:32.638757)  DataLoader#DataLoader#DataLoader#iterator to done!!!!\n",
            " (07:19:32.639667)  DataLoader#DataLoader#DataLoader#yielding                    3!!!! yield self.after_batch(xla:1 y xla:1) len of b is 2\n",
            ">>>> =======================================================================\n",
            ">>>> =======================================================================\n",
            ">>>>                                           Learner#ALL_BATCHES CALL ENTER [16]\n",
            ">>>> =======================================================================\n",
            ">>>> ======================================================================= OOOOOOOOOOOOOOOO call self.one_batch\n",
            " (07:19:42.138351)                   Learner#one_batch@BATCH 16\n",
            " (07:19:42.138759)                   Learner#one_batch@SPLIT 16\n",
            " (07:19:42.147651)                 Learner#one_batch@ 16 called begin_batch\n",
            " (07:19:42.452301)                 Learner#one_batch@ 16 called after_pred\n",
            " (07:19:42.452916)                 Learner#one_batch@ 16 did not return becasue yb.length == 0\n",
            " (07:19:42.462576)                 Learner#one_batch@ 16 called after_loss\n",
            " (07:19:42.462990)                 Learner#one_batch@ 16 did not return because we are training\n",
            " (07:19:42.972250)                 Learner#one_batch@ 16 called after_backward\n",
            " (07:19:43.040582)                 Learner#one_batch@ 16 called after_step\n",
            " (07:19:43.048877)                 Learner#one_batch@ 16 called zero_grad!!!!!!!\n",
            " (07:19:43.049373)                 16 WILL CALL after_batch\n",
            " (07:19:43.349970)                 16 called after_batch\n",
            "<<<< ======================================================================= OOOOOOOOOOOOOOOO\n",
            "<<<< =======================================================================\n",
            "<<<< =======================================================================\n",
            "<<<<                                           Learner#ALL_BATCHES CALL EXIT\n",
            "<<<< ====================================================================***\n",
            "<<<< ========================================*******************************\n",
            "<<<< ===========================********************************************\n",
            " (07:19:43.353072)  DataLoader#DataLoader#DataLoader#yielding                    4!!!!\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "======================================================================================     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            " (07:19:43.369419)  DataLoader#DataLoader#DataLoader#__iter__ FOR FOR FOR FOR ---------------------------------------------\n",
            " (07:19:43.369791)  DataLoader#DataLoader#DataLoader#iterator to device from cpu y cpu to xla:1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm_BSVGBIroQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq04XX8WJSEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}