{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems we have hit testing different models and transforms is that sometimes it is slower even than CPUs, but this happens because we hit operations on pytorch that are only handled by CPU and not by hte accelerator. `print_aten_ops` calls directly some pytorch metrics wich ouputs to stdout, so the only way to get that info is capture it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_aten_ops():\n",
    "    import torch_xla.debug.metrics as met\n",
    "    from io import StringIO \n",
    "    import sys\n",
    "\n",
    "    class Capturing(list):\n",
    "        def __enter__(self):\n",
    "            self._stdout = sys.stdout\n",
    "            sys.stdout = self._stringio = StringIO()\n",
    "            return self\n",
    "        def __exit__(self, *args):\n",
    "            self.extend(self._stringio.getvalue().splitlines())\n",
    "            del self._stringio    # free up some memory\n",
    "            sys.stdout = self._stdout\n",
    "\n",
    "    out = met.metrics_report()\n",
    "    if out.find(\"aten::\"):\n",
    "        print_now=False\n",
    "        lines = out.split(\"\\n\")\n",
    "        for l in lines:\n",
    "            if print_now:\n",
    "                print_now=False\n",
    "                print(l)\n",
    "            if l.find(\"aten::\")>-1:\n",
    "                print(\"needs lowering:\", l)\n",
    "                print_now=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
