{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems we have hit testing different models and transforms is that sometimes it is slower even than CPUs, but this happens because we hit operations on pytorch that are only handled by CPU and not by hte accelerator. `print_aten_ops` calls directly some pytorch metrics wich ouputs to stdout, so the only way to get that info is capture it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "!pip install -Uqq cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import warnings\n",
    "try:\n",
    "    import torch_xla\n",
    "except ImportError as e:\n",
    "    if DEBUG:\n",
    "        warnings.warn('TPU Environment not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "#hide_output\n",
    "import sys\n",
    "\n",
    "def xla_imported(): \n",
    "    return 'torch_xla' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "if not xla_imported():\n",
    "    from types import SimpleNamespace\n",
    "    def fake_metrics_report(*args,**kwargs):\n",
    "        return \"\"\n",
    "    met = SimpleNamespace(\n",
    "        metrics_report = fake_metrics_report\n",
    "    )\n",
    "else:\n",
    "    import torch_xla.debug.metrics as met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_aten_ops():\n",
    "    # import torch_xla.debug.metrics as met\n",
    "    from io import StringIO \n",
    "    import sys\n",
    "\n",
    "    class Capturing(list):\n",
    "        def __enter__(self):\n",
    "            self._stdout = sys.stdout\n",
    "            sys.stdout = self._stringio = StringIO()\n",
    "            return self\n",
    "        def __exit__(self, *args):\n",
    "            self.extend(self._stringio.getvalue().splitlines())\n",
    "            del self._stringio    # free up some memory\n",
    "            sys.stdout = self._stdout\n",
    "\n",
    "    out = met.metrics_report()\n",
    "    if out.find(\"aten::\"):\n",
    "        print_now=False\n",
    "        lines = out.split(\"\\n\")\n",
    "        for l in lines:\n",
    "            if print_now:\n",
    "                print_now=False\n",
    "                print(l)\n",
    "            if l.find(\"aten::\")>-1:\n",
    "                print(\"needs lowering:\", l)\n",
    "                print_now=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
