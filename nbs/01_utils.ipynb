{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/butchland/fastai_xla_extensions/blob/master/nbs/01_utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Utilities used by other modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 133.6MB 77kB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 3.1MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -Uqq cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Waiting for TPU to be start up with version pytorch-1.7...\n",
      "WARNING:root:Waiting for TPU to be start up with version pytorch-1.7...\n",
      "WARNING:root:TPU has started up successfully with version pytorch-1.7\n"
     ]
    }
   ],
   "source": [
    "#exporti\n",
    "try:\n",
    "    import torch_xla\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sys\n",
    "\n",
    "def xla_imported():\n",
    "    \"Check whether the `torch_xla` module has been successfully imported\"\n",
    "    return 'torch_xla' in sys.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xla_imported` is a utility method that is used to check if the `torch_xla` module has been successfully imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# fake out xla modules on environments not configured for TPU \n",
    "if not xla_imported():\n",
    "    from types import SimpleNamespace\n",
    "    def fake_metrics_report(*args,**kwargs):\n",
    "        return \"\"\n",
    "    met = SimpleNamespace(\n",
    "        metrics_report = fake_metrics_report\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "if xla_imported():\n",
    "    import torch_xla.debug.metrics as met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_aten_ops():\n",
    "    \"print out xla aten operations (from xla debug metrics report `torch_xla.debug.metrics`)\"\n",
    "    # import torch_xla.debug.metrics as met\n",
    "    from io import StringIO\n",
    "    import sys\n",
    "\n",
    "    class Capturing(list):\n",
    "        def __enter__(self):\n",
    "            self._stdout = sys.stdout\n",
    "            sys.stdout = self._stringio = StringIO()\n",
    "            return self\n",
    "        def __exit__(self, *args):\n",
    "            self.extend(self._stringio.getvalue().splitlines())\n",
    "            del self._stringio    # free up some memory\n",
    "            sys.stdout = self._stdout\n",
    "\n",
    "    out = met.metrics_report()\n",
    "    if out.find(\"aten::\"):\n",
    "        print_now=False\n",
    "        lines = out.split(\"\\n\")\n",
    "        for l in lines:\n",
    "            if print_now:\n",
    "                print_now=False\n",
    "                print(l)\n",
    "            if l.find(\"aten::\")>-1:\n",
    "                print(\"needs lowering:\", l)\n",
    "                print_now=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One of the problems we have hit testing different models and transforms is that sometimes it is slower on TPUs compared to running on CPUs, but this happens because we hit operations on Pytorch XLA that are only handled by the CPU and not by the accelerator. \n",
    "\n",
    "`print_aten_ops` calls directly some pytorch metrics which outputs to `stdout`, so the only way to get that info is capture it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "#test that torch_xla has been imported on colab\n",
    "assert xla_imported()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#TODO: Add example usage for print_aten_ops"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
