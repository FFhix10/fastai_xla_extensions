{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "rerun-working5-03_multi_core-pets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/butchland/fastai_xla_extensions/blob/master/nbs/03_multi_core.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftg-xxJvgxCR"
      },
      "source": [
        "#default_exp multi_core"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HdzqYFBgxCT"
      },
      "source": [
        "# Multi Core XLA extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibTpWSBfgxCa"
      },
      "source": [
        "## Setup torch XLA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV182O4ugxCa"
      },
      "source": [
        "This is the official way to install Pytorch-XLA 1.7 [instructions here](https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb#scrollTo=CHzziBW5AoZH)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O53lrJMDn9Rd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20552738-559b-4d7d-8ef5-7c74ff24c1c5"
      },
      "source": [
        "#colab\n",
        "!pip install -Uqq cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 133.6MB 29kB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 3.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsHVFGHSgxCY"
      },
      "source": [
        "## Install fastai\n",
        "\n",
        "Use latest fastai and fastcore versions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5brhMy3uzfS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03bc38c4-a6cd-47f8-b7ed-8e85ccd778c0"
      },
      "source": [
        "#colab\n",
        "# !pip install -Uqq git+https://github.com/fastai/fastai.git \n",
        "!pip install -Uqq fastai --upgrade"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 194kB 4.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 5.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HYizmSonC3r",
        "outputId": "e3b5d027-37da-4321-989f-6b56ef33b957"
      },
      "source": [
        "#colab\n",
        "!pip install -Uqq git+https://github.com/butchland/my_timesaver_utils.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for my-timesaver-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-maBHnlmDPVw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fbf530d-f488-4a00-fa06-950121dfeab9"
      },
      "source": [
        "#hide\n",
        "#colab\n",
        "!curl -s https://course19.fast.ai/setup/colab | bash"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updating fastai...\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfLJEMVZFS2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03e948d4-1f09-407b-d9df-77c8d60a8e7a"
      },
      "source": [
        "#hide\n",
        "!pip freeze | grep torch\n",
        "!pip freeze | grep fast"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch==1.7.0+cu101\n",
            "torch-xla==1.7\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.3.1\n",
            "torchvision==0.8.1+cu101\n",
            "fastai==2.2.5\n",
            "fastcore==1.3.19\n",
            "fastdtw==0.3.4\n",
            "fastprogress==1.0.0\n",
            "fastrlock==0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWkIiImyhW7Z"
      },
      "source": [
        "Start of kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvPuVkqphmoy",
        "outputId": "a6e57cfe-97c3-4f92-f7de-6b80ba60b201"
      },
      "source": [
        "#export\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:TPU has started up successfully with version pytorch-1.7\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iApl16-1tZst"
      },
      "source": [
        "#exporti\n",
        "from fastcore.basics import patch_to\n",
        "from fastai.optimizer import _BaseOptimizer\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch.utils.data as th_data\n",
        "from fastcore.foundation import L\n",
        "from pathlib import Path\n",
        "from fastcore.xtras import *\n",
        "from fastcore.transform import Pipeline\n",
        "from fastai.data.core import DataLoaders\n",
        "from functools import partial\n",
        "import torch.utils.data.distributed as torch_distrib\n",
        "from pathlib import Path\n",
        "import fastcore.xtras\n",
        "import math\n",
        "from fastcore.basics import store_attr\n",
        "from operator import attrgetter\n",
        "from fastai.data.load import _FakeLoader\n",
        "from fastai.data.core import TfmdDL\n",
        "from fastai.torch_core import find_bs, TensorBase\n",
        "import random\n",
        "import torch\n",
        "from fastai.data.load import _loaders\n",
        "from fastai.torch_core import to_device\n",
        "from fastcore.basics import first\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnDq_CqPgxCh"
      },
      "source": [
        "## Patching BaseOptimizer to be Pickable\n",
        "Patching Base Optimizer `__getstate__` and `__setstate__` whichi is used in pickling\n",
        "the optimizer which should fix the bug in running the learner in multiple TPU cores\n",
        "in XLA by which the  `def _fetch_gradients(optimizer)` in `for param_group in optimizer.__getstate__()['param_groups']:` fails, and this patch fixes the \"copy constructor\" to include the param_groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdRrHZuAhAbk"
      },
      "source": [
        "#export\n",
        "@patch_to(_BaseOptimizer)\n",
        "def __getstate__(self):\n",
        "    d = {\n",
        "            'state': self.state_dict(),\n",
        "            'param_groups': self.param_groups,\n",
        "        }\n",
        "    if hasattr(self,'defaults'): \n",
        "        d['defaults'] = self.defaults\n",
        "    return d\n",
        "\n",
        "@patch_to(_BaseOptimizer)\n",
        "def __setstate__(self, data):\n",
        "    if 'defaults' in data:\n",
        "        self.defaults = data['defaults']\n",
        "    self.load_state_dict(data['state'])\n",
        "    self.param_groups = data['param_groups']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmUS-lYC9Ea8"
      },
      "source": [
        "#export\n",
        "def _recast2tensor(o):\n",
        "    if isinstance(o,TensorBase):\n",
        "        # return plain tensor since pl.parallelloader doesn't\n",
        "        # seem to work with tensor subclasses\n",
        "        return torch.tensor(o.numpy())\n",
        "    return o\n",
        "\n",
        "def _round_to_multiple(number,multiple): \n",
        "    return int(math.ceil(number/multiple)*multiple)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7WJ3PkVLODO"
      },
      "source": [
        "#export\n",
        "class TPUDistributedDL(TfmdDL):\n",
        "    \"\"\"A `TfmdDL` which splits a batch into equal size pieces for each TPU core\n",
        "       It also recasts the output of a batch from a Tensorbase subclass to \n",
        "       a regular tensor since the XLA Parallel loader doesnt seem compatible\n",
        "       to it.\n",
        "    \"\"\"\n",
        "    _default = 'dl'\n",
        "    def __init__(self,dl,rank,world_size, seed=42):\n",
        "        store_attr()\n",
        "        self.bs,self.device,self.num_workers,self.drop_last,self.dataset,self.offs,fake = \\\n",
        "            attrgetter('bs','device','num_workers','drop_last','dataset','offs','fake_l')(dl)\n",
        "        self.fake_l = _FakeLoader(self, fake.pin_memory, fake.num_workers, fake.timeout, \n",
        "                                  persistent_workers=fake.persistent_workers)\n",
        "        self.epoch = 0\n",
        "        random.seed(self.seed)\n",
        "        self.dl.rng = random.Random(random.randint(0,2**32-1))\n",
        "        self.reset_rng()\n",
        "\n",
        "    def reset_rng(self):\n",
        "        random.seed(self.seed + self.epoch)\n",
        "        self.rng = random.Random(random.randint(0,2**32-1))\n",
        "\n",
        "    def __len__(self): \n",
        "        return _round_to_multiple(len(self.dl),self.world_size)//self.world_size\n",
        "\n",
        "    def set_epoch(self, epoch):\n",
        "        self.epoch = epoch\n",
        "\n",
        "    def get_idxs(self):\n",
        "        idxs = self.dl.get_idxs()\n",
        "        # do your own shuffling which factors in self.epoch + self.seed in\n",
        "        # generating a random sequence (underlying self.dl does not)\n",
        "        if self.shuffle: \n",
        "            idxs = self.shuffle_fn(idxs)\n",
        "        self.n = len(idxs)              \n",
        "        # we assumed n was dl.n but we really care about number of idxs\n",
        "        # add extra samples to make it evenly divisible\n",
        "        self.n_padded = _round_to_multiple(self.n,self.world_size)\n",
        "        idxs += (idxs * (self.n_padded//self.n))[:self.n_padded-self.n] \n",
        "        # idx needs to be repeated when n_padded>>n\n",
        "        # slice padded idxs so that each rank gets self.n_padded//self.world_size tensors\n",
        "        start_pos = self.rank*self.n_padded//self.world_size\n",
        "        end_pos = (self.rank+1)*self.n_padded//self.world_size\n",
        "        return idxs[start_pos:end_pos]\n",
        "\n",
        "    def before_iter(self):\n",
        "        self.dl.before_iter()\n",
        "\n",
        "    def randomize(self): \n",
        "        self.reset_rng()\n",
        "        self.dl.randomize()\n",
        "\n",
        "    def after_batch(self,b):\n",
        "        b = self.dl.after_batch(b)\n",
        "        # recast tensor subclasses to plain tensors\n",
        "        # undoing work of self.retain()\n",
        "        tb = [_recast2tensor(o) for o in b]\n",
        "        b = tuple(tb)\n",
        "        return b\n",
        "\n",
        "    def after_iter(self): \n",
        "        self.dl.after_iter()\n",
        "\n",
        "    def create_batches(self,samps): \n",
        "        return self.dl.create_batches(samps)\n",
        "\n",
        "    def to(self, device):\n",
        "        self.dl.device = device\n",
        "        self.device = device\n",
        "        return self\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyhvUv_p1zkC"
      },
      "source": [
        "#exporti\n",
        "from fastai.torch_core import default_device, apply\n",
        "import torch \n",
        "from fastcore.xtras import is_listy\n",
        "import torch\n",
        "import torch.utils.hooks\n",
        "from fastcore.basics import patch\n",
        "from fastai.torch_core import TensorBase\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq5Ix_qet7cX"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PugEC6aQt_Wn"
      },
      "source": [
        "#exporti\n",
        "from fastcore.basics import patch_to\n",
        "import torch.utils.data.distributed as th_distrib\n",
        "import torch.utils.data as th_data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgyHxWMruI15"
      },
      "source": [
        "#export\n",
        "class TfmdTorchDS(th_data.Dataset):\n",
        "    def __init__(self, items, x_tfm=None, y_tfm=None):\n",
        "        self.items = items\n",
        "        self.x_tfm = x_tfm\n",
        "        self.y_tfm = y_tfm\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.items[index]\n",
        "        x = self.x_tfm(item) if self.x_tfm is not None else x\n",
        "        y = self.y_tfm(item) if self.y_tfm is not None else y\n",
        "        return (x,y)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvREsA2huL85"
      },
      "source": [
        "#exporti\n",
        "from fastcore.xtras import is_listy\n",
        "import torchvision as thv\n",
        "from operator import itemgetter\n",
        "from fastcore.imports import noop\n",
        "from fastcore.foundation import L"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_yjqBvnuVFh"
      },
      "source": [
        "#export\n",
        "def to_list(o):\n",
        "    return [] if o is None else [o] if not is_listy(o) else o\n",
        "\n",
        "def has_setup(tfms):\n",
        "    \"\"\"returns last index if at least 1 `tfm` in `tfms` has a method `setup` else return -1\"\"\"\n",
        "    setups = L(tfms).attrgot('setup',None).argwhere(noop) # get indexes where tfm has `setup` attribute\n",
        "    return -1 if len(setups) == 0 else setups[-1]\n",
        "\n",
        "def run_setups(tfms, items):\n",
        "    \"\"\"run tfm setups including tfm for all items\"\"\"\n",
        "    indx = has_setup(tfms)\n",
        "    if indx == -1: # no setup found\n",
        "        return\n",
        "\n",
        "    for i,tfm in enumerate(tfms):\n",
        "        if hasattr(tfm,'setup'):\n",
        "            tfm.setup(items)\n",
        "        if i < indx:\n",
        "            # tfm items to be fed into next tfm\n",
        "            items = [tfm(item) for item in items]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maFG_ofSuSF2"
      },
      "source": [
        "#export\n",
        "class TorchDatasetBuilder:\n",
        "    def __init__(self, source, get_items, splitter,\n",
        "                x_tfms, y_tfms,\n",
        "                x_type_tfms=None,\n",
        "                x_train_tfms=None, x_test_tfms=None,\n",
        "                do_setup=False):\n",
        "        self.source = source\n",
        "        self.get_items = get_items\n",
        "        self.splitter = splitter\n",
        "        self.do_setup = do_setup\n",
        "        self.x_tfms = to_list(x_tfms)\n",
        "        self.y_tfms = to_list(y_tfms)\n",
        "        self.x_type_tfms = to_list(x_type_tfms)\n",
        "        self.x_train_tfms = to_list(x_train_tfms)\n",
        "        self.x_test_tfms = to_list(x_test_tfms)\n",
        "\n",
        "    def setup(self, items, do_setup=None, setup_x=False):\n",
        "        self.do_setup = do_setup if do_setup is not None else self.do_setup\n",
        "        if self.do_setup:\n",
        "            all_x_tfms = [*self.x_type_tfms, *self.x_train_tfms, *self.x_tfms]\n",
        "            if setup_x:\n",
        "                run_setups(all_x_tfms, items)\n",
        "            run_setups(self.y_tfms, items)\n",
        "            self.do_setup = False\n",
        "\n",
        "    def get_datasets(self, do_setup=None):\n",
        "        self.do_setup = do_setup if do_setup is not None else self.do_setup\n",
        "        items = self.get_items(self.source)\n",
        "        train_idxs, test_idxs = self.splitter(items)\n",
        "\n",
        "        train_items = itemgetter(*train_idxs)(items)\n",
        "        test_items = itemgetter(*test_idxs)(items)\n",
        "        self.setup(train_items)\n",
        "        allx_test_tfms = [*self.x_type_tfms, *self.x_test_tfms, *self.x_tfms]\n",
        "        allx_train_tfms = [*self.x_type_tfms, *self.x_train_tfms, *self.x_tfms]\n",
        "        train_x_tfm = thv.transforms.Compose(allx_train_tfms)\n",
        "        test_x_tfm = thv.transforms.Compose(allx_test_tfms)\n",
        "        y_tfm = thv.transforms.Compose(self.y_tfms)\n",
        "        train_ds = TfmdTorchDS(train_items, x_tfm=train_x_tfm, y_tfm=y_tfm)\n",
        "        test_ds = TfmdTorchDS(test_items, x_tfm=test_x_tfm, y_tfm=y_tfm)\n",
        "        return train_ds, test_ds"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Uopgt8nugIu"
      },
      "source": [
        "#export\n",
        "from fastai.data.transforms import CategoryMap\n",
        "\n",
        "class VocabularyMapper:\n",
        "    \"\"\"A simplified version of the fastai Categorize Transform\"\"\"\n",
        "    def __init__(self, vocab=None):\n",
        "        self.vocab = vocab\n",
        "        self.c = 0\n",
        "    def setup(self, items):\n",
        "        self.vocab = CategoryMap(items)\n",
        "        self.c = len(self.vocab)\n",
        "    def __call__(self, o):\n",
        "        if self.vocab is None: return o\n",
        "        try:\n",
        "            return torch.tensor(self.vocab.o2i[o])\n",
        "        except KeyError as e:\n",
        "            raise KeyError(f\"Label '{o}' was not included in the training dataset\") from e\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH3fz-rnulz4"
      },
      "source": [
        "import torchvision as thv\n",
        "\n",
        "pil2tensor = thv.transforms.ToTensor()\n",
        "resize28 = thv.transforms.Resize(28)\n",
        "norm = thv.transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
        "\n",
        "from fastai.vision.core import PILImage\n",
        "from fastai.data.transforms import get_image_files, GrandparentSplitter, parent_label\n",
        "from fastai.data.external import untar_data, URLs\n",
        "\n",
        "path = untar_data(URLs.MNIST_TINY)\n",
        "mnist_dset_builder =  TorchDatasetBuilder(\n",
        "                source=path, \n",
        "                get_items=get_image_files, \n",
        "                splitter=GrandparentSplitter(),\n",
        "                x_tfms=[resize28,pil2tensor,norm,], \n",
        "                y_tfms=[parent_label,VocabularyMapper(),],\n",
        "                x_type_tfms=PILImage.create)\n",
        "\n",
        "from fastcore.test import test_eq\n",
        "\n",
        "train_ds, test_ds = mnist_dset_builder.get_datasets(do_setup=True)\n",
        "\n",
        "test_eq(len(train_ds),709)\n",
        "test_eq(len(test_ds),699)\n",
        "test_eq(mnist_dset_builder.y_tfms[1].vocab, ('3','7'))\n",
        "test_eq(mnist_dset_builder.y_tfms[1].c, 2)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4B3fycqurf5"
      },
      "source": [
        "#export\n",
        "@patch_to(th_data.DataLoader)\n",
        "def to(self, device):\n",
        "    self.device = device"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTqdDgrWuuUu"
      },
      "source": [
        "#export\n",
        "def make_torch_dataloaders(train_dataset, test_dataset,\n",
        "                     rank,\n",
        "                     world_size,\n",
        "                     bs,\n",
        "                     num_workers=4,\n",
        "                     distrib=True,\n",
        "                     sync_valid=False):\n",
        "    if distrib:\n",
        "        train_sampler = th_distrib.DistributedSampler(\n",
        "            train_dataset,\n",
        "            num_replicas=world_size,\n",
        "            rank=rank,\n",
        "            shuffle=True)\n",
        "        train_loader = th_data.DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=bs,\n",
        "            sampler=train_sampler,\n",
        "            # shuffle=True,\n",
        "            num_workers=num_workers,\n",
        "            drop_last=True)\n",
        "        \n",
        "        if sync_valid:\n",
        "            test_sampler = th_distrib.DistributedSampler(\n",
        "                test_dataset,\n",
        "                num_replicas=world_size,\n",
        "                rank=rank,\n",
        "                shuffle=False)\n",
        "\n",
        "            test_loader = th_data.DataLoader(\n",
        "                test_dataset,\n",
        "                batch_size=bs,\n",
        "                sampler=test_sampler,\n",
        "                # shuffle=False,\n",
        "                num_workers=num_workers,\n",
        "                drop_last=True)\n",
        "        else:\n",
        "            test_loader = th_data.DataLoader(\n",
        "                test_dataset,\n",
        "                batch_size=bs,\n",
        "                shuffle=False,\n",
        "                num_workers=num_workers,\n",
        "                drop_last=True)\n",
        "\n",
        "    else:\n",
        "        train_loader = th_data.DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=bs,\n",
        "            # sampler=train_sampler,\n",
        "            shuffle=True,\n",
        "            num_workers=num_workers,\n",
        "            drop_last=True)\n",
        "\n",
        "        test_loader = th_data.DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=bs,\n",
        "            shuffle=False,\n",
        "            num_workers=num_workers,\n",
        "            drop_last=True)\n",
        "    dataloaders = DataLoaders(train_loader, test_loader, device=None)\n",
        "    return dataloaders"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7PRqbPj_m-4"
      },
      "source": [
        "#exporti\n",
        "import re"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DGacnwo_GHB"
      },
      "source": [
        "#export\n",
        "class FileNamePatternLabeller:\n",
        "    \"Delayed action version of fastai RegexLabeller with file name selection\"\n",
        "    def __init__(self, pat_str, match=False):\n",
        "        self.pat_str = pat_str\n",
        "        self.match = match\n",
        "        self.matcher = None\n",
        "        self.pat = None\n",
        "    def __call__(self, f):\n",
        "        if isinstance(f,str):\n",
        "            f = Path(f)\n",
        "        o = f.name\n",
        "        if self.pat is None:\n",
        "            self.pat = re.compile(self.pat_str)\n",
        "            self.matcher = self.pat.match if self.match else self.pat.search\n",
        "        res  = self.matcher(o)\n",
        "        assert res, f'Failed to find \"{self.pat}\" in {o}'\n",
        "        return res.group(1)\n",
        "            "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RStThuhf8rxb"
      },
      "source": [
        "#export\n",
        "def make_distributed_dataloaders(dls, rank, world_size, sync_valid=False):\n",
        "    \"\"\"Wrap dataloaders with distributed TPU aware dataloader \"\"\"\n",
        "    new_loaders = []\n",
        "    for i,dl in enumerate(dls.loaders):\n",
        "        if i == 0 or sync_valid:\n",
        "            use_rank = rank\n",
        "            use_size = world_size\n",
        "        else: \n",
        "            use_rank = 0\n",
        "            use_size = 1         \n",
        "        dl = TPUDistributedDL(dl,\n",
        "                            rank=use_rank, \n",
        "                            world_size=use_size)\n",
        "        new_loaders += [dl]\n",
        "    return DataLoaders(*new_loaders, path=dls.path, device=dls.device)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLSy5nkmc2KT"
      },
      "source": [
        "#export\n",
        "# def DataBlock.dataloaders(self, source, path='.', verbose=False, **kwargs):\n",
        "def make_fastai_dataloaders(datablock, source, rank, world_size, device=None, path='.', sync_valid=False, verbose=False,**kwargs):\n",
        "    dls = datablock.dataloaders(source=source, path=path, device=device, **kwargs)\n",
        "    distrib_dls = make_distributed_dataloaders(dls, rank, world_size, sync_valid=sync_valid)\n",
        "    return distrib_dls\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfcvERSmoW3n"
      },
      "source": [
        "#export\n",
        "def wrap_parallel_loader(loader, device):\n",
        "    para_loader = pl.ParallelLoader(loader, [device])\n",
        "    loop_loader = para_loader.per_device_loader(device)\n",
        "    return loop_loader"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C81ZkqX-KnZ"
      },
      "source": [
        "#exporti\n",
        "from fastai.callback.core import TrainEvalCallback\n",
        "from fastai.learner import Recorder\n",
        "from fastai.torch_core import one_param\n",
        "import torch\n",
        "from fastai.callback.core import Callback\n",
        "from fastai.learner import CancelTrainException, CancelValidException, CancelStepException\n",
        "from fastai.torch_core import tensor, TensorCategory"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQN6muqukJnE"
      },
      "source": [
        "#export\n",
        "class XLATrainingCallback(Callback):\n",
        "    run_before = Recorder\n",
        "    run_valid = False\n",
        "    order = -5 # after TrainEvalCallback \n",
        "    def __init__(self, device, rank=0, sync_valid=False):\n",
        "        self.pdevice = device\n",
        "        self.rank = rank\n",
        "        self.sync_valid = sync_valid\n",
        "\n",
        "    def before_fit(self):\n",
        "       xm.master_print('start fit')\n",
        "\n",
        "    def before_epoch(self):\n",
        "        # set the epoch on train only to make sure shuffle produces same seq \n",
        "        # across all ranks\n",
        "        if hasattr(self.learn.dls.train,'sampler'):\n",
        "            if hasattr(self.learn.dls.train.sampler,'set_epoch'):\n",
        "                self.learn.dls.train.sampler.set_epoch(self.learn.epoch) \n",
        "        elif hasattr(self.learn.dls.train,'set_epoch'):\n",
        "            self.learn.dls.train.set_epoch(self.learn.epoch)\n",
        "\n",
        "        if self.sync_valid: # update epoch on valid if sync_valid\n",
        "            if hasattr(self.learn.dls.valid,'sampler'):\n",
        "                if hasattr(self.learn.dls.valid.sampler,'set_epoch'):\n",
        "                    self.learn.dls.valid.sampler.set_epoch(self.learn.epoch) \n",
        "            elif hasattr(self.learn.dls.valid,'set_epoch'):\n",
        "                self.learn.dls.valid.set_epoch(self.learn.epoch)\n",
        "\n",
        "    def before_train(self):\n",
        "        self.learn.dl = wrap_parallel_loader(self.dls.train, self.pdevice)\n",
        "\n",
        "    def before_validate(self):\n",
        "        \"Set the model in validation mode\"\n",
        "        if self.rank != 0 and not self.sync_valid: \n",
        "        # no need to compute valid loss/ metric if not master if not sync valid\n",
        "            raise CancelValidException()    \n",
        "        self.learn.dl = wrap_parallel_loader(self.dls.valid, self.pdevice)\n",
        "\n",
        "    def before_step(self):\n",
        "        raise CancelStepException()\n",
        "\n",
        "    def after_cancel_step(self):\n",
        "        xm.optimizer_step(self.learn.opt)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QO1qlyiOEPf"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwcUJUXQkv1C"
      },
      "source": [
        "#exporti\n",
        "import copy\n",
        "from fastcore.imports import noop\n",
        "from fastcore.foundation import L\n",
        "from fastai.learner import Metric, AvgMetric, AvgLoss, AvgSmoothLoss\n",
        "import torch\n",
        "import pickle\n",
        "from fastai.torch_core import find_bs, to_detach"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdxx3hCglJsh"
      },
      "source": [
        "#export\n",
        "\n",
        "@patch\n",
        "def update_metric(self:Metric, other_metrics):\n",
        "    # dunno how to handle updates for metrics other than AvgMetric, AvgLoss\n",
        "    pass\n",
        "\n",
        "@patch\n",
        "def update_metric(self:(AvgMetric,AvgLoss), other_metrics):\n",
        "    other_metrics = L(other_metrics)\n",
        "    # other metrics must also be AvgMetric or AvgLoss\n",
        "    assert len(other_metrics.map(lambda o: not isinstance(o, (AvgLoss,AvgMetric))).argwhere(noop)) == 0\n",
        "    # other metrics must have same name\n",
        "    assert len(other_metrics.attrgot('name').map(lambda o: o != self.name).argwhere(noop)) == 0\n",
        "    self.total = other_metrics.attrgot('total').sum()\n",
        "    self.count = other_metrics.attrgot('count').sum()\n",
        "\n",
        "def unpack_sync(res):\n",
        "    return [pickle.loads(o) for o in res]\n",
        "        "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jip5cOFdlxQ1"
      },
      "source": [
        "#exporti\n",
        "from fastai.learner import _maybe_item\n",
        "from fastprogress.fastprogress import format_time\n",
        "import time\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yujKicr3lzBv"
      },
      "source": [
        "#export\n",
        "class SyncRecorderCallback(Callback):\n",
        "    \"\"\"Sync metrics from each spawned process update statistics \n",
        "       accordingly so it will display correctly in the progress callback\n",
        "    \"\"\"\n",
        "    order  = 55 # after Recorder, before ProgressCallback\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def before_fit(self):       \n",
        "        if not xm.is_master_ordinal():\n",
        "            return\n",
        "        if 'progress' in self.learn.cbs.attrgot('name',None):\n",
        "            self._sync_stats_log = self.progress._write_stats\n",
        "        else:\n",
        "            self._sync_stats_log = self.learn.logger\n",
        "        \n",
        "    def after_fit(self):\n",
        "        xm.rendezvous('sync recorder after_fit')        \n",
        "\n",
        "    def before_epoch(self):\n",
        "        self.sync_log = copy.copy(self.recorder.log)\n",
        "    \n",
        "    def after_epoch(self):\n",
        "        if 'recorder' not in self.learn.cbs.attrgot('name'):\n",
        "            all_metrics = {\n",
        "                'train_mets': L([]),\n",
        "                'valid_mets': L([]),\n",
        "            }\n",
        "        else:\n",
        "            all_metrics = {\n",
        "                'train_mets': self.recorder._train_mets, \n",
        "                'valid_mets': self.recorder._valid_mets,\n",
        "            }\n",
        "        # send metrics data to sync ranks across spawned processes     \n",
        "        sync_tag = f'sync_recorder_after_epoch{self.learn.epoch}'\n",
        "        res = xm.rendezvous(sync_tag, pickle.dumps(all_metrics))\n",
        "        \n",
        "        if xm.is_master_ordinal():\n",
        "            all_metrics = unpack_sync(res)\n",
        "            self._sync_log(all_metrics) # use metrics across ranks to update log\n",
        "            \n",
        "            self.learn.final_record = self.sync_log[:1].copy()\n",
        "            del self.recorder.values[-1] # remove last entry added by recorder\n",
        "            self.recorder.values.append(self.learn.final_record) # add updated metrics\n",
        "            if self.recorder.add_time:\n",
        "                updated_time = (time.time() - self.recorder.start_epoch)\n",
        "                self.sync_log.append(format_time(updated_time))\n",
        "            self.recorder.log = self.sync_log\n",
        "            self._sync_stats_log(self.sync_log) # write_stats to output\n",
        "            self.learn.logger = self.orig_logger # restore orig logger after skipping recorder.logger(log) \n",
        "            \n",
        "       \n",
        "    def before_validate(self):\n",
        "        pass\n",
        "    \n",
        "    def after_validate(self):\n",
        "        if xm.is_master_ordinal():\n",
        "            self.orig_logger = self.learn.logger\n",
        "            self.learn.logger = noop # write to logger disabled so calling recorder.logger(log) wont print\n",
        "        pass\n",
        "    \n",
        "    def before_batch(self):\n",
        "        pass\n",
        "    \n",
        "    def _sync_log(self, all_metrics):\n",
        "        all_metrics = L(all_metrics)\n",
        "        \n",
        "        for i,m in enumerate(self.recorder._train_mets):\n",
        "            m.update_metric(all_metrics.attrgot('train_mets').itemgot(i))\n",
        "            self.sync_log += _maybe_item(m)\n",
        "            \n",
        "        for i,m in enumerate(self.recorder._valid_mets):\n",
        "            m.update_metric(all_metrics.attrgot('valid_mets').itemgot(i))\n",
        "            self.sync_log += _maybe_item(m)   \n",
        "        \n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v5-GB25_Y_b"
      },
      "source": [
        "#exporti\n",
        "from fastcore.imports import noop\n",
        "from fastcore.basics import patch\n",
        "from fastai.learner import Learner\n",
        "from fastai.callback.progress import ProgressCallback\n",
        "from fastcore.xtras import join_path_file\n",
        "from fastai.torch_core import get_model\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgsk9Amvihw4"
      },
      "source": [
        "#export\n",
        "@patch\n",
        "def save(self:Learner, file, **kwargs):\n",
        "    file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n",
        "    with_opt = self.opt is not None  \n",
        "    state = self.model.state_dict()\n",
        "    if with_opt:\n",
        "        # add opt state to state to be saved\n",
        "        opt_state = self.opt.state_dict() \n",
        "        state = {'model': state, 'opt':opt_state}\n",
        "    xm.save(state, file) # use xm.save instead of torch.save\n",
        "    return file\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLU7Y-X97TxR"
      },
      "source": [
        "#export\n",
        "@patch\n",
        "def to_xla(self:Learner,device, rank, sync_valid=False):\n",
        "    if 'xla_training' not in self.cbs.attrgot('name'):\n",
        "        self.dls.device = None\n",
        "        self.add_cbs(XLATrainingCallback(device, rank, sync_valid=sync_valid))\n",
        "    else:\n",
        "        self.xla_training.pdevice = device\n",
        "        self.xla_training.rank = rank\n",
        "        self.xla_training.sync_valid = sync_valid\n",
        "\n",
        "    if sync_valid and 'sync_recorder' not in self.cbs.attrgot('name'):\n",
        "        self.add_cbs(SyncRecorderCallback)\n",
        "    elif not sync_valid:\n",
        "        self.remove_cbs(SyncRecorderCallback)\n",
        "\n",
        "    if rank != 0: # progress bar only for rank 0\n",
        "        self.remove_cbs(ProgressCallback)\n",
        "    self.logger = xm.master_print"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIkMAXsQ9jhO"
      },
      "source": [
        "#export\n",
        "# for testing\n",
        "def do_one_loop(dls, rank, world_size, device, sync_valid, is_train=True):\n",
        "    if is_train:\n",
        "        dl = dls.train\n",
        "    else:\n",
        "        dl = dls.valid\n",
        "\n",
        "    n_batches = len(dl)\n",
        "    print(f'xla: {rank} world_size: {world_size} train:{is_train} n_batches:{n_batches} sync_valid: {sync_valid}')\n",
        "\n",
        "    if sync_valid or is_train or rank == 0:\n",
        "        print(f'xla: {rank} wrapping ploader')\n",
        "        pdl = wrap_parallel_loader(dl, device=device)\n",
        "    for i,b in enumerate(pdl):\n",
        "        if i > 1:\n",
        "            break\n",
        "        xb, yb = b\n",
        "        print(f'xla: {rank} iter:{i} xb.shape {xb.shape} yb.shape: {yb.shape}')        \n",
        "        print(f'xla: {rank} iter:{i} xb.device {xb.device} yb.device: {yb.device}')        \n",
        "        print(f'xla: {rank} iter:{i} xb.dtype {xb.dtype} yb.device: {yb.dtype}')        \n",
        "        \n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkCSxNkuwodp"
      },
      "source": [
        "## Test out the code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIYME3Bz-Knc"
      },
      "source": [
        "from functools import partial\n",
        "from fastai.metrics import accuracy\n",
        "from fastai.optimizer import SGD, Adam\n",
        "\n",
        "from fastcore.basics import first\n",
        "from fastai.callback.schedule import *\n",
        "from fastai.test_utils import VerboseCallback\n",
        "from my_timesaver_utils.profiling import *\n",
        "from my_timesaver_utils.profiling_callback import *"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvrYtVMss5ou"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruOsGucus8up"
      },
      "source": [
        "def train_torch_model(rank):\n",
        "    torch.manual_seed(1)\n",
        "    xm.rendezvous('start_train_torch_model')\n",
        "    # Scale learning rate to num cores\n",
        "    learning_rate = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
        "    IS_PROFILING = FLAGS['is_profiling']\n",
        "    SYNC_VALID = FLAGS['sync_valid']\n",
        "\n",
        "    # Get loss function, optimizer, and model\n",
        "    device = xm.xla_device()\n",
        "    model = WRAPPED_MODEL.to(device)\n",
        "    bs = FLAGS['batch_size']\n",
        "    world_size = xm.xrt_world_size()\n",
        "    moms =(FLAGS['momentum'],FLAGS['momentum'],FLAGS['momentum'])\n",
        "    wd = FLAGS['weight_decay']\n",
        "    num_workers = FLAGS['num_workers']\n",
        "\n",
        "    if IS_PROFILING:\n",
        "        rec_name = 'rank' + str(rank) + '_dset_build'\n",
        "        print(f'start {rec_name}')\n",
        "        start_record(rec_name)\n",
        "    dsets = DSET_BUILDER.get_datasets()\n",
        "    if IS_PROFILING:\n",
        "        end_record(rec_name)\n",
        "        print_prof_data(rec_name)\n",
        "        print(f'finished {rec_name}')\n",
        "\n",
        "    if IS_PROFILING:\n",
        "        rec_name2 = 'rank' + str(rank) + '_dataloader_build'\n",
        "        print(f'start {rec_name2}')\n",
        "        start_record(rec_name2)\n",
        "    dls = make_torch_dataloaders(*dsets, \n",
        "                                  rank=rank, \n",
        "                                  world_size=world_size, \n",
        "                                  bs=bs,\n",
        "                                  num_workers=num_workers,\n",
        "                                  sync_valid=SYNC_VALID,\n",
        "                                 )\n",
        "\n",
        "    if IS_PROFILING:\n",
        "        end_record(rec_name2)\n",
        "        print_prof_data(rec_name2)\n",
        "        print(f'finished {rec_name2}')\n",
        "\n",
        "    # do_one_loop(dls,rank,world_size,device,sync_valid=SYNC_VALID, is_train=True)\n",
        "    # do_one_loop(dls,rank,world_size,device,sync_valid=SYNC_VALID, is_train=False)\n",
        "\n",
        "    xm.master_print('build learner')\n",
        "    learner = Learner(dls, model, \n",
        "                      loss_func=LOSS_FUNC, \n",
        "                      opt_func=OPT_FUNC, \n",
        "                      metrics=accuracy, \n",
        "                      wd=wd,\n",
        "                      moms=moms\n",
        "                      )\n",
        "                      \n",
        "    learner.to_xla(device, rank=xm.get_ordinal(), sync_valid=SYNC_VALID)\n",
        "    if rank == 0:\n",
        "        learner.to_my_profile()\n",
        "                               \n",
        "    epochs = FLAGS['num_epochs']\n",
        "    xm.master_print('start running fit')\n",
        "    learner.unfreeze()\n",
        "\n",
        "    if IS_PROFILING:\n",
        "        rec_name3 = 'rank' + str(rank) + '_run_fit'\n",
        "        print(f'start {rec_name3}')\n",
        "        start_record(rec_name3)\n",
        "    learner.fit_one_cycle(epochs, lr_max=slice(learning_rate/10))\n",
        "\n",
        "    if IS_PROFILING:\n",
        "        end_record(rec_name3)\n",
        "        print_prof_data(rec_name3)\n",
        "        print(f'finished {rec_name3}')\n",
        "\n",
        "    learner.save('stage-1')\n",
        "    if rank == 0:\n",
        "        learner.my_profile.print_stats()\n",
        "    xm.mark_step() \n",
        "    xm.rendezvous('end_train_torch_model')\n",
        "    if IS_PROFILING:\n",
        "        clear_prof_data() \n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJK9oFg4MVAi"
      },
      "source": [
        "def train_model(rank):\n",
        "    torch.manual_seed(1)\n",
        "    xm.rendezvous('start_train_model')\n",
        "    print(f'xla {rank} start train model')\n",
        "\n",
        "    # Scale learning rate to num cores\n",
        "    learning_rate = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
        "    SYNC_VALID = FLAGS['sync_valid']\n",
        "    IS_PROFILING = FLAGS['is_profiling']\n",
        "    # Get loss function, optimizer, and model\n",
        "    device = xm.xla_device()\n",
        "    model = WRAPPED_MODEL.to(device)\n",
        "    bs = FLAGS['batch_size']\n",
        "    moms =(FLAGS['momentum'],FLAGS['momentum'],FLAGS['momentum'])\n",
        "    wd = FLAGS['weight_decay']\n",
        "\n",
        "    world_size = xm.xrt_world_size()\n",
        "    if IS_PROFILING:\n",
        "        rec_name = 'rank' + str(rank) + '_dataloader_build'\n",
        "        print(f'start {rec_name}')\n",
        "        start_record(rec_name)\n",
        "\n",
        "    dls = make_fastai_dataloaders(\n",
        "                            DATA, \n",
        "                            PATH, \n",
        "                            rank=rank, \n",
        "                            world_size=world_size, \n",
        "                            sync_valid=SYNC_VALID,\n",
        "                            bs=bs,)\n",
        "    if IS_PROFILING:\n",
        "        end_record(rec_name)\n",
        "        print_prof_data(rec_name)\n",
        "        print(f'finished {rec_name}')\n",
        "\n",
        "    xm.master_print('build learner')\n",
        "    learner = Learner(dls, model, \n",
        "                      loss_func=LOSS_FUNC, \n",
        "                      opt_func=OPT_FUNC, \n",
        "                      metrics=accuracy, \n",
        "                      wd=wd,\n",
        "                      moms=moms)\n",
        "                      \n",
        "    learner.to_xla(device, rank=xm.get_ordinal(), sync_valid=SYNC_VALID)\n",
        "    if rank == 0:\n",
        "        learner.to_my_profile()\n",
        "                               \n",
        "    epochs = FLAGS['num_epochs']\n",
        "    xm.master_print('start running fit')\n",
        "    learner.unfreeze()\n",
        "    if IS_PROFILING:\n",
        "        rec_name3 = 'rank' + str(rank) + '_run_fit'\n",
        "        print(f'start {rec_name3}')\n",
        "        start_record(rec_name3)\n",
        "\n",
        "    learner.fit_one_cycle(epochs, lr_max=slice(learning_rate/10))\n",
        "    if IS_PROFILING:\n",
        "        end_record(rec_name3)\n",
        "        print_prof_data(rec_name3)\n",
        "        print(f'finished {rec_name3}')\n",
        "\n",
        "    learner.save('stage-1')\n",
        "    if rank == 0:\n",
        "        learner.my_profile.print_stats()\n",
        "    xm.mark_step()  \n",
        "    xm.rendezvous('end_train_model')\n",
        "    if IS_PROFILING:\n",
        "        clear_prof_data() \n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2nL4HmloEyl"
      },
      "source": [
        "# Start training processes\n",
        "def _mp_fn(rank, flags):\n",
        "    global FLAGS\n",
        "    FLAGS = flags\n",
        "    train_model(rank)\n",
        "\n",
        "def _mp_fn2(rank, flags):\n",
        "    global FLAGS\n",
        "    FLAGS = flags\n",
        "    train_torch_model(rank)\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxafi-nogR0P"
      },
      "source": [
        "import torch\n",
        "from fastcore.transform import DisplayedTransform, Transform\n",
        "from fastcore.basics import store_attr\n",
        "from fastai.vision.core import PILImage, PILBase, image2tensor\n",
        "from fastai.data.block import TransformBlock"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdfCw9OWsgEX"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBaA-8DyVV3_"
      },
      "source": [
        "from fastai.data.transforms import get_c\n",
        "# from fastai.vision.all import *\n",
        "from fastai.data.block import DataBlock, CategoryBlock\n",
        "from fastai.vision.data import ImageBlock\n",
        "from fastai.data.transforms import get_image_files, parent_label, GrandparentSplitter\n",
        "from fastai.vision.augment import Resize, aug_transforms\n",
        "from fastai.data.external import untar_data, URLs\n",
        "from fastai.data.transforms import Normalize\n",
        "from fastai.vision.core import imagenet_stats"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pxqjcGUo8p2"
      },
      "source": [
        "LOSS_FUNC = nn.CrossEntropyLoss()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23CfyVhuhswS"
      },
      "source": [
        "OPT_FUNC = Adam"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHFYqsxF6-sc"
      },
      "source": [
        "from fastai.data.transforms import RandomSplitter"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgh0Q0h3L5oj"
      },
      "source": [
        "from fastai.vision.learner import create_cnn_model\n",
        "from fastai.vision.models import resnet34"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWMc8rT7j8p6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSRaAgHFth2X"
      },
      "source": [
        "import os\n",
        "# Define Parameters\n",
        "FLAGS = {}\n",
        "# FLAGS['batch_size'] = 1024\n",
        "FLAGS['sync_valid'] = True\n",
        "FLAGS['is_profiling'] = False\n",
        "FLAGS['batch_size'] = 64\n",
        "FLAGS['num_workers'] = 4\n",
        "FLAGS['learning_rate'] = 5e-3\n",
        "FLAGS['image_size'] = 224\n",
        "FLAGS['momentum'] = 0.85\n",
        "FLAGS['weight_decay'] = 2e-4\n",
        "FLAGS['num_epochs'] = 20\n",
        "FLAGS['num_cores'] = 8 if os.environ.get('TPU_NAME', None) else 1\n",
        "# FLAGS['num_cores'] = 1 \n",
        "ARCH = resnet34\n",
        "USE_DBLOCK = False"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKcfNzyUhMKX"
      },
      "source": [
        "from pathlib import Path\n",
        "from fastcore.xtras import *\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19o6dPgMHbSg"
      },
      "source": [
        "PATH = untar_data(URLs.PETS)/'images'\n",
        "# PATH = untar_data(URLs.MNIST)\n",
        "# PATH = untar_data(URLs.MNIST_TINY)\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeoBGaxRWvQK"
      },
      "source": [
        "if USE_DBLOCK:\n",
        "    pat = r'(.+)_\\d+.jpg$'\n",
        "    fname_labeller = FileNamePatternLabeller(pat)\n",
        "    splitter=RandomSplitter(seed=42)\n",
        "    DATA = DataBlock(\n",
        "        blocks=(ImageBlock, CategoryBlock),\n",
        "        get_items=get_image_files,\n",
        "        get_y=fname_labeller,\n",
        "        splitter=splitter,\n",
        "        item_tfms=[Resize(FLAGS['image_size']),],\n",
        "        batch_tfms=[]\n",
        "    )\n",
        "    vocab = CategoryMap(get_image_files(PATH).map(fname_labeller))\n",
        "    N_OUT = len(vocab)\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi0nP_NQv_si"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGtI9W3Ox60a",
        "outputId": "6d7a4a1a-861e-40a1-c5c7-bc36a55f0c7c"
      },
      "source": [
        "if not USE_DBLOCK:\n",
        "    imagenet_norm = thv.transforms.Normalize(\n",
        "        mean=(0.485, 0.456, 0.406), \n",
        "        std=(0.229, 0.224, 0.225))\n",
        "\n",
        "    cifar_norm = thv.transforms.Normalize(\n",
        "        mean=(0.4914, 0.4822, 0.4465), \n",
        "        std=(0.2023, 0.1994, 0.2010))\n",
        "\n",
        "    image_size = FLAGS['image_size']\n",
        "    splitter = RandomSplitter(seed=42)\n",
        "    pat = r'(.+)_\\d+.jpg$'\n",
        "    fname_labeller = FileNamePatternLabeller(pat)\n",
        "\n",
        "    DSET_BUILDER = TorchDatasetBuilder(\n",
        "        PATH, \n",
        "        get_items=get_image_files,\n",
        "        splitter=splitter,\n",
        "        x_tfms=[thv.transforms.Resize((image_size,image_size)), thv.transforms.ToTensor(), imagenet_norm],\n",
        "        y_tfms=[fname_labeller, VocabularyMapper(),],\n",
        "        x_type_tfms=PILImage.create,\n",
        "    ) \n",
        "    start_record('master_vocab_setup')\n",
        "    DSET_BUILDER.setup(get_image_files(PATH),do_setup=True)\n",
        "    end_record('master_vocab_setup')\n",
        "    print_prof_data('master_vocab_setup')\n",
        "    clear_prof_data()\n",
        "    N_OUT = DSET_BUILDER.y_tfms[1].c     "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Function master_vocab_setup called 1 times.\n",
            "Execution time max: 0.131, average: 0.131\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AY8BvR8zYhX"
      },
      "source": [
        "assert N_OUT is not None and N_OUT > 0,f'N_OUT {N_OUT} should be > 0'"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbwZiK1wHXCR"
      },
      "source": [
        "\n",
        "custom_model = create_cnn_model(ARCH, N_OUT, \n",
        "                                pretrained=True,\n",
        "                                concat_pool=False)\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp8h1S0U7Jqs"
      },
      "source": [
        "# Only instantiate model weights once in memory.\n",
        "WRAPPED_MODEL = xmp.MpModelWrapper(custom_model)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz4JUeQLtTWP"
      },
      "source": [
        "SERIAL_EXEC = xmp.MpSerialExecutor()"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bila9S7CPU6d"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWsDceGV8EAt",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "outputId": "dc42eee8-3c35-4d0d-bfe3-490b06b7ef41"
      },
      "source": [
        "%%time\n",
        "# !rm -f /content/models/stage-1.pth\n",
        "if USE_DBLOCK:\n",
        "    xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'],\n",
        "            start_method='fork')\n",
        "else:\n",
        "    xmp.spawn(_mp_fn2, args=(FLAGS,), nprocs=FLAGS['num_cores'],\n",
        "            start_method='fork')\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "build learner\n",
            "start running fit\n",
            "start fit\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.415562</td>\n",
              "      <td>1.130980</td>\n",
              "      <td>0.697266</td>\n",
              "      <td>00:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.404932</td>\n",
              "      <td>2.204097</td>\n",
              "      <td>0.494141</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.089429</td>\n",
              "      <td>4.243430</td>\n",
              "      <td>0.316406</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.030001</td>\n",
              "      <td>14.848968</td>\n",
              "      <td>0.139648</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.033107</td>\n",
              "      <td>5.174943</td>\n",
              "      <td>0.225586</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.942981</td>\n",
              "      <td>3.685695</td>\n",
              "      <td>0.236328</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.843764</td>\n",
              "      <td>2.449688</td>\n",
              "      <td>0.486328</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.729406</td>\n",
              "      <td>1.739083</td>\n",
              "      <td>0.605469</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.615905</td>\n",
              "      <td>1.985155</td>\n",
              "      <td>0.603516</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.516536</td>\n",
              "      <td>1.263756</td>\n",
              "      <td>0.707031</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.420070</td>\n",
              "      <td>0.890628</td>\n",
              "      <td>0.779297</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.341372</td>\n",
              "      <td>0.990934</td>\n",
              "      <td>0.770508</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.275497</td>\n",
              "      <td>0.756113</td>\n",
              "      <td>0.821289</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.219644</td>\n",
              "      <td>0.659085</td>\n",
              "      <td>0.842773</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.174957</td>\n",
              "      <td>0.609641</td>\n",
              "      <td>0.857422</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.139931</td>\n",
              "      <td>0.616062</td>\n",
              "      <td>0.852539</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.112050</td>\n",
              "      <td>0.599445</td>\n",
              "      <td>0.865234</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.090083</td>\n",
              "      <td>0.597237</td>\n",
              "      <td>0.863281</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.072620</td>\n",
              "      <td>0.595642</td>\n",
              "      <td>0.865234</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.059478</td>\n",
              "      <td>0.589818</td>\n",
              "      <td>0.863281</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "fit  called 1 times. max: 733.510 avg: 733.510\n",
            "   epoch  called 20 times. max: 37.731 avg: 35.775\n",
            "      train  called 20 times. max: 31.429 avg: 30.357\n",
            "         train_batch  called 220 times. max: 12.297 avg: 1.382\n",
            "            train_pred  called 220 times. max: 12.121 avg: 1.030\n",
            "            train_loss  called 220 times. max: 0.001 avg: 0.000\n",
            "            train_backward  called 220 times. max: 0.024 avg: 0.005\n",
            "            train_step  called 220 times. max: 12.137 avg: 0.343\n",
            "            train_zero_grad  called 220 times. max: 0.005 avg: 0.003\n",
            "      valid  called 20 times. max: 6.395 avg: 5.417\n",
            "         valid_batch  called 40 times. max: 0.011 avg: 0.005\n",
            "            valid_pred  called 40 times. max: 0.010 avg: 0.004\n",
            "            valid_loss  called 40 times. max: 0.001 avg: 0.000\n",
            "CPU times: user 204 ms, sys: 237 ms, total: 440 ms\n",
            "Wall time: 13min 4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5X_mylR4oGp"
      },
      "source": [
        "if USE_DBLOCK: DATA.summary(PATH)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C4R7cXn9BaE"
      },
      "source": [
        "if USE_DBLOCK:\n",
        "    mdls = DATA.dataloaders(PATH, bs=FLAGS['batch_size'])\n",
        "else:\n",
        "    mdsets = DSET_BUILDER.get_datasets()\n",
        "    mdls = make_torch_dataloaders(*mdsets,\n",
        "                                  rank=0,\n",
        "                                  world_size=1,\n",
        "                                  bs=FLAGS['batch_size'],\n",
        "                                  num_workers=FLAGS['num_workers']\n",
        "                                  )"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWXz9cNmvlvb",
        "outputId": "cf56e383-7cb7-4c3c-da19-1e2239e229bf"
      },
      "source": [
        "mlearner = Learner(mdls, custom_model, \n",
        "                    loss_func=LOSS_FUNC, \n",
        "                    opt_func=OPT_FUNC, \n",
        "                    metrics=accuracy, \n",
        "                    wd=FLAGS['weight_decay'],\n",
        "                    moms=(FLAGS['momentum'],FLAGS['momentum'],FLAGS['momentum']))\n",
        "mlearner.load('stage-1')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<fastai.learner.Learner at 0x7f71605a9668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2d8WiQnYRar"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12CplWJcYSuA"
      },
      "source": [
        "mlearner.dls.device"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT5GiT4lYSuB"
      },
      "source": [
        "from fastai.torch_core import one_param"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76WuTlrvYSuB",
        "outputId": "24235d1e-6398-4e1c-89e8-d00d2827e6dd"
      },
      "source": [
        "one_param(mlearner.model).device"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "ayyjXIW_YGcd",
        "outputId": "d434b0d5-e4b0-46f6-def1-dc7f3e3ae50b"
      },
      "source": [
        "%%time\n",
        "valid_metrics = mlearner.validate();print(valid_metrics)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0.6023236513137817, 0.8620923757553101]\n",
            "CPU times: user 7min 52s, sys: 6.23 s, total: 7min 58s\n",
            "Wall time: 25.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6cw2CjR7ckJ"
      },
      "source": [
        "# master_device = xm.xla_device()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4aiYo7l7qNq"
      },
      "source": [
        "# mlearner.dls.device = master_device\n",
        "# mlearner.model.to(master_device)\n",
        "# mlearner.opt = None\n",
        "# mlearner.create_opt()"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvMC68SExbZn"
      },
      "source": [
        "# %%time\n",
        "# valid_metrics = mlearner.validate(); valid_metrics"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECOD3xG0y40j"
      },
      "source": [
        "# mlearner.dls.device"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_7ijcwE4zGf"
      },
      "source": [
        "# one_param(mlearner.model).device"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi3_YNi67YbR"
      },
      "source": [
        ""
      ],
      "execution_count": 62,
      "outputs": []
    }
  ]
}