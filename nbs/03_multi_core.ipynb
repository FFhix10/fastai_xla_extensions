{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp multi_core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Core XLA extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup torch XLA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the official way to install Pytorch-XLA 1.7 [instructions here](https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb#scrollTo=CHzziBW5AoZH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 133.6MB 78kB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 3.6MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "!pip install -Uqq cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install fastai\n",
    "\n",
    "Use latest fastai and fastcore versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 194kB 5.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 5.2MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "# !pip install -Uqq git+https://github.com/fastai/fastai.git \n",
    "!pip install -Uqq fastai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for my-timesaver-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -Uqq git+https://github.com/butchland/my_timesaver_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating fastai...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!curl -s https://course19.fast.ai/setup/colab | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==1.7.0+cu101\n",
      "torch-xla==1.7\n",
      "torchsummary==1.5.1\n",
      "torchtext==0.3.1\n",
      "torchvision==0.8.1+cu101\n",
      "fastai==2.2.5\n",
      "fastcore==1.3.19\n",
      "fastdtw==0.3.4\n",
      "fastprogress==1.0.0\n",
      "fastrlock==0.5\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!pip freeze | grep torch\n",
    "!pip freeze | grep fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#colab\n",
    "%cd /content\n",
    "!ln -s /content/drive/MyDrive/fastai_xla_extensions fastai_xla_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/fastai_xla_extensions/fastai_xla_extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start of kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastai_xla_extensions.utils import xla_imported\n",
    "from fastai_xla_extensions.misc_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:TPU has started up successfully with version pytorch-1.7\n"
     ]
    }
   ],
   "source": [
    "#exporti\n",
    "try:\n",
    "    import torch_xla\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#local\n",
    "if not xla_imported():\n",
    "    # replace torch xla modules with fake equivalents\n",
    "    from types import SimpleNamespace\n",
    "    torch_xla = SimpleNamespace (\n",
    "    )\n",
    "    from typing import Union,BinaryIO\n",
    "    import os\n",
    "    import pickle\n",
    "    import torch.cuda\n",
    "\n",
    "    def fake_opt_step(opt,barrier=False):\n",
    "        opt.step()\n",
    "        \n",
    "    def fake_device(n=None, devkind=None):\n",
    "        gpu_available = torch.cuda.is_available()\n",
    "        if gpu_available:\n",
    "            return torch.device(torch.cuda.current_device()) \n",
    "        return torch.device('cpu')\n",
    "\n",
    "    def fake_save(obj, f: Union[str, os.PathLike, BinaryIO], \n",
    "                master_only=True, global_master=False): \n",
    "        return torch.save(obj,f,pickle_module=pickle, \n",
    "                        pickle_protocol=2, \n",
    "                        _use_new_zipfile_serialization=True)\n",
    "    def fake_rate():\n",
    "        return 230.20\n",
    "\n",
    "    def fake_global_rate():\n",
    "        return 830.10\n",
    "\n",
    "    def fake_add(*args,**kwargs):\n",
    "        pass\n",
    "\n",
    "    def fake_RateTracker():\n",
    "        return SimpleNamespace(\n",
    "            rate = fake_rate,\n",
    "            global_rate = fake_global_rate,\n",
    "            add = fake_add\n",
    "        )\n",
    "    def fake_xrt_world_size():\n",
    "        return 1\n",
    "    def fake_get_ordinal():\n",
    "        return 0\n",
    "    xm = SimpleNamespace(\n",
    "        optimizer_step = fake_opt_step,\n",
    "        xla_device = fake_device,\n",
    "        save = fake_save,\n",
    "        RateTracker = fake_RateTracker,\n",
    "        master_print = print,\n",
    "        xrt_world_size = fake_xrt_world_size,\n",
    "        get_ordinal = fake_get_ordinal\n",
    "    )\n",
    "\n",
    "    def fake_metrics_report():\n",
    "        return \"Fake Metrics Report \\n\\n\\n\\n\"\n",
    "    met = SimpleNamespace (\n",
    "        metrics_report = fake_metrics_report\n",
    "    )\n",
    "\n",
    "    class FakeParallelLoader:\n",
    "        def __init__(self, loader, *args):\n",
    "            self.loader = loader\n",
    "        def per_device_loader(self,device):\n",
    "            return self.loader\n",
    "        \n",
    "    pl = SimpleNamespace(\n",
    "        ParallelLoader = FakeParallelLoader\n",
    "    )\n",
    "\n",
    "    def fake_MpModelWrapper(o):\n",
    "        return o\n",
    "\n",
    "    def fake_run(f,*args, **kwargs):\n",
    "            return f(*args,**kwargs)\n",
    "        \n",
    "    def fake_MpSerialExecutor():\n",
    "        return SimpleNamespace(\n",
    "            run = fake_run\n",
    "        )\n",
    "    def fake_spawn(f, args=None, nprocs=0, start_method=None):\n",
    "        return f(0,*args)\n",
    "\n",
    "    xmp = SimpleNamespace (\n",
    "        MpModelWrapper = fake_MpModelWrapper,\n",
    "        MpSerialExecutor = fake_MpSerialExecutor,\n",
    "        spawn = fake_spawn\n",
    "    )\n",
    "\n",
    "    xu = SimpleNamespace (\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "if xla_imported():\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.debug.metrics as met\n",
    "    import torch_xla.distributed.parallel_loader as pl\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "    import torch_xla.utils.utils as xu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastcore.basics import patch_to\n",
    "from fastai.optimizer import _BaseOptimizer\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as th_data\n",
    "from fastcore.foundation import L\n",
    "from pathlib import Path\n",
    "from fastcore.xtras import *\n",
    "from fastcore.transform import Pipeline\n",
    "from fastai.data.core import DataLoaders\n",
    "from functools import partial\n",
    "import torch.utils.data.distributed as torch_distrib\n",
    "from pathlib import Path\n",
    "import fastcore.xtras\n",
    "import math\n",
    "from fastcore.basics import store_attr\n",
    "from operator import attrgetter\n",
    "from fastai.data.load import _FakeLoader\n",
    "from fastai.data.core import TfmdDL\n",
    "from fastai.torch_core import find_bs, TensorBase\n",
    "import random\n",
    "import torch\n",
    "from fastai.data.load import _loaders\n",
    "from fastai.torch_core import to_device\n",
    "from fastcore.basics import first, patch_to, patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from fastai.torch_core import find_bs, TensorBase\n",
    "import math\n",
    "\n",
    "def revert_tensor(o):\n",
    "    \"\"\"Remove tensor subclass info from class and revert back to Tensor\"\"\"\n",
    "    try:\n",
    "        o.__class__ = torch.Tensor\n",
    "    except:\n",
    "        raise RuntimeError(f'could not convert {o} to torch.Tensor')\n",
    "    return o\n",
    "\n",
    "def _recast2tensor(o):\n",
    "    if isinstance(o,TensorBase):\n",
    "        # return plain tensor since pl.parallelloader doesn't\n",
    "        # seem to work with tensor subclasses\n",
    "        # return torch.as_tensor(o.numpy())\n",
    "        return revert_tensor(o)\n",
    "    return o\n",
    "\n",
    "def _round_to_multiple(number,multiple):\n",
    "    return int(math.ceil(number/multiple)*multiple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TPUDistributedDL(TfmdDL):\n",
    "    \"\"\"A `TfmdDL` which splits a batch into equal size pieces for each TPU core\n",
    "       It also recasts the output of a batch from a Tensorbase subclass to\n",
    "       a regular tensor since the XLA Parallel loader doesnt seem compatible\n",
    "       to it.\n",
    "    \"\"\"\n",
    "    _default = 'dl'\n",
    "    def __init__(self,dl,rank,world_size, seed=42):\n",
    "        store_attr()\n",
    "        self.bs,self.device,self.num_workers,self.drop_last,self.dataset,self.offs,fake = \\\n",
    "            attrgetter('bs','device','num_workers','drop_last','dataset','offs','fake_l')(dl)\n",
    "        self.fake_l = _FakeLoader(self, fake.pin_memory, fake.num_workers, fake.timeout,\n",
    "                                  persistent_workers=fake.persistent_workers)\n",
    "        self.epoch = 0\n",
    "        random.seed(self.seed)\n",
    "        self.dl.rng = random.Random(random.randint(0,2**32-1))\n",
    "        self.reset_rng()\n",
    "\n",
    "    def reset_rng(self):\n",
    "        random.seed(self.seed + self.epoch)\n",
    "        self.rng = random.Random(random.randint(0,2**32-1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return _round_to_multiple(len(self.dl),self.world_size)//self.world_size\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def get_idxs(self):\n",
    "        idxs = self.dl.get_idxs()\n",
    "        # do your own shuffling which factors in self.epoch + self.seed in\n",
    "        # generating a random sequence (underlying self.dl does not)\n",
    "        if self.shuffle:\n",
    "            idxs = self.shuffle_fn(idxs)\n",
    "        self.n = len(idxs)\n",
    "        # we assumed n was dl.n but we really care about number of idxs\n",
    "        # add extra samples to make it evenly divisible\n",
    "        self.n_padded = _round_to_multiple(self.n,self.world_size)\n",
    "        idxs += (idxs * (self.n_padded//self.n))[:self.n_padded-self.n]\n",
    "        # idx needs to be repeated when n_padded>>n\n",
    "        # slice padded idxs so that each rank gets self.n_padded//self.world_size tensors\n",
    "        start_pos = self.rank*self.n_padded//self.world_size\n",
    "        end_pos = (self.rank+1)*self.n_padded//self.world_size\n",
    "        return idxs[start_pos:end_pos]\n",
    "\n",
    "    def before_iter(self):\n",
    "        self.dl.before_iter()\n",
    "\n",
    "    def randomize(self):\n",
    "        self.reset_rng()\n",
    "        self.dl.randomize()\n",
    "\n",
    "    def after_batch(self,b):\n",
    "        b = self.dl.after_batch(b)\n",
    "        # recast tensor subclasses to plain tensors\n",
    "        # undoing work of self.retain()\n",
    "        tb = [_recast2tensor(o) for o in b]\n",
    "        b = tuple(tb)\n",
    "        return b\n",
    "\n",
    "    def after_iter(self):\n",
    "        self.dl.after_iter()\n",
    "\n",
    "    def create_batches(self,samps):\n",
    "        return self.dl.create_batches(samps)\n",
    "\n",
    "    def to(self, device):\n",
    "        self.dl.device = device\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    def one_batch(self):\n",
    "        return self.dl.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastai.torch_core import default_device, apply\n",
    "import torch\n",
    "from fastcore.xtras import is_listy\n",
    "import torch\n",
    "import torch.utils.hooks\n",
    "from fastcore.basics import patch\n",
    "from fastai.torch_core import TensorBase\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def make_distributed_dataloaders(dls, rank, world_size, sync_valid=False):\n",
    "    \"\"\"Wrap dataloaders with distributed TPU aware dataloader \"\"\"\n",
    "    new_loaders = []\n",
    "    for i,dl in enumerate(dls.loaders):\n",
    "        if i == 0 or sync_valid:\n",
    "            use_rank = rank\n",
    "            use_size = world_size\n",
    "        else:\n",
    "            use_rank = 0\n",
    "            use_size = 1\n",
    "        dl = TPUDistributedDL(dl,\n",
    "                            rank=use_rank,\n",
    "                            world_size=use_size)\n",
    "        new_loaders += [dl]\n",
    "    return DataLoaders(*new_loaders, path=dls.path, device=dls.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# def DataBlock.dataloaders(self, source, path='.', verbose=False, **kwargs):\n",
    "def make_fastai_dataloaders(datablock, source, rank, world_size, device=None, path='.', sync_valid=False, verbose=False,**kwargs):\n",
    "    dls = datablock.dataloaders(source=source, path=path, device=device, **kwargs)\n",
    "    distrib_dls = make_distributed_dataloaders(dls, rank, world_size, sync_valid=sync_valid)\n",
    "    return distrib_dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def wrap_parallel_loader(loader, device):\n",
    "    para_loader = pl.ParallelLoader(loader, [device])\n",
    "    loop_loader = para_loader.per_device_loader(device)\n",
    "    return loop_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastai.callback.core import TrainEvalCallback\n",
    "from fastai.learner import Recorder\n",
    "from fastai.torch_core import one_param\n",
    "import torch\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.learner import CancelTrainException, CancelValidException, CancelStepException\n",
    "from fastai.torch_core import tensor, TensorCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class XLATrainingCallback(Callback):\n",
    "    run_before = Recorder\n",
    "    run_valid = False\n",
    "    order = -5 # after TrainEvalCallback\n",
    "    def __init__(self, device, rank=0, sync_valid=False):\n",
    "        self.pdevice = device\n",
    "        self.rank = rank\n",
    "        self.sync_valid = sync_valid\n",
    "\n",
    "    def before_fit(self):\n",
    "       xm.master_print('start fit')\n",
    "\n",
    "    def before_epoch(self):\n",
    "        # set the epoch on train only to make sure shuffle produces same seq\n",
    "        # across all ranks\n",
    "        if hasattr(self.learn.dls.train,'sampler'):\n",
    "            if hasattr(self.learn.dls.train.sampler,'set_epoch'):\n",
    "                self.learn.dls.train.sampler.set_epoch(self.learn.epoch)\n",
    "        elif hasattr(self.learn.dls.train,'set_epoch'):\n",
    "            self.learn.dls.train.set_epoch(self.learn.epoch)\n",
    "\n",
    "        if self.sync_valid: # update epoch on valid if sync_valid\n",
    "            if hasattr(self.learn.dls.valid,'sampler'):\n",
    "                if hasattr(self.learn.dls.valid.sampler,'set_epoch'):\n",
    "                    self.learn.dls.valid.sampler.set_epoch(self.learn.epoch)\n",
    "            elif hasattr(self.learn.dls.valid,'set_epoch'):\n",
    "                self.learn.dls.valid.set_epoch(self.learn.epoch)\n",
    "\n",
    "    def before_train(self):\n",
    "        self.learn.dl = wrap_parallel_loader(self.dls.train, self.pdevice)\n",
    "\n",
    "    def before_validate(self):\n",
    "        \"Set the model in validation mode\"\n",
    "        if self.rank != 0 and not self.sync_valid:\n",
    "        # no need to compute valid loss/ metric if not master if not sync valid\n",
    "            raise CancelValidException()\n",
    "        self.learn.dl = wrap_parallel_loader(self.dls.valid, self.pdevice)\n",
    "\n",
    "    def before_step(self):\n",
    "        raise CancelStepException()\n",
    "\n",
    "    def after_cancel_step(self):\n",
    "        xm.optimizer_step(self.learn.opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "import copy\n",
    "from fastcore.foundation import L\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastai.learner import _maybe_item\n",
    "from fastprogress.fastprogress import format_time\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pack_metric(metrics):\n",
    "    counts = metrics.attrgot('count',0)\n",
    "    totals = metrics.attrgot('total',0)\n",
    "    metrics_list = counts + totals\n",
    "    return metrics_list\n",
    "\n",
    "def make_tensor(o, device):\n",
    "    if not isinstance(o, torch.Tensor):\n",
    "        o = torch.tensor(o)\n",
    "    return o.float().to(device)\n",
    "    \n",
    "def pack_metrics(all_metrics, device):\n",
    "    metrics_list = pack_metric(all_metrics['train_mets']) + pack_metric(all_metrics['valid_mets'])\n",
    "    return [make_tensor(item,device) for item in metrics_list ]\n",
    "\n",
    "def restore_metrics(reduced_metrics, all_metrics):\n",
    "    n_train = len(all_metrics['train_mets'])\n",
    "    n_valid = len(all_metrics['valid_mets'])\n",
    "    train_counts = reduced_metrics[:n_train]\n",
    "    train_totals = reduced_metrics[n_train: n_train*2]\n",
    "    valid_counts = reduced_metrics[n_train*2: n_train*2 + n_valid]\n",
    "    valid_totals = reduced_metrics[n_train*2 + n_valid:]\n",
    "    for i,metric in enumerate(all_metrics['train_mets']):\n",
    "        if hasattr(metric,'count'):\n",
    "            metric.count = train_counts[i].clone().detach().long()\n",
    "        if hasattr(metric,'total'):\n",
    "            metric.total = train_totals[i].clone().detach()\n",
    "    for i,metric in enumerate(all_metrics['valid_mets']):\n",
    "        if hasattr(metric,'count'):\n",
    "            metric.count = valid_counts[i].clone().detach().long()\n",
    "        if hasattr(metric,'total'):\n",
    "            metric.total = valid_totals[i].clone().detach()\n",
    "    return all_metrics  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SyncRecorderCallback(Callback):\n",
    "    \"\"\"Sync metrics from each spawned process update statistics\n",
    "       accordingly so it will display correctly in the progress callback\n",
    "    \"\"\"\n",
    "    order  = 55 # after Recorder, before ProgressCallback\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def before_fit(self):\n",
    "        if not xm.is_master_ordinal():\n",
    "            return\n",
    "        if 'progress' in self.learn.cbs.attrgot('name',None):\n",
    "            self._sync_stats_log = self.progress._write_stats\n",
    "        else:\n",
    "            self._sync_stats_log = self.learn.logger\n",
    "\n",
    "    def after_fit(self):\n",
    "        pass\n",
    "        # xm.rendezvous('sync recorder after_fit')\n",
    "\n",
    "    def before_epoch(self):\n",
    "        self.sync_log = copy.copy(self.recorder.log)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        if 'recorder' not in self.learn.cbs.attrgot('name'):\n",
    "            all_metrics = {\n",
    "                'train_mets': L([]),\n",
    "                'valid_mets': L([]),\n",
    "            }\n",
    "        else:\n",
    "            all_metrics = {\n",
    "                'train_mets': self.recorder._train_mets,\n",
    "                'valid_mets': self.recorder._valid_mets,\n",
    "            }\n",
    "        # send metrics data to sync ranks across spawned processes\n",
    "        device = self.learn.xla_training.pdevice\n",
    "        packed_metrics = pack_metrics(all_metrics, device) # convert metrics to tensor list on TPU\n",
    "        reduced_metrics = xm.all_reduce(xm.REDUCE_SUM, packed_metrics)\n",
    "        xm.mark_step()\n",
    "        if xm.is_master_ordinal():\n",
    "            all_metrics = restore_metrics(reduced_metrics, all_metrics) # convert list to metric objects\n",
    "            for m in self.recorder._train_mets:\n",
    "                self.sync_log += _maybe_item(m)\n",
    "\n",
    "            for m in self.recorder._valid_mets:\n",
    "                self.sync_log += _maybe_item(m)\n",
    "\n",
    "            self.learn.final_record = self.sync_log[:1].copy()\n",
    "            del self.recorder.values[-1] # remove last entry added by recorder\n",
    "            self.recorder.values.append(self.learn.final_record) # add updated metrics\n",
    "            if self.recorder.add_time:\n",
    "                updated_time = (time.time() - self.recorder.start_epoch)\n",
    "                self.sync_log.append(format_time(updated_time))\n",
    "            self.recorder.log = self.sync_log\n",
    "            self._sync_stats_log(self.sync_log) # write_stats to output\n",
    "            self.learn.logger = self.orig_logger # restore orig logger after skipping recorder.logger(log)\n",
    "\n",
    "    def before_validate(self):\n",
    "        pass\n",
    "\n",
    "    def after_validate(self):\n",
    "        if xm.is_master_ordinal():\n",
    "            self.orig_logger = self.learn.logger\n",
    "            self.learn.logger = noop # write to logger disabled so calling recorder.logger(log) wont print\n",
    "        pass\n",
    "\n",
    "    def before_batch(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastcore.imports import noop\n",
    "from fastcore.basics import patch\n",
    "from fastai.learner import Learner\n",
    "from fastai.callback.progress import ProgressCallback\n",
    "from fastcore.xtras import join_path_file\n",
    "from fastai.torch_core import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@patch\n",
    "def save(self:Learner, file, **kwargs):\n",
    "    file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n",
    "    with_opt = self.opt is not None\n",
    "    state = self.model.state_dict()\n",
    "    if with_opt:\n",
    "        # add opt state to state to be saved\n",
    "        opt_state = self.opt.state_dict()\n",
    "        state = {'model': state, 'opt':opt_state}\n",
    "    xm.save(state, file) # use xm.save instead of torch.save\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@patch\n",
    "def to_xla(self:Learner,device, rank, sync_valid=False):\n",
    "    if 'xla_training' not in self.cbs.attrgot('name'):\n",
    "        self.dls.device = None\n",
    "        self.add_cbs(XLATrainingCallback(device, rank, sync_valid=sync_valid))\n",
    "    else:\n",
    "        self.xla_training.pdevice = device\n",
    "        self.xla_training.rank = rank\n",
    "        self.xla_training.sync_valid = sync_valid\n",
    "\n",
    "    if sync_valid and 'sync_recorder' not in self.cbs.attrgot('name'):\n",
    "        self.add_cbs(SyncRecorderCallback)\n",
    "    elif not sync_valid:\n",
    "        self.remove_cbs(SyncRecorderCallback)\n",
    "\n",
    "    if rank != 0: # progress bar only for rank 0\n",
    "        self.remove_cbs(ProgressCallback)\n",
    "    self.logger = xm.master_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# for testing\n",
    "def do_one_loop(dls, rank, world_size, device, sync_valid, is_train=True):\n",
    "    if is_train:\n",
    "        dl = dls.train\n",
    "    else:\n",
    "        dl = dls.valid\n",
    "\n",
    "    n_batches = len(dl)\n",
    "    print(f'xla: {rank} world_size: {world_size} train:{is_train} n_batches:{n_batches} sync_valid: {sync_valid}')\n",
    "\n",
    "    if sync_valid or is_train or rank == 0:\n",
    "        print(f'xla: {rank} wrapping ploader')\n",
    "        pdl = wrap_parallel_loader(dl, device=device)\n",
    "    for i,b in enumerate(pdl):\n",
    "        if i > 1:\n",
    "            break\n",
    "        xb, yb = b\n",
    "        print(f'xla: {rank} iter:{i} xb.shape {xb.shape} yb.shape: {yb.shape}')\n",
    "        print(f'xla: {rank} iter:{i} xb.device {xb.device} yb.device: {yb.device}')\n",
    "        print(f'xla: {rank} iter:{i} xb.dtype {xb.dtype} yb.device: {yb.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "from functools import partial\n",
    "from fastai.metrics import accuracy\n",
    "from fastai.optimizer import SGD, Adam\n",
    "\n",
    "from fastcore.basics import first\n",
    "from fastai.callback.schedule import *\n",
    "from fastai.test_utils import VerboseCallback\n",
    "from my_timesaver_utils.profiling import *\n",
    "from my_timesaver_utils.profiling_callback import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "def train_model(rank):\n",
    "    torch.manual_seed(1)\n",
    "    xm.rendezvous('start_train_model')\n",
    "    print(f'xla {rank} start train model')\n",
    "\n",
    "    # Scale learning rate to num cores\n",
    "    learning_rate = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
    "    SYNC_VALID = FLAGS['sync_valid']\n",
    "    IS_PROFILING = FLAGS['is_profiling']\n",
    "    # Get loss function, optimizer, and model\n",
    "    device = xm.xla_device()\n",
    "    model = WRAPPED_MODEL.to(device)\n",
    "    bs = FLAGS['batch_size']\n",
    "    moms =(FLAGS['momentum'],FLAGS['momentum'],FLAGS['momentum'])\n",
    "    wd = FLAGS['weight_decay']\n",
    "\n",
    "    world_size = xm.xrt_world_size()\n",
    "    if IS_PROFILING:\n",
    "        rec_name = 'rank' + str(rank) + '_dataloader_build'\n",
    "        print(f'start {rec_name}')\n",
    "        start_record(rec_name)\n",
    "\n",
    "    dls = make_fastai_dataloaders(\n",
    "                            DATA, \n",
    "                            PATH, \n",
    "                            rank=rank, \n",
    "                            world_size=world_size, \n",
    "                            sync_valid=SYNC_VALID,\n",
    "                            bs=bs,)\n",
    "    if IS_PROFILING:\n",
    "        end_record(rec_name)\n",
    "        print_prof_data(rec_name)\n",
    "        print(f'finished {rec_name}')\n",
    "\n",
    "    xm.master_print('build learner')\n",
    "    learner = Learner(dls, model, \n",
    "                      loss_func=LOSS_FUNC, \n",
    "                      opt_func=OPT_FUNC, \n",
    "                      metrics=accuracy, \n",
    "                      wd=wd,\n",
    "                      moms=moms)\n",
    "                      \n",
    "    learner.to_xla(device, rank=xm.get_ordinal(), sync_valid=SYNC_VALID)\n",
    "    if rank == 0:\n",
    "        learner.to_my_profile()\n",
    "                               \n",
    "    epochs = FLAGS['num_epochs']\n",
    "    xm.master_print('start running fit')\n",
    "    learner.unfreeze()\n",
    "    if IS_PROFILING:\n",
    "        rec_name3 = 'rank' + str(rank) + '_run_fit'\n",
    "        print(f'start {rec_name3}')\n",
    "        start_record(rec_name3)\n",
    "\n",
    "    learner.fit_one_cycle(epochs, lr_max=slice(learning_rate/10))\n",
    "    if IS_PROFILING:\n",
    "        end_record(rec_name3)\n",
    "        print_prof_data(rec_name3)\n",
    "        print(f'finished {rec_name3}')\n",
    "\n",
    "    learner.save('stage-1')\n",
    "    if rank == 0:\n",
    "        learner.my_profile.print_stats()\n",
    "    xm.mark_step()  \n",
    "    xm.rendezvous('end_train_model')\n",
    "    if IS_PROFILING:\n",
    "        clear_prof_data() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# Start training processes\n",
    "def _mp_fn(rank, flags):\n",
    "    global FLAGS\n",
    "    FLAGS = flags\n",
    "    train_model(rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fastcore.transform import DisplayedTransform, Transform\n",
    "from fastcore.basics import store_attr\n",
    "from fastai.vision.core import PILImage, PILBase, image2tensor\n",
    "from fastai.data.block import TransformBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.transforms import get_c\n",
    "# from fastai.vision.all import *\n",
    "from fastai.data.block import DataBlock, CategoryBlock\n",
    "from fastai.vision.data import ImageBlock\n",
    "from fastai.data.transforms import get_image_files, parent_label, GrandparentSplitter\n",
    "from fastai.vision.augment import Resize, aug_transforms\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastai.data.transforms import Normalize\n",
    "from fastai.vision.core import imagenet_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "LOSS_FUNC = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.optimizer import Adam\n",
    "OPT_FUNC = Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.transforms import RandomSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.learner import create_cnn_model\n",
    "from fastai.vision.models import resnet34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Define Parameters\n",
    "FLAGS = {}\n",
    "# FLAGS['batch_size'] = 1024\n",
    "FLAGS['sync_valid'] = True\n",
    "FLAGS['is_profiling'] = False\n",
    "FLAGS['batch_size'] = 64\n",
    "FLAGS['num_workers'] = 4\n",
    "FLAGS['learning_rate'] = 1e-3\n",
    "FLAGS['image_size'] = 32\n",
    "FLAGS['momentum'] = 0.85\n",
    "FLAGS['weight_decay'] = 2e-4\n",
    "FLAGS['num_epochs'] = 5\n",
    "FLAGS['num_cores'] = 8 if os.environ.get('TPU_NAME', None) else 1\n",
    "# FLAGS['num_cores'] = 1 \n",
    "ARCH = resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from fastcore.xtras import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "PATH = untar_data(URLs.PETS)/'images'\n",
    "# PATH = untar_data(URLs.MNIST)\n",
    "# PATH = untar_data(URLs.MNIST_TINY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "\n",
    "pat = r'(.+)_\\d+.jpg$'\n",
    "fname_labeller = using_attr(RegexLabeller(pat),'name') \n",
    "splitter=RandomSplitter(seed=42)\n",
    "DATA = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),\n",
    "    get_items=get_image_files,\n",
    "    get_y=fname_labeller,\n",
    "    splitter=splitter,\n",
    "    item_tfms=[Resize(FLAGS['image_size']),],\n",
    "    batch_tfms=[]\n",
    ")\n",
    "vocab = CategoryMap(get_image_files(PATH).map(fname_labeller))\n",
    "N_OUT = len(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "assert N_OUT is not None and N_OUT > 0,f'N_OUT {N_OUT} should be > 0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "custom_model = create_cnn_model(ARCH, N_OUT, \n",
    "                                pretrained=True,\n",
    "                                concat_pool=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# Only instantiate model weights once in memory.\n",
    "WRAPPED_MODEL = xmp.MpModelWrapper(custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build learner\n",
      "start running fit\n",
      "start fit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.207130</td>\n",
       "      <td>4.118487</td>\n",
       "      <td>0.047852</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.725443</td>\n",
       "      <td>7.102302</td>\n",
       "      <td>0.065430</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.201487</td>\n",
       "      <td>4.366318</td>\n",
       "      <td>0.119141</td>\n",
       "      <td>00:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.506165</td>\n",
       "      <td>3.342862</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.727757</td>\n",
       "      <td>3.203462</td>\n",
       "      <td>0.166992</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit  called 1 times. max: 199.537 avg: 199.537\n",
      "   epoch  called 5 times. max: 43.851 avg: 39.726\n",
      "      train  called 5 times. max: 37.078 avg: 33.036\n",
      "         train_batch  called 55 times. max: 6.104 avg: 1.449\n",
      "            train_pred  called 55 times. max: 5.079 avg: 0.705\n",
      "            train_loss  called 55 times. max: 0.041 avg: 0.005\n",
      "            train_backward  called 55 times. max: 0.245 avg: 0.042\n",
      "            train_step  called 55 times. max: 1.463 avg: 0.628\n",
      "            train_zero_grad  called 55 times. max: 0.145 avg: 0.066\n",
      "      valid  called 5 times. max: 6.849 avg: 6.639\n",
      "         valid_batch  called 10 times. max: 0.087 avg: 0.049\n",
      "            valid_pred  called 10 times. max: 0.073 avg: 0.040\n",
      "            valid_loss  called 10 times. max: 0.023 avg: 0.009\n",
      "CPU times: user 111 ms, sys: 126 ms, total: 237 ms\n",
      "Wall time: 4min 4s\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "%%time\n",
    "# !rm -f /content/models/stage-1.pth\n",
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'],\n",
    "        start_method='fork')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "DATA.summary(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "mdls = DATA.dataloaders(PATH, bs=FLAGS['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.learner.Learner at 0x7f0e778ba198>"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#colab\n",
    "mlearner = Learner(mdls, custom_model, \n",
    "                    loss_func=LOSS_FUNC, \n",
    "                    opt_func=OPT_FUNC, \n",
    "                    metrics=accuracy, \n",
    "                    wd=FLAGS['weight_decay'],\n",
    "                    moms=(FLAGS['momentum'],FLAGS['momentum'],FLAGS['momentum']))\n",
    "mlearner.load('stage-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "mlearner.dls.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.torch_core import one_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#colab\n",
    "one_param(mlearner.model).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.198190450668335, 0.16915760934352875]\n",
      "CPU times: user 9.33 s, sys: 198 ms, total: 9.53 s\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "%%time\n",
    "valid_metrics = mlearner.validate();print(valid_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expose xla fit methods on learner to simplify usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "# from fastai.vision.all import *\n",
    "# import torch_xla.core.xla_model as xm\n",
    "# import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _make_xla_child_learner(rank, sync_valid,learner_args):\n",
    "    sync_valid = True\n",
    "    device = xm.xla_device()\n",
    "    world_size = xm.xrt_world_size()\n",
    "    dls = make_distributed_dataloaders(learner_args.pop('base_dls'), \n",
    "                                       rank, world_size, sync_valid=sync_valid)\n",
    "    \n",
    "    model = learner_args.pop('wrapped_model').to(device)\n",
    "\n",
    "    learner = Learner(dls, model,**learner_args)\n",
    "    learner.to_xla(device, rank, sync_valid=sync_valid)\n",
    "    return learner\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _xla_run_fit(rank, learner_args, fit_args):\n",
    "    sync_valid = True\n",
    "    learner = _make_xla_child_learner(rank, sync_valid, learner_args)    \n",
    "    learner.fit(**fit_args)\n",
    "    learner.save('_xla_tmp_model')\n",
    "    xm.mark_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _xla_run_fit_one_cycle(rank, learner_args, fit_args):\n",
    "    sync_valid = True\n",
    "    learner = _make_xla_child_learner(rank, sync_valid, learner_args)      \n",
    "    learner.fit_one_cycle(**fit_args)\n",
    "    learner.save('_xla_tmp_model')\n",
    "    xm.mark_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.basics import defaults, patch_to, patch\n",
    "from fastai.learner import Learner\n",
    "from fastai.callback.progress import ProgressCallback\n",
    "@patch_to(Learner)\n",
    "def pack_learner_args(self):\n",
    "    learner_args = {}\n",
    "    learner_args['wrapped_model'] =  xmp.MpModelWrapper(self.model)\n",
    "    learner_args['base_dls'] = self.dls\n",
    "    learner_args['opt_func'] = self.opt_func\n",
    "    learner_args['loss_func'] = self.loss_func\n",
    "    learner_args['metrics'] = self.metrics\n",
    "    # fetch only cbs not in defaults\n",
    "    if ProgressCallback not in defaults.callbacks:\n",
    "        defaults.callbacks.append(ProgressCallback)\n",
    "    learner_args['cbs'] = [cb for cb in self.cbs \n",
    "                      if cb.name not in L(defaults.callbacks).attrgot('name')]\n",
    "\n",
    "    learner_args['wd'] = self.wd\n",
    "    learner_args['moms'] = self.moms\n",
    "    learner_args['lr'] = self.lr\n",
    "    learner_args['splitter'] = self.splitter\n",
    "    learner_args['path'] = self.path\n",
    "    learner_args['model_dir'] = self.model_dir\n",
    "    learner_args['wd_bn_bias'] = self.wd_bn_bias\n",
    "    learner_args['train_bn'] = self.train_bn\n",
    "    return learner_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch_to(Learner)\n",
    "def reload_child_model(self):\n",
    "    # blatantly stolen from fastai LRFinder after_fit :)\n",
    "    tmp_f = self.path/self.model_dir/'_xla_tmp_model.pth'\n",
    "    if tmp_f.exists():\n",
    "        self.opt.zero_grad() \n",
    "        self.load('_xla_tmp_model', with_opt=False)\n",
    "        os.remove(tmp_f)  \n",
    "        self.create_opt()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from fastcore.meta import delegates\n",
    "@patch\n",
    "@delegates(Learner.fit, but='num_cores')\n",
    "def xla_fit(self:Learner, n_epoch, num_cores=8, **kwargs):\n",
    "    \"\"\"call fit in multicore tpu environment\"\"\"\n",
    "    learner_args = self.pack_learner_args()\n",
    "    fit_args={**kwargs}\n",
    "    fit_args['n_epoch'] = n_epoch\n",
    "    xmp.spawn(_xla_run_fit,\n",
    "              args=(learner_args, fit_args,),\n",
    "              nprocs=num_cores,\n",
    "              start_method='fork')\n",
    "    self.reload_child_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.learner import Learner\n",
    "from fastai.callback.schedule import *\n",
    "@patch\n",
    "@delegates(Learner.fit_one_cycle, but='num_cores')\n",
    "def xla_fit_one_cycle(self:Learner, n_epoch, num_cores=8, **kwargs):\n",
    "    \"\"\"call fit_one_cycle in multicore tpu environment\"\"\"\n",
    "    learner_args = self.pack_learner_args()\n",
    "    fit_args={**kwargs}\n",
    "    fit_args['n_epoch'] = n_epoch\n",
    "    xmp.spawn(_xla_run_fit_one_cycle,\n",
    "              args=(learner_args, fit_args,),\n",
    "              nprocs=num_cores,\n",
    "              start_method='fork')\n",
    "    self.reload_child_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "path = untar_data(URLs.MNIST_TINY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "data = DataBlock(\n",
    "    blocks=(ImageBlock,CategoryBlock),\n",
    "    get_items=get_image_files,\n",
    "    get_y=parent_label,\n",
    "    splitter=GrandparentSplitter(),\n",
    "    item_tfms=Resize(28),\n",
    "    batch_tfms=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "dls = data.dataloaders(path, bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# concat_pool must be false due to a TPU bug that is triggered if using fastai AdaptivePool\n",
    "from fastai.vision.learner import cnn_learner\n",
    "from torchvision.models.resnet import resnet18\n",
    "learner = cnn_learner(dls, resnet18, metrics=accuracy, concat_pool=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "#hide\n",
    "assert hasattr(learner,'xla_fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.204781</td>\n",
       "      <td>0.384089</td>\n",
       "      <td>0.927557</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.184112</td>\n",
       "      <td>0.361853</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.212741</td>\n",
       "      <td>0.367217</td>\n",
       "      <td>0.846591</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.243097</td>\n",
       "      <td>0.301363</td>\n",
       "      <td>0.879261</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.253019</td>\n",
       "      <td>0.289174</td>\n",
       "      <td>0.901989</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 164 ms, sys: 229 ms, total: 392 ms\n",
      "Wall time: 59.6 s\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "%%time\n",
    "learner.xla_fit_one_cycle(5,lr_max=slice(2e-3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBase(0.8984)\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "res = learner.get_preds()\n",
    "print(accuracy(*res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Sequential (Input shape: 16)\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     16 x 64 x 14 x 14   \n",
       "Conv2d                                    9408       False     \n",
       "BatchNorm2d                               128        True      \n",
       "ReLU                                                           \n",
       "MaxPool2d                                                      \n",
       "Conv2d                                    36864      False     \n",
       "BatchNorm2d                               128        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    36864      False     \n",
       "BatchNorm2d                               128        True      \n",
       "Conv2d                                    36864      False     \n",
       "BatchNorm2d                               128        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    36864      False     \n",
       "BatchNorm2d                               128        True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 128 x 4 x 4    \n",
       "Conv2d                                    73728      False     \n",
       "BatchNorm2d                               256        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    147456     False     \n",
       "BatchNorm2d                               256        True      \n",
       "Conv2d                                    8192       False     \n",
       "BatchNorm2d                               256        True      \n",
       "Conv2d                                    147456     False     \n",
       "BatchNorm2d                               256        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    147456     False     \n",
       "BatchNorm2d                               256        True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 256 x 2 x 2    \n",
       "Conv2d                                    294912     False     \n",
       "BatchNorm2d                               512        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    589824     False     \n",
       "BatchNorm2d                               512        True      \n",
       "Conv2d                                    32768      False     \n",
       "BatchNorm2d                               512        True      \n",
       "Conv2d                                    589824     False     \n",
       "BatchNorm2d                               512        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    589824     False     \n",
       "BatchNorm2d                               512        True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 512 x 1 x 1    \n",
       "Conv2d                                    1179648    False     \n",
       "BatchNorm2d                               1024       True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    2359296    False     \n",
       "BatchNorm2d                               1024       True      \n",
       "Conv2d                                    131072     False     \n",
       "BatchNorm2d                               1024       True      \n",
       "Conv2d                                    2359296    False     \n",
       "BatchNorm2d                               1024       True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    2359296    False     \n",
       "BatchNorm2d                               1024       True      \n",
       "AdaptiveAvgPool2d                                              \n",
       "Flatten                                                        \n",
       "BatchNorm1d                               1024       True      \n",
       "Dropout                                                        \n",
       "Linear                                    262144     True      \n",
       "ReLU                                                           \n",
       "BatchNorm1d                               1024       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     16 x 2              \n",
       "Linear                                    1024       True      \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 11,441,728\n",
       "Total trainable params: 274,816\n",
       "Total non-trainable params: 11,166,912\n",
       "\n",
       "Optimizer used: <function Adam at 0x7f0e888a87b8>\n",
       "Loss function: FlattenedLoss of CrossEntropyLoss()\n",
       "\n",
       "Model unfrozen\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - Recorder\n",
       "  - ProgressCallback"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#colab\n",
    "#hide\n",
    "learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Sequential (Input shape: 16)\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     16 x 64 x 14 x 14   \n",
       "Conv2d                                    9408       True      \n",
       "BatchNorm2d                               128        True      \n",
       "ReLU                                                           \n",
       "MaxPool2d                                                      \n",
       "Conv2d                                    36864      True      \n",
       "BatchNorm2d                               128        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    36864      True      \n",
       "BatchNorm2d                               128        True      \n",
       "Conv2d                                    36864      True      \n",
       "BatchNorm2d                               128        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    36864      True      \n",
       "BatchNorm2d                               128        True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 128 x 4 x 4    \n",
       "Conv2d                                    73728      True      \n",
       "BatchNorm2d                               256        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    147456     True      \n",
       "BatchNorm2d                               256        True      \n",
       "Conv2d                                    8192       True      \n",
       "BatchNorm2d                               256        True      \n",
       "Conv2d                                    147456     True      \n",
       "BatchNorm2d                               256        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    147456     True      \n",
       "BatchNorm2d                               256        True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 256 x 2 x 2    \n",
       "Conv2d                                    294912     True      \n",
       "BatchNorm2d                               512        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    589824     True      \n",
       "BatchNorm2d                               512        True      \n",
       "Conv2d                                    32768      True      \n",
       "BatchNorm2d                               512        True      \n",
       "Conv2d                                    589824     True      \n",
       "BatchNorm2d                               512        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    589824     True      \n",
       "BatchNorm2d                               512        True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 512 x 1 x 1    \n",
       "Conv2d                                    1179648    True      \n",
       "BatchNorm2d                               1024       True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    2359296    True      \n",
       "BatchNorm2d                               1024       True      \n",
       "Conv2d                                    131072     True      \n",
       "BatchNorm2d                               1024       True      \n",
       "Conv2d                                    2359296    True      \n",
       "BatchNorm2d                               1024       True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    2359296    True      \n",
       "BatchNorm2d                               1024       True      \n",
       "AdaptiveAvgPool2d                                              \n",
       "Flatten                                                        \n",
       "BatchNorm1d                               1024       True      \n",
       "Dropout                                                        \n",
       "Linear                                    262144     True      \n",
       "ReLU                                                           \n",
       "BatchNorm1d                               1024       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     16 x 2              \n",
       "Linear                                    1024       True      \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 11,441,728\n",
       "Total trainable params: 11,441,728\n",
       "Total non-trainable params: 0\n",
       "\n",
       "Optimizer used: <function Adam at 0x7f0e888a87b8>\n",
       "Loss function: FlattenedLoss of CrossEntropyLoss()\n",
       "\n",
       "Model unfrozen\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - Recorder\n",
       "  - ProgressCallback"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#colab\n",
    "#hide\n",
    "learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.073536</td>\n",
       "      <td>0.028147</td>\n",
       "      <td>0.990057</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.082206</td>\n",
       "      <td>0.767614</td>\n",
       "      <td>0.946023</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.073394</td>\n",
       "      <td>1.637862</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.071335</td>\n",
       "      <td>0.842607</td>\n",
       "      <td>0.901989</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.075052</td>\n",
       "      <td>0.100458</td>\n",
       "      <td>0.974432</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#colab\n",
    "learner.xla_fit(n_epoch=5, lr=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#2) [0.04073491320014,0.9871244430541992]"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#colab\n",
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
