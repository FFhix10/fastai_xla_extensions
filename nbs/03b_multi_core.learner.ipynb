{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "# attach gdrive holding repo\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp multi_core.learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Core XLA Learner extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/butchland/fastai_xla_extensions/blob/master/nbs/03b_multi_core.learner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Learner method patches to invoke multi-core `fit` and other operations prefixed by `xla_`. \n",
    "\n",
    "> These provide an alternate way to run multi core operations with minimal changes to existing fastai notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 133.6MB 93kB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 2.9MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -Uqq cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp37-cp37m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 194kB 4.7MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 3.4MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "# !pip install -Uqq git+https://github.com/fastai/fastai.git \n",
    "!pip install -Uqq fastai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 51kB 2.0MB/s \n",
      "\u001b[K     |████████████████████████████████| 51kB 2.9MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -qqq nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for my-timesaver-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -Uqq git+https://github.com/butchland/my_timesaver_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating fastai...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!curl -s https://course19.fast.ai/setup/colab | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==1.7.1+cu101\n",
      "torch-xla==1.7\n",
      "torchsummary==1.5.1\n",
      "torchtext==0.3.1\n",
      "torchvision==0.8.2+cu101\n",
      "fastai==2.2.7\n",
      "fastcore==1.3.19\n",
      "fastdtw==0.3.4\n",
      "fastprogress==1.0.0\n",
      "fastrelease==0.1.11\n",
      "fastrlock==0.5\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!pip freeze | grep torch\n",
    "!pip freeze | grep fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "%cd /content\n",
    "!ln -s /content/drive/MyDrive/fastai_xla_extensions fastai_xla_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Start of kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/fastai_xla_extensions\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "%cd /content/fastai_xla_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "try:\n",
    "    import torch_xla\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Waiting for TPU to be start up with version pytorch-1.7...\n",
      "WARNING:root:Waiting for TPU to be start up with version pytorch-1.7...\n",
      "WARNING:root:TPU has started up successfully with version pytorch-1.7\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from fastai_xla_extensions.utils import xla_imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai_xla_extensions.multi_core.base import *\n",
    "from fastai_xla_extensions.misc_utils import *\n",
    "from fastai_xla_extensions.multi_core.callback import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "# import sys\n",
    "# def xla_imported():\n",
    "#     return 'torch_xla' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "if not xla_imported():\n",
    "    # replace torch xla modules with fake equivalents\n",
    "    from types import SimpleNamespace\n",
    "    torch_xla = SimpleNamespace (\n",
    "    )\n",
    "    from typing import Union,BinaryIO\n",
    "    import os\n",
    "    import pickle\n",
    "    import torch.cuda\n",
    "\n",
    "    def fake_opt_step(opt,barrier=False):\n",
    "        opt.step()\n",
    "        \n",
    "    def fake_device(n=None, devkind=None):\n",
    "        gpu_available = torch.cuda.is_available()\n",
    "        if gpu_available:\n",
    "            return torch.device(torch.cuda.current_device()) \n",
    "        return torch.device('cpu')\n",
    "\n",
    "    def fake_save(obj, f: Union[str, os.PathLike, BinaryIO], \n",
    "                master_only=True, global_master=False): \n",
    "        return torch.save(obj,f,pickle_module=pickle, \n",
    "                        pickle_protocol=2, \n",
    "                        _use_new_zipfile_serialization=True)\n",
    "    def fake_rate():\n",
    "        return 230.20\n",
    "\n",
    "    def fake_global_rate():\n",
    "        return 830.10\n",
    "\n",
    "    def fake_add(*args,**kwargs):\n",
    "        pass\n",
    "\n",
    "    def fake_RateTracker():\n",
    "        return SimpleNamespace(\n",
    "            rate = fake_rate,\n",
    "            global_rate = fake_global_rate,\n",
    "            add = fake_add\n",
    "        )\n",
    "    def fake_xrt_world_size():\n",
    "        return 1\n",
    "    def fake_get_ordinal():\n",
    "        return 0\n",
    "    xm = SimpleNamespace(\n",
    "        optimizer_step = fake_opt_step,\n",
    "        xla_device = fake_device,\n",
    "        save = fake_save,\n",
    "        RateTracker = fake_RateTracker,\n",
    "        master_print = print,\n",
    "        xrt_world_size = fake_xrt_world_size,\n",
    "        get_ordinal = fake_get_ordinal\n",
    "    )\n",
    "\n",
    "    def fake_metrics_report():\n",
    "        return \"Fake Metrics Report \\n\\n\\n\\n\"\n",
    "    met = SimpleNamespace (\n",
    "        metrics_report = fake_metrics_report\n",
    "    )\n",
    "\n",
    "    class FakeParallelLoader:\n",
    "        def __init__(self, loader, *args):\n",
    "            self.loader = loader\n",
    "        def per_device_loader(self,device):\n",
    "            return self.loader\n",
    "        \n",
    "    pl = SimpleNamespace(\n",
    "        ParallelLoader = FakeParallelLoader\n",
    "    )\n",
    "\n",
    "    def fake_MpModelWrapper(o):\n",
    "        return o\n",
    "\n",
    "    def fake_run(f,*args, **kwargs):\n",
    "            return f(*args,**kwargs)\n",
    "        \n",
    "    def fake_MpSerialExecutor():\n",
    "        return SimpleNamespace(\n",
    "            run = fake_run\n",
    "        )\n",
    "    def fake_spawn(f, args=None, nprocs=0, start_method=None):\n",
    "        return f(0,*args)\n",
    "\n",
    "    xmp = SimpleNamespace (\n",
    "        MpModelWrapper = fake_MpModelWrapper,\n",
    "        MpSerialExecutor = fake_MpSerialExecutor,\n",
    "        spawn = fake_spawn\n",
    "    )\n",
    "\n",
    "    xu = SimpleNamespace (\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "if xla_imported():\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add master_cbs property to Learner \n",
    "\n",
    "> Master callbacks are callbacks that will be executed on the master ordinal (rank 0 thread) only.\n",
    "\n",
    "This means existing fastai notebooks must be checked if any additional callbacks used\n",
    "can cause conflicts if run on different threads at the same time.\n",
    "\n",
    "Note that for default callbacks (`TrainEvalCallback`, `Recorder`, `ProgressCallback`) only `ProgressCallback` causes this problem. \n",
    "\n",
    "However, the `fastai_xla_extensions.multi_core.base` module already handles\n",
    "this so that if used (which it is, by default), the `ProgressCallback` is attached only on the master ordinal thread.\n",
    "\n",
    "Moreover, the `Recorder` callback is also handled such that validation losses and metrics are collated correctly by the `fastai_xla_extensions.multi_core.base.SyncRecorderCallback` so that the validation metrics and losses are reported correctly at the end of each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastcore.basics import patch\n",
    "from fastai.learner import Learner\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.foundation import L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from fastai.learner import Learner\n",
    "from fastcore.basics import patch\n",
    "@patch(as_prop=True)\n",
    "def master_cbs(self:Learner):\n",
    "    \"list all cbs to be run on the master ordinal thread\"\n",
    "    if not hasattr(self,'_master_cbs'):\n",
    "        self._master_cbs = L()\n",
    "    return self._master_cbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"Learner.master_cbs\" class=\"doc_header\"><code>Learner.master_cbs</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\nlist all cbs to be run on the master ordinal thread",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "from fastai.learner import Learner\n",
    "show_doc(Learner.master_cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def add_master_cb(self:Learner, cb):\n",
    "    \"add a master callback\"\n",
    "    if not hasattr(self,'_master_cbs'):\n",
    "        self._master_cbs = L()\n",
    "    if isinstance(cb, type): cb = cb()\n",
    "#     cb.learn = self\n",
    "#     setattr(self, cb.name, cb)\n",
    "    self._master_cbs.append(cb)\n",
    "    \n",
    "@patch\n",
    "def add_master_cbs(self:Learner, cbs):\n",
    "    \"add master callbacks\"\n",
    "    L(cbs).map(self.add_master_cb)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"Learner.add_master_cb\" class=\"doc_header\"><code>Learner.add_master_cb</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Learner.add_master_cb</code>(**`cb`**)\n\nadd a master callback",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": "<h4 id=\"Learner.add_master_cbs\" class=\"doc_header\"><code>Learner.add_master_cbs</code><a href=\"__main__.py#L12\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Learner.add_master_cbs</code>(**`cbs`**)\n\nadd master callbacks",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "show_doc(Learner.add_master_cb)\n",
    "show_doc(Learner.add_master_cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@patch\n",
    "def grab_master_cbs(self:Learner, cb_cls):\n",
    "    \"find instance of `cb_cls` in master_cbs\"\n",
    "    return L(cb for cb in self._master_cbs if isinstance(cb, cb_cls))\n",
    "\n",
    "@patch\n",
    "def remove_master_cb(self:Learner, cb):\n",
    "    \"remove a cb from master callbacks\"\n",
    "    if isinstance(cb, type): self.remove_master_cbs(self.grab_master_cbs(cb))\n",
    "    else:\n",
    "#         cb.learn = None\n",
    "#         if hasattr(self, cb.name): delattr(self, cb.name)\n",
    "        if cb in self._master_cbs: self._master_cbs.remove(cb)\n",
    "    return self\n",
    "\n",
    "@patch\n",
    "def remove_master_cbs(self:Learner, cbs):\n",
    "    \"remove callbacks from master callbacks\"\n",
    "    L(cbs).map(self.remove_master_cb)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"Learner.grab_master_cbs\" class=\"doc_header\"><code>Learner.grab_master_cbs</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Learner.grab_master_cbs</code>(**`cb_cls`**)\n\nfind instance of `cb_cls` in master_cbs",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": "<h4 id=\"Learner.remove_master_cbs\" class=\"doc_header\"><code>Learner.remove_master_cbs</code><a href=\"__main__.py#L18\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Learner.remove_master_cbs</code>(**`cbs`**)\n\nremove callbacks from master callbacks",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": "<h4 id=\"Learner.remove_master_cb\" class=\"doc_header\"><code>Learner.remove_master_cb</code><a href=\"__main__.py#L8\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Learner.remove_master_cb</code>(**`cb`**)\n\nremove a cb from master callbacks",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "show_doc(Learner.grab_master_cbs)\n",
    "show_doc(Learner.remove_master_cbs)\n",
    "show_doc(Learner.remove_master_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility methods to implement XLA `fit` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from fastai.callback.progress import ProgressCallback\n",
    "from fastai.learner import Learner\n",
    "\n",
    "def make_xla_child_learner(rank, sync_valid,learner_args, add_args, ctrl_args):\n",
    "    \"create a learner using passed parameters\"\n",
    "    device = xm.xla_device()\n",
    "    world_size = xm.xrt_world_size()\n",
    "    dls = build_distributed_dataloaders(learner_args.pop('base_dls'),\n",
    "                                       rank, world_size, sync_valid=sync_valid)\n",
    "\n",
    "    model = learner_args.pop('wrapped_model').to(device)\n",
    "    master_cbs = learner_args.pop('master_cbs')\n",
    "    if master_cbs is None:\n",
    "        master_cbs = L()\n",
    "    learner = Learner(dls, model,**learner_args)\n",
    "    learner.__stored_args__ = {**learner.__stored_args__, **add_args}\n",
    "\n",
    "    learner.to_multi_xla(device, rank, sync_valid=sync_valid)\n",
    "    \n",
    "    if not ctrl_args['use_progress'] and 'progress' in L(learner.cbs).attrgot('name'):\n",
    "        learner.remove_cbs(ProgressCallback)\n",
    "        \n",
    "    if rank == 0: \n",
    "        learner.add_cbs(master_cbs)\n",
    "        \n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def setup_fit_cbs(rank, fit_args):\n",
    "    \"add master cbs to cbs fit args if rank 0\"\n",
    "    master_cbs = L(fit_args.pop('master_cbs'))\n",
    "    if rank != 0:\n",
    "        master_cbs = L()\n",
    "    if 'cbs' in fit_args:\n",
    "        cbs = L(fit_args.pop('cbs'))\n",
    "    else:\n",
    "        cbs = L()\n",
    "    if len(master_cbs) > 0 or len(cbs) > 0: \n",
    "        fit_args['cbs'] = [*cbs, *master_cbs]  \n",
    "    return fit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def xla_run_method(rank, fit_method, learner_args, add_args, fit_args, ctrl_args):\n",
    "    \"run fit method on spawned process\"\n",
    "    sync_valid = True\n",
    "    learner = make_xla_child_learner(rank, sync_valid, learner_args, add_args, ctrl_args)    \n",
    "    fit_args = setup_fit_cbs(rank, fit_args)\n",
    "    fit_method(learner, **fit_args)\n",
    "    xm.rendezvous('xla_run_method')\n",
    "    learner.save('_xla_tmp_model',rendezvous=False)\n",
    "    xm.mark_step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.basics import defaults, patch_to, patch\n",
    "\n",
    "_extra_args = ['concat_pool', 'arch', 'n_out', 'pretrained','normalize']\n",
    "\n",
    "@patch\n",
    "def pack_learner_args(self:Learner):\n",
    "    \"pack learner args into dict to pass to spawned process\"\n",
    "    learner_args = {**self.__stored_args__}\n",
    "    learner_args['wrapped_model'] =  xmp.MpModelWrapper(self.model)\n",
    "    learner_args['base_dls'] = self.dls\n",
    "    # fetch only cbs not in defaults\n",
    "    if ProgressCallback not in defaults.callbacks:\n",
    "        defaults.callbacks.append(ProgressCallback)\n",
    "    default_cbs = [cls() for cls in defaults.callbacks]\n",
    "    learner_args['cbs'] = [cb for cb in self.cbs\n",
    "                      if cb.name not in L(default_cbs).attrgot('name')]\n",
    "    \n",
    "    learner_args['master_cbs'] = self.master_cbs \n",
    "    \n",
    "    # remove extra args from learner args (in __stored_args__ but not in init args)\n",
    "    add_args = {}\n",
    "    for arg in _extra_args:\n",
    "        if arg in learner_args:\n",
    "            add_args[arg] = learner_args.pop(arg)\n",
    "    return learner_args, add_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "\n",
    "@patch\n",
    "def reload_child_model(self:Learner):\n",
    "    \"reload model built by spawned processes\"\n",
    "    # blatantly stolen from fastai LRFinder after_fit :)\n",
    "    tmp_f = self.path/self.model_dir/'_xla_tmp_model.pth'\n",
    "    if tmp_f.exists():\n",
    "        self.opt.zero_grad()\n",
    "        self.load('_xla_tmp_model', with_opt=False)\n",
    "        os.remove(tmp_f)\n",
    "        self.create_opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from fastcore.foundation import L\n",
    "from pathlib import Path\n",
    "\n",
    "tmp_files = ['_paramsched_hps.pkl', '_rec_attr.pkl']\n",
    "@patch\n",
    "def delete_tmp_files(self:Learner):\n",
    "    '''remove files created by spawned process prior to \n",
    "    potentially recreating them'''\n",
    "    for fn in tmp_files:\n",
    "        fn = Path(fn)\n",
    "        if fn.is_file():\n",
    "            fn.unlink()\n",
    "\n",
    "\n",
    "@patch\n",
    "def pre_xla_fit(self:Learner, ctrl_args={}):\n",
    "    \"prepare learner for running spawned processes\"\n",
    "    progress_removed = False\n",
    "    if 'progress' in L(self.cbs).attrgot('name'):\n",
    "        self.remove_cbs(ProgressCallback)\n",
    "        progress_removed = True\n",
    "    ctrl_args['use_progress'] = progress_removed\n",
    "    self.delete_tmp_files()\n",
    "    return ctrl_args\n",
    "\n",
    "@patch\n",
    "def post_xla_fit(self:Learner, ctrl_args):\n",
    "    \"clean up learner after running spawned processes\"\n",
    "    self.recorder.reload_attrs()\n",
    "    self.recorder.reload_hps()\n",
    "    if ctrl_args['use_progress']:\n",
    "        self.add_cbs(ProgressCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prep_fit_args(n_epoch, master_cbs, **kwargs):\n",
    "    \"prepare fit method args for running spawned processes\"\n",
    "    fit_args={**kwargs}\n",
    "    fit_args['master_cbs'] = master_cbs\n",
    "    fit_args['n_epoch'] = n_epoch \n",
    "    return fit_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLA fit methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from fastcore.meta import delegates\n",
    "\n",
    "@patch\n",
    "@delegates(Learner.fit, but='num_cores,start_method,master_cbs')\n",
    "def xla_fit(self:Learner, n_epoch, num_cores=8, \n",
    "            start_method='fork', master_cbs=None, **kwargs):\n",
    "    \"\"\"call fit in a multicore tpu environment\"\"\"\n",
    "    ctrl_args = self.pre_xla_fit()\n",
    "    learner_args, add_args = self.pack_learner_args()\n",
    "    \n",
    "    fit_args = prep_fit_args(n_epoch, master_cbs, **kwargs)\n",
    "   \n",
    "    xmp.spawn(xla_run_method,\n",
    "              args=(Learner.fit, learner_args, add_args, fit_args, ctrl_args),\n",
    "              nprocs=num_cores,\n",
    "              start_method=start_method)\n",
    "\n",
    "    self.reload_child_model()\n",
    "    self.post_xla_fit(ctrl_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.learner import Learner\n",
    "from fastai.callback.schedule import *\n",
    "@patch\n",
    "@delegates(Learner.fit_one_cycle, but='num_cores,start_method,master_cbs')\n",
    "def xla_fit_one_cycle(self:Learner, n_epoch, num_cores=8, \n",
    "                      start_method='fork', master_cbs=None, **kwargs):\n",
    "    \"\"\"call fit_one_cycle in a multicore tpu environment\"\"\"\n",
    "    ctrl_args = self.pre_xla_fit()\n",
    "    learner_args, add_args = self.pack_learner_args()\n",
    "    \n",
    "    fit_args = prep_fit_args(n_epoch, master_cbs, **kwargs)\n",
    "    \n",
    "    xmp.spawn(xla_run_method,\n",
    "              args=(Learner.fit_one_cycle, learner_args, add_args, fit_args, ctrl_args),\n",
    "              nprocs=num_cores,\n",
    "              start_method=start_method)\n",
    "\n",
    "    self.reload_child_model()\n",
    "    self.post_xla_fit(ctrl_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.learner import Learner\n",
    "from fastai.callback.schedule import *\n",
    "@patch\n",
    "@delegates(Learner.fit_flat_cos, but='num_cores,start_method,master_cbs')\n",
    "def xla_fit_flat_cos(self:Learner, n_epoch, num_cores=8, \n",
    "                      start_method='fork', master_cbs=None, **kwargs):\n",
    "    \"\"\"call fit_flat_cos in a multicore tpu environment\"\"\"\n",
    "    ctrl_args = self.pre_xla_fit()\n",
    "    learner_args, add_args = self.pack_learner_args()\n",
    "    \n",
    "    fit_args = prep_fit_args(n_epoch, master_cbs, **kwargs)\n",
    "    \n",
    "    xmp.spawn(xla_run_method,\n",
    "              args=(Learner.fit_flat_cos, learner_args, add_args, fit_args, ctrl_args),\n",
    "              nprocs=num_cores,\n",
    "              start_method=start_method)\n",
    "\n",
    "    self.reload_child_model()\n",
    "    self.post_xla_fit(ctrl_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.learner import Learner\n",
    "from fastai.callback.schedule import *\n",
    "\n",
    "def prep_fit_sgdr_args(n_cycles, cycle_len, master_cbs, **kwargs):\n",
    "    \"prepare fit_sgdr method args for running spawned processes\"\n",
    "    fit_args={**kwargs}\n",
    "    fit_args['master_cbs'] = master_cbs\n",
    "    fit_args['n_cycles'] = n_cycles\n",
    "    fit_args['cycle_len'] = cycle_len\n",
    "    return fit_args    \n",
    "\n",
    "@patch\n",
    "@delegates(Learner.fit_sgdr, but='num_cores,start_method,master_cbs')\n",
    "def xla_fit_sgdr(self:Learner, n_cycles, cycle_len, num_cores=8, \n",
    "                      start_method='fork', master_cbs=None, **kwargs):\n",
    "    \"\"\"call fit_sgdr in multicore tpu environment\"\"\"\n",
    "    ctrl_args = self.pre_xla_fit()\n",
    "    learner_args, add_args = self.pack_learner_args()\n",
    "    fit_args = prep_fit_sgdr_args(n_cycles, cycle_len, master_cbs, **kwargs)\n",
    "    \n",
    "    xmp.spawn(xla_run_method,\n",
    "              args=(Learner.fit_sgdr, learner_args, add_args, fit_args, ctrl_args),\n",
    "              nprocs=num_cores,\n",
    "              start_method=start_method)\n",
    "\n",
    "    self.reload_child_model()\n",
    "    self.post_xla_fit(ctrl_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.learner import Learner\n",
    "from fastai.callback.schedule import *\n",
    "\n",
    "def prep_finetune_args(epochs, master_cbs, **kwargs):\n",
    "    \"prepare finetune method args for running spawned processes\"\n",
    "    fit_args={**kwargs}\n",
    "    fit_args['master_cbs'] = master_cbs\n",
    "    fit_args['epochs'] = epochs\n",
    "    return fit_args\n",
    "\n",
    "@patch\n",
    "@delegates(Learner.fine_tune, but='num_cores,start_method,master_cbs')\n",
    "def xla_fine_tune(self:Learner, epochs, num_cores=8, \n",
    "                      start_method='fork', master_cbs=None, **kwargs):\n",
    "    \"\"\"call fine_tune in multicore tpu environment\"\"\"\n",
    "    ctrl_args = self.pre_xla_fit()\n",
    "    learner_args, add_args = self.pack_learner_args()\n",
    "    \n",
    "    fit_args = prep_finetune_args(epochs, master_cbs, **kwargs)\n",
    "    \n",
    "    xmp.spawn(xla_run_method,\n",
    "              args=(Learner.fine_tune, learner_args, add_args, fit_args, ctrl_args),\n",
    "              nprocs=num_cores,\n",
    "              start_method=start_method)\n",
    "\n",
    "    self.reload_child_model()\n",
    "    self.post_xla_fit(ctrl_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Train MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#colab\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#colab\n",
    "path = untar_data(URLs.MNIST_TINY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "data = DataBlock(\n",
    "    blocks=(ImageBlock,CategoryBlock),\n",
    "    get_items=get_image_files,\n",
    "    get_y=parent_label,\n",
    "    splitter=GrandparentSplitter(),\n",
    "    item_tfms=Resize(28),\n",
    "    batch_tfms=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "dls = data.dataloaders(path, bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# concat_pool must be false due to a TPU bug that is triggered if using fastai AdaptivePool\n",
    "from fastai.vision.learner import cnn_learner\n",
    "from torchvision.models.resnet import resnet18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "learner = cnn_learner(dls, resnet18, metrics=accuracy, concat_pool=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "learner.add_master_cbs([SaveModelCallback(fname='best_model')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "#hide\n",
    "assert hasattr(learner,'xla_fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "class PrintValuesCallback(Callback):\n",
    "    order = 56 # after recorder, sync recorder, before save model callback  \n",
    "    def after_epoch(self):\n",
    "        print(f'final record: {self.learn.final_record}')\n",
    "        vlen = len(self.recorder.values)\n",
    "        print(f'values len: {vlen}')\n",
    "        if vlen > 0:   \n",
    "            last_idx = self.recorder.values[-1]  \n",
    "            len_last_idx = len(last_idx)\n",
    "            print(f'values last idx len: {len_last_idx}')\n",
    "            print(f'last idx: {last_idx}')\n",
    "            if 'save_model' in L(self.cbs).attrgot('name'):\n",
    "                save_model_idx = self.save_model.idx\n",
    "                print(f'save_model idx: {save_model_idx}')     \n",
    "                if save_model_idx < len_last_idx:\n",
    "                    val = self.recorder.values[-1][self.save_model.idx]\n",
    "                    print(f'best_value: {val}')\n",
    "        if 'sync_recorder' in L(self.cbs).attrgot('name'):\n",
    "            sync_log = self.sync_recorder.sync_log\n",
    "            len_sync_log = len(sync_log)\n",
    "            print(f'sync rec sync_log len: {len_sync_log}')\n",
    "            print(f'sync rec sync_log: {sync_log}')\n",
    "\n",
    "            if len_sync_log > 0:\n",
    "                print(f'sync rec sync_log[1:]: {sync_log[1:]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# cbs = [PrintValuesCallback(), SaveModelCallback(fname='best_model')]\n",
    "cbs = [PrintValuesCallback()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.248156</td>\n",
       "      <td>0.515241</td>\n",
       "      <td>0.813920</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.200013</td>\n",
       "      <td>0.743561</td>\n",
       "      <td>0.538352</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.246930</td>\n",
       "      <td>0.687081</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.261705</td>\n",
       "      <td>0.880469</td>\n",
       "      <td>0.538352</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.295985</td>\n",
       "      <td>1.625116</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.5152414441108704.\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "learner.xla_fit_one_cycle(5,lr_max=slice(2e-3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBase(0.7568)\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "res = learner.get_preds()\n",
    "print(accuracy(*res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/fastai/learner.py:56: UserWarning: Saved filed doesn't contain an optimizer state.\n",
      "  elif with_opt: warn(\"Saved filed doesn't contain an optimizer state.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fastai.learner.Learner at 0x7f70b9c63e50>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#colab\n",
    "learner.load('best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBase(0.7568)\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "res = learner.get_preds()\n",
    "print(accuracy(*res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Sequential (Input shape: 16)\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     16 x 64 x 14 x 14   \n",
       "Conv2d                                    9408       False     \n",
       "BatchNorm2d                               128        True      \n",
       "ReLU                                                           \n",
       "MaxPool2d                                                      \n",
       "Conv2d                                    36864      False     \n",
       "BatchNorm2d                               128        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    36864      False     \n",
       "BatchNorm2d                               128        True      \n",
       "Conv2d                                    36864      False     \n",
       "BatchNorm2d                               128        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    36864      False     \n",
       "BatchNorm2d                               128        True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 128 x 4 x 4    \n",
       "Conv2d                                    73728      False     \n",
       "BatchNorm2d                               256        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    147456     False     \n",
       "BatchNorm2d                               256        True      \n",
       "Conv2d                                    8192       False     \n",
       "BatchNorm2d                               256        True      \n",
       "Conv2d                                    147456     False     \n",
       "BatchNorm2d                               256        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    147456     False     \n",
       "BatchNorm2d                               256        True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 256 x 2 x 2    \n",
       "Conv2d                                    294912     False     \n",
       "BatchNorm2d                               512        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    589824     False     \n",
       "BatchNorm2d                               512        True      \n",
       "Conv2d                                    32768      False     \n",
       "BatchNorm2d                               512        True      \n",
       "Conv2d                                    589824     False     \n",
       "BatchNorm2d                               512        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    589824     False     \n",
       "BatchNorm2d                               512        True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 512 x 1 x 1    \n",
       "Conv2d                                    1179648    False     \n",
       "BatchNorm2d                               1024       True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    2359296    False     \n",
       "BatchNorm2d                               1024       True      \n",
       "Conv2d                                    131072     False     \n",
       "BatchNorm2d                               1024       True      \n",
       "Conv2d                                    2359296    False     \n",
       "BatchNorm2d                               1024       True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    2359296    False     \n",
       "BatchNorm2d                               1024       True      \n",
       "AdaptiveAvgPool2d                                              \n",
       "Flatten                                                        \n",
       "BatchNorm1d                               1024       True      \n",
       "Dropout                                                        \n",
       "Linear                                    262144     True      \n",
       "ReLU                                                           \n",
       "BatchNorm1d                               1024       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     16 x 2              \n",
       "Linear                                    1024       True      \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 11,441,728\n",
       "Total trainable params: 274,816\n",
       "Total non-trainable params: 11,166,912\n",
       "\n",
       "Optimizer used: <function Adam at 0x7f70ca86d440>\n",
       "Loss function: FlattenedLoss of CrossEntropyLoss()\n",
       "\n",
       "Model unfrozen\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - Recorder\n",
       "  - ProgressCallback"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Sequential (Input shape: 16)\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     16 x 64 x 14 x 14   \n",
       "Conv2d                                    9408       True      \n",
       "BatchNorm2d                               128        True      \n",
       "ReLU                                                           \n",
       "MaxPool2d                                                      \n",
       "Conv2d                                    36864      True      \n",
       "BatchNorm2d                               128        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    36864      True      \n",
       "BatchNorm2d                               128        True      \n",
       "Conv2d                                    36864      True      \n",
       "BatchNorm2d                               128        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    36864      True      \n",
       "BatchNorm2d                               128        True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 128 x 4 x 4    \n",
       "Conv2d                                    73728      True      \n",
       "BatchNorm2d                               256        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    147456     True      \n",
       "BatchNorm2d                               256        True      \n",
       "Conv2d                                    8192       True      \n",
       "BatchNorm2d                               256        True      \n",
       "Conv2d                                    147456     True      \n",
       "BatchNorm2d                               256        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    147456     True      \n",
       "BatchNorm2d                               256        True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 256 x 2 x 2    \n",
       "Conv2d                                    294912     True      \n",
       "BatchNorm2d                               512        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    589824     True      \n",
       "BatchNorm2d                               512        True      \n",
       "Conv2d                                    32768      True      \n",
       "BatchNorm2d                               512        True      \n",
       "Conv2d                                    589824     True      \n",
       "BatchNorm2d                               512        True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    589824     True      \n",
       "BatchNorm2d                               512        True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 512 x 1 x 1    \n",
       "Conv2d                                    1179648    True      \n",
       "BatchNorm2d                               1024       True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    2359296    True      \n",
       "BatchNorm2d                               1024       True      \n",
       "Conv2d                                    131072     True      \n",
       "BatchNorm2d                               1024       True      \n",
       "Conv2d                                    2359296    True      \n",
       "BatchNorm2d                               1024       True      \n",
       "ReLU                                                           \n",
       "Conv2d                                    2359296    True      \n",
       "BatchNorm2d                               1024       True      \n",
       "AdaptiveAvgPool2d                                              \n",
       "Flatten                                                        \n",
       "BatchNorm1d                               1024       True      \n",
       "Dropout                                                        \n",
       "Linear                                    262144     True      \n",
       "ReLU                                                           \n",
       "BatchNorm1d                               1024       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     16 x 2              \n",
       "Linear                                    1024       True      \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 11,441,728\n",
       "Total trainable params: 11,441,728\n",
       "Total non-trainable params: 0\n",
       "\n",
       "Optimizer used: <function Adam at 0x7f70ca86d440>\n",
       "Loss function: FlattenedLoss of CrossEntropyLoss()\n",
       "\n",
       "Model unfrozen\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - Recorder\n",
       "  - ProgressCallback"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#colab\n",
    "one_param(learner.model).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.050473</td>\n",
       "      <td>10.583854</td>\n",
       "      <td>0.492898</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.047812</td>\n",
       "      <td>2.253268</td>\n",
       "      <td>0.720170</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.056125</td>\n",
       "      <td>0.019719</td>\n",
       "      <td>0.991477</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.051844</td>\n",
       "      <td>0.034788</td>\n",
       "      <td>0.990057</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.047623</td>\n",
       "      <td>0.010792</td>\n",
       "      <td>0.997159</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 10.583853721618652.\n",
      "Better model found at epoch 1 with valid_loss value: 2.253268003463745.\n",
      "Better model found at epoch 2 with valid_loss value: 0.01971946842968464.\n",
      "Better model found at epoch 4 with valid_loss value: 0.010792052373290062.\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "learner.xla_fit(n_epoch=5, lr=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#2) [0.012101042084395885,0.9942775368690491]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#colab\n",
    "learner.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using torch datasets and dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "FLAGS = {}\n",
    "FLAGS['batch_size']  = 64\n",
    "FLAGS['num_workers'] = 4\n",
    "FLAGS['data_dir'] = Path('/content/data/cifar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    norm = transforms.Normalize(\n",
    "        mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        norm,\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        norm,\n",
    "    ])\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=FLAGS['data_dir'],\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform_train)\n",
    "    test_dataset = datasets.CIFAR10(\n",
    "        root=FLAGS['data_dir'],\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform_test)\n",
    "    \n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/data/cifar/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c365b42eb3df41ae9b6840cdfea71891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /content/data/cifar/cifar-10-python.tar.gz to /content/data/cifar\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "train_dataset, test_dataset = get_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=FLAGS['batch_size'],\n",
    "#   sampler=train_sampler,\n",
    "    shuffle=True,\n",
    "    num_workers=FLAGS['num_workers'],\n",
    "    drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=FLAGS['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=FLAGS['num_workers'],\n",
    "    drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# fastai dls using torch dataloaders\n",
    "dls = DataLoaders(train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "learner = cnn_learner(dls, resnet18, metrics=accuracy, \n",
    "                      n_out=10, \n",
    "                      loss_func=nn.CrossEntropyLoss(),\n",
    "                      concat_pool=False \n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.275580</td>\n",
       "      <td>1.169747</td>\n",
       "      <td>0.596968</td>\n",
       "      <td>01:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.121143</td>\n",
       "      <td>0.998941</td>\n",
       "      <td>0.652231</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.024472</td>\n",
       "      <td>1.067758</td>\n",
       "      <td>0.639523</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.993983</td>\n",
       "      <td>0.907262</td>\n",
       "      <td>0.687425</td>\n",
       "      <td>01:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.973521</td>\n",
       "      <td>0.885192</td>\n",
       "      <td>0.695187</td>\n",
       "      <td>01:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#colab\n",
    "learner.xla_fit(5,lr=2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
