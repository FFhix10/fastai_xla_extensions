{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "# attach gdrive holding repo\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp multi_core.base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Core XLA Base \n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/butchland/fastai_xla_extensions/blob/master/nbs/03_multi_core.base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Base module for Multi TPU Core implementation\n",
    "\n",
    "Multi-core TPU implementation is enabled by importing this module.\n",
    "```\n",
    "from fastai_xla_extensions.multi_core.base import *\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 133.6MB 90kB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 3.0MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -Uqq cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 194kB 2.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 2.9MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "# !pip install -Uqq git+https://github.com/fastai/fastai.git \n",
    "!pip install -Uqq fastai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for my-timesaver-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -Uqq git+https://github.com/butchland/my_timesaver_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r\u001b[K     |███████▏                        | 10kB 20.2MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 20kB 23.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 30kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 40kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 3.0MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -qqq nbdev --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating fastai...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!curl -s https://course19.fast.ai/setup/colab | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==1.7.0+cu101\n",
      "torch-xla==1.7\n",
      "torchsummary==1.5.1\n",
      "torchtext==0.3.1\n",
      "torchvision==0.8.1+cu101\n",
      "fastai==2.2.5\n",
      "fastcore==1.3.19\n",
      "fastdtw==0.3.4\n",
      "fastprogress==1.0.0\n",
      "fastrlock==0.5\n",
      "my-timesaver-utils==0.0.2\n",
      "nbdev==1.1.12\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!pip freeze | grep torch\n",
    "!pip freeze | grep fast\n",
    "!pip freeze | grep timesaver\n",
    "!pip freeze | grep nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "# link repo to work dir\n",
    "%cd /content\n",
    "!ln -s /content/drive/MyDrive/fastai_xla_extensions fastai_xla_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# <!-- Start of kernel -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/fastai_xla_extensions\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "%cd /content/fastai_xla_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:TPU has started up successfully with version pytorch-1.7\n"
     ]
    }
   ],
   "source": [
    "#exporti\n",
    "\n",
    "#from fastai.vision.all import *\n",
    "from fastai_xla_extensions.utils import xla_imported\n",
    "from fastai_xla_extensions.misc_utils import *\n",
    "from fastai_xla_extensions.core import XLAOptCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "try:\n",
    "    import torch_xla\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "# fake out torch_xla modules if not running on xla supported envs\n",
    "if not xla_imported():\n",
    "    # replace torch xla modules with fake equivalents\n",
    "    from types import SimpleNamespace\n",
    "    torch_xla = SimpleNamespace (\n",
    "    )\n",
    "    from typing import Union,BinaryIO\n",
    "    import os\n",
    "    import pickle\n",
    "    import torch.cuda\n",
    "\n",
    "    def fake_opt_step(opt,barrier=False):\n",
    "        opt.step()\n",
    "        \n",
    "    def fake_device(n=None, devkind=None):\n",
    "        gpu_available = torch.cuda.is_available()\n",
    "        if gpu_available:\n",
    "            return torch.device(torch.cuda.current_device()) \n",
    "        return torch.device('cpu')\n",
    "\n",
    "    def fake_save(obj, f: Union[str, os.PathLike, BinaryIO], \n",
    "                master_only=True, global_master=False): \n",
    "        return torch.save(obj,f,pickle_module=pickle, \n",
    "                        pickle_protocol=2, \n",
    "                        _use_new_zipfile_serialization=True)\n",
    "    def fake_rate():\n",
    "        return 230.20\n",
    "\n",
    "    def fake_global_rate():\n",
    "        return 830.10\n",
    "\n",
    "    def fake_add(*args,**kwargs):\n",
    "        pass\n",
    "\n",
    "    def fake_RateTracker():\n",
    "        return SimpleNamespace(\n",
    "            rate = fake_rate,\n",
    "            global_rate = fake_global_rate,\n",
    "            add = fake_add\n",
    "        )\n",
    "    def fake_xrt_world_size():\n",
    "        return 1\n",
    "    def fake_get_ordinal():\n",
    "        return 0\n",
    "    xm = SimpleNamespace(\n",
    "        optimizer_step = fake_opt_step,\n",
    "        xla_device = fake_device,\n",
    "        save = fake_save,\n",
    "        RateTracker = fake_RateTracker,\n",
    "        master_print = print,\n",
    "        xrt_world_size = fake_xrt_world_size,\n",
    "        get_ordinal = fake_get_ordinal\n",
    "    )\n",
    "\n",
    "    def fake_metrics_report():\n",
    "        return \"Fake Metrics Report \\n\\n\\n\\n\"\n",
    "    met = SimpleNamespace (\n",
    "        metrics_report = fake_metrics_report\n",
    "    )\n",
    "\n",
    "    class FakeParallelLoader:\n",
    "        def __init__(self, loader, *args):\n",
    "            self.loader = loader\n",
    "        def per_device_loader(self,device):\n",
    "            return self.loader\n",
    "        \n",
    "    pl = SimpleNamespace(\n",
    "        ParallelLoader = FakeParallelLoader\n",
    "    )\n",
    "\n",
    "    def fake_MpModelWrapper(o):\n",
    "        return o\n",
    "\n",
    "    def fake_run(f,*args, **kwargs):\n",
    "            return f(*args,**kwargs)\n",
    "        \n",
    "    def fake_MpSerialExecutor():\n",
    "        return SimpleNamespace(\n",
    "            run = fake_run\n",
    "        )\n",
    "    def fake_spawn(f, args=None, nprocs=0, start_method=None):\n",
    "        return f(0,*args)\n",
    "\n",
    "    xmp = SimpleNamespace (\n",
    "        MpModelWrapper = fake_MpModelWrapper,\n",
    "        MpSerialExecutor = fake_MpSerialExecutor,\n",
    "        spawn = fake_spawn\n",
    "    )\n",
    "\n",
    "    xu = SimpleNamespace (\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "if xla_imported():\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.parallel_loader as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from fastcore.foundation import L\n",
    "from fastai.data.core import DataLoaders\n",
    "import math\n",
    "from fastcore.basics import store_attr\n",
    "from operator import attrgetter\n",
    "from fastai.data.load import _FakeLoader\n",
    "\n",
    "from fastai.torch_core import TensorBase\n",
    "import random\n",
    "from fastcore.basics import patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def revert_tensor(o):\n",
    "    \"Remove tensor subclass and revert to `torch.Tensor`\"\n",
    "    try:\n",
    "        o.__class__ = torch.Tensor\n",
    "    except:\n",
    "        raise RuntimeError(f'could not convert {o} to torch.Tensor')\n",
    "    return o\n",
    "\n",
    "def recast2tensor(o):\n",
    "    \"Recast `fastai.torch_core.TensorBase` subclassed tensors to torch.Tensors\"\n",
    "    if isinstance(o,TensorBase):\n",
    "        # return plain tensor since pl.parallelloader doesn't\n",
    "        # seem to work with tensor subclasses\n",
    "        # return torch.as_tensor(o.numpy())\n",
    "        # TODO: recreate bug in notebook gist to file bug to torch_xla team\n",
    "        return revert_tensor(o)\n",
    "    return o\n",
    "\n",
    "def round_to_multiple(number,multiple):\n",
    "    \"round up batch samples to fill number of cores\"\n",
    "    return int(math.ceil(number/multiple)*multiple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from fastai.data.core import TfmdDL\n",
    "\n",
    "class TPUDistributedDL(TfmdDL):\n",
    "    \"\"\"A `TfmdDL` which splits a batch into equal size pieces for each TPU core\n",
    "       It also recasts the output of a batch from a TensorBase subclass to\n",
    "       a regular tensor since the XLA Parallel loader doesn't seem to be compatible\n",
    "       to it.\n",
    "       Code implementation was based on @tmabraham's `TPUDistributedDL` implementation\n",
    "       here: https://github.com/tmabraham/fastai_tpu/blob/master/fastai_v2/tpu_distributed_dl.py\n",
    "    \"\"\"\n",
    "    _default = 'dl'\n",
    "    def __init__(self,dl,rank,world_size, seed=42):\n",
    "        store_attr()\n",
    "        self.bs,self.device,self.num_workers, \\\n",
    "        self.drop_last,self.dataset,self.offs,fake, self.shuffle = \\\n",
    "            attrgetter('bs','device','num_workers',\n",
    "                       'drop_last','dataset','offs','fake_l', 'shuffle')(dl)\n",
    "        self.fake_l = _FakeLoader(self, fake.pin_memory, fake.num_workers, fake.timeout,\n",
    "                                  persistent_workers=fake.persistent_workers)\n",
    "        self.epoch = 0\n",
    "        random.seed(self.seed)\n",
    "        # setting inner dl rng\n",
    "        self.dl.rng = random.Random(random.randint(0,2**32-1))\n",
    "        self.reset_rng()\n",
    "\n",
    "    def reset_rng(self):\n",
    "        random.seed(self.seed + self.epoch)\n",
    "        # setting outer dl rng\n",
    "        self.rng = random.Random(random.randint(0,2**32-1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return round_to_multiple(len(self.dl),self.world_size)//self.world_size\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def get_idxs(self):\n",
    "        idxs = self.dl.get_idxs()\n",
    "        # do your own shuffling which factors in self.epoch + self.seed in\n",
    "        # generating a random sequence (underlying self.dl does not)\n",
    "        if self.shuffle:\n",
    "            idxs = self.shuffle_fn(idxs)\n",
    "        self.n = len(idxs)\n",
    "        # we assumed n was dl.n but we really care about number of idxs\n",
    "        # add extra samples to make it evenly divisible\n",
    "        self.n_padded = _round_to_multiple(self.n,self.world_size)\n",
    "        idxs += (idxs * (self.n_padded//self.n))[:self.n_padded-self.n]\n",
    "        # idx needs to be repeated when n_padded>>n\n",
    "        # slice padded idxs so that each rank gets self.n_padded//self.world_size tensors\n",
    "        start_pos = self.rank*self.n_padded//self.world_size\n",
    "        end_pos = (self.rank+1)*self.n_padded//self.world_size\n",
    "        return idxs[start_pos:end_pos]\n",
    "\n",
    "    def before_iter(self):\n",
    "        self.dl.before_iter()\n",
    "\n",
    "    def randomize(self):\n",
    "        self.reset_rng()\n",
    "        self.dl.randomize()\n",
    "\n",
    "    def after_batch(self,b):\n",
    "        b = self.dl.after_batch(b)\n",
    "        # recast tensor subclasses to plain tensors\n",
    "        # undoing work of self.retain()\n",
    "        tb = [recast2tensor(o) for o in b]\n",
    "        b = tuple(tb)\n",
    "        return b\n",
    "\n",
    "    def after_iter(self):\n",
    "        self.dl.after_iter()\n",
    "\n",
    "    def create_batches(self,samps):\n",
    "        return self.dl.create_batches(samps)\n",
    "\n",
    "    def to(self, device):\n",
    "        self.dl.device = device\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    def one_batch(self):\n",
    "        return self.dl.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h2 id=\"TPUDistributedDL\" class=\"doc_header\"><code>class</code> <code>TPUDistributedDL</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n\n> <code>TPUDistributedDL</code>(**`dl`**, **`rank`**, **`world_size`**, **`seed`**=*`42`*) :: `TfmdDL`\n\nA `TfmdDL` which splits a batch into equal size pieces for each TPU core\nIt also recasts the output of a batch from a TensorBase subclass to\na regular tensor since the XLA Parallel loader doesn't seem to be compatible\nto it.\nCode implementation was based on @tmabraham's [`TPUDistributedDL`](/fastai_xla_extensions/multi_core.base.html#TPUDistributedDL) implementation\nhere: https://github.com/tmabraham/fastai_tpu/blob/master/fastai_v2/tpu_distributed_dl.py",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "#colab\n",
    "show_doc(TPUDistributedDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#colab\n",
    "#TODO: add tests for distrib tpu dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "from fastai.torch_core import TensorBase, TensorImage, TensorCategory\n",
    "from fastai.data.core import TfmdDL\n",
    "\n",
    "n_batches = 10\n",
    "bs = 6\n",
    "world_size = 8\n",
    "# setup a dataloader as base dl for tpu \n",
    "items = [(TensorImage(torch.tensor(i).float()), TensorCategory(i)) for i in range(n_batches * bs * world_size)]\n",
    "dl = TfmdDL(items, bs=bs, shuffle=True)\n",
    "assert len(dl) == n_batches * world_size\n",
    "b0 = next(iter(dl))\n",
    "assert isinstance(b0[0], TensorImage)\n",
    "assert isinstance(b0[1],TensorCategory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "tpu_dl = TPUDistributedDL(dl, rank=0, world_size=world_size)\n",
    "# the batches for dl for each rank is divided across all ranks\n",
    "assert len(tpu_dl) == n_batches\n",
    "tpu_b0 = next(iter(tpu_dl))\n",
    "# the types of each batch (x,y) have been reverted to torch tensors\n",
    "# and are no longer Tensor subclasses (e.g. TensorBase)\n",
    "assert isinstance(tpu_b0[0], torch.Tensor)\n",
    "assert isinstance(tpu_b0[1], torch.Tensor)\n",
    "assert not isinstance(tpu_b0[0], TensorBase)\n",
    "assert not isinstance(tpu_b0[1], TensorBase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# add tests to make sure all items are retrieved per epoch\n",
    "# create dl for each rank across all ranks\n",
    "tpu_dls = [TPUDistributedDL(dl, rank=rank, world_size=world_size) for rank in range(world_size)]\n",
    "rank_batches = [list(tpu_dl) for tpu_dl in tpu_dls]\n",
    "# TODO: check that each rank dont contain common items\n",
    "# TODO: check that all items in dl are accounted for in the tpu_dls across all ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def build_distributed_dataloaders(dls, rank, world_size, sync_valid=False):\n",
    "    \"\"\"Wrap dataloaders with distributed TPU aware dataloader \"\"\"\n",
    "    new_loaders = []\n",
    "    for i,dl in enumerate(dls.loaders):\n",
    "        if i == 0 or sync_valid:\n",
    "            use_rank = rank\n",
    "            use_size = world_size\n",
    "        else:\n",
    "            use_rank = 0\n",
    "            use_size = 1\n",
    "        dl = TPUDistributedDL(dl,\n",
    "                            rank=use_rank,\n",
    "                            world_size=use_size)\n",
    "        new_loaders += [dl]\n",
    "    return DataLoaders(*new_loaders, path=dls.path, device=dls.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"build_distributed_dataloaders\" class=\"doc_header\"><code>build_distributed_dataloaders</code><a href=\"__main__.py#L5\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>build_distributed_dataloaders</code>(**`dls`**, **`rank`**, **`world_size`**, **`sync_valid`**=*`False`*)\n\nWrap dataloaders with distributed TPU aware dataloader ",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "show_doc(build_distributed_dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.meta import delegates\n",
    "from fastai.data.block import DataBlock\n",
    "\n",
    "@delegates(DataBlock.dataloaders,but='datablock,rank,world_size,sync_valid,device')\n",
    "def make_fastai_dataloaders(datablock, source, rank, world_size, device=None, path='.', sync_valid=False, verbose=False,**kwargs):\n",
    "    \"create fastai-based dataloaders from a datablock and wrap a tpu distributed dataloader around them\"\n",
    "    dls = datablock.dataloaders(source=source, path=path, device=device, **kwargs)\n",
    "    distrib_dls = build_distributed_dataloaders(dls, rank, world_size, sync_valid=sync_valid)\n",
    "    return distrib_dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"make_fastai_dataloaders\" class=\"doc_header\"><code>make_fastai_dataloaders</code><a href=\"__main__.py#L5\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>make_fastai_dataloaders</code>(**`datablock`**, **`source`**, **`rank`**, **`world_size`**, **`device`**=*`None`*, **`path`**=*`'.'`*, **`sync_valid`**=*`False`*, **`verbose`**=*`False`*)\n\ncreate fastai-based dataloaders from a datablock and wrap a tpu distributed dataloader around them",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "show_doc(make_fastai_dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def wrap_parallel_loader(loader, device):\n",
    "    \"wraps a tpu distributed loader or a torch dataloader (with distributed sampler) with xla parallel loader\"\n",
    "    para_loader = pl.ParallelLoader(loader, [device])\n",
    "    loop_loader = para_loader.per_device_loader(device)\n",
    "    return loop_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"wrap_parallel_loader\" class=\"doc_header\"><code>wrap_parallel_loader</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>wrap_parallel_loader</code>(**`loader`**, **`device`**)\n\nwraps a tpu distributed loader or a torch dataloader (with distributed sampler) with xla parallel loader",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "show_doc(wrap_parallel_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "from fastai.learner import Recorder\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.learner import CancelValidException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class XLATrainingCallback(Callback):\n",
    "    \"A callback for training as a spawned process on multi-core TPUs\"\n",
    "    run_before = Recorder\n",
    "    run_valid = False\n",
    "    order = -5 # after TrainEvalCallback\n",
    "    def __init__(self, device, rank=0, sync_valid=False):\n",
    "        self.pdevice = device\n",
    "        self.rank = rank\n",
    "        self.sync_valid = sync_valid\n",
    "\n",
    "    def before_fit(self):\n",
    "       xm.master_print('start fit')\n",
    "\n",
    "    def before_epoch(self):\n",
    "        # set the epoch on train only to make sure shuffle produces same seq\n",
    "        # across all ranks\n",
    "        if hasattr(self.learn.dls.train,'sampler'):\n",
    "            if hasattr(self.learn.dls.train.sampler,'set_epoch'):\n",
    "                self.learn.dls.train.sampler.set_epoch(self.learn.epoch)\n",
    "        elif hasattr(self.learn.dls.train,'set_epoch'):\n",
    "            self.learn.dls.train.set_epoch(self.learn.epoch)\n",
    "\n",
    "        if self.sync_valid: # update epoch on valid if sync_valid\n",
    "            if hasattr(self.learn.dls.valid,'sampler'):\n",
    "                if hasattr(self.learn.dls.valid.sampler,'set_epoch'):\n",
    "                    self.learn.dls.valid.sampler.set_epoch(self.learn.epoch)\n",
    "            elif hasattr(self.learn.dls.valid,'set_epoch'):\n",
    "                self.learn.dls.valid.set_epoch(self.learn.epoch)\n",
    "\n",
    "    def before_train(self):\n",
    "        self.learn.dl = wrap_parallel_loader(self.dls.train, self.pdevice)\n",
    "\n",
    "    def before_validate(self):\n",
    "        \"Set the model in validation mode\"\n",
    "        if self.rank != 0 and not self.sync_valid:\n",
    "        # no need to compute valid loss/ metric if not master if not sync valid\n",
    "            raise CancelValidException()\n",
    "        self.learn.dl = wrap_parallel_loader(self.dls.valid, self.pdevice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `XLATrainingCallback` is responsible for the following functions:\n",
    "   * sets the `epoch` on either the torch dataloader sampler or the TPU distributed DL before each epoch. This ensures that for each epoch, samples in each batch are the same across all ranks, but each rank will pick the subset of batches for each rank.\n",
    "\n",
    "   The `TPUDistributedDL` (and the torch distributed sampler) ensures that all the samples (with some duplication if the samples are not exactly divisible by the number of ranks) are seen by one of the dataloaders across the ranks least once per epoch.\n",
    "   * wraps the dataloader (either training or validation) with the XLA Parallel Loader (`torch_xla.distributed.parallel_loader.ParallelLoader`) before each training or validation run.\n",
    "   * sidesteps the call to `opt.step` and instead calls `xm.optimizer_step(opt)` to sync the model gradients across all the ranks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "import copy\n",
    "from fastai.learner import _maybe_item\n",
    "from fastprogress.fastprogress import format_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pack_metric(metrics):\n",
    "    \"extract counts and totals from avg metrics and avg losses into a list\"\n",
    "    counts = metrics.attrgot('count',0)\n",
    "    totals = metrics.attrgot('total',0)\n",
    "    metrics_list = counts + totals\n",
    "    return metrics_list\n",
    "\n",
    "def make_tensor(o, device):\n",
    "    \"convert a scalar or tensor into a float tensor and move them to `device`\"\n",
    "    if not isinstance(o, torch.Tensor):\n",
    "        o = torch.tensor(o)\n",
    "    return o.float().to(device)\n",
    "\n",
    "def pack_metrics(all_metrics, device):\n",
    "    \"pack train and valid metrics into a list of float tensors and move them to `device`\"\n",
    "    metrics_list = pack_metric(all_metrics['train_mets']) + pack_metric(all_metrics['valid_mets'])\n",
    "    return [make_tensor(item,device) for item in metrics_list ]\n",
    "\n",
    "def restore_metrics(reduced_metrics, all_metrics):\n",
    "    \"restore list of float tensors (count and values) back into train and valid metrics\"\n",
    "    n_train = len(all_metrics['train_mets'])\n",
    "    n_valid = len(all_metrics['valid_mets'])\n",
    "    train_counts = reduced_metrics[:n_train]\n",
    "    train_totals = reduced_metrics[n_train: n_train*2]\n",
    "    valid_counts = reduced_metrics[n_train*2: n_train*2 + n_valid]\n",
    "    valid_totals = reduced_metrics[n_train*2 + n_valid:]\n",
    "    for i,metric in enumerate(all_metrics['train_mets']):\n",
    "        if hasattr(metric,'count'):\n",
    "            metric.count = train_counts[i].clone().detach().long()\n",
    "        if hasattr(metric,'total'):\n",
    "            metric.total = train_totals[i].clone().detach()\n",
    "    for i,metric in enumerate(all_metrics['valid_mets']):\n",
    "        if hasattr(metric,'count'):\n",
    "            metric.count = valid_counts[i].clone().detach().long()\n",
    "        if hasattr(metric,'total'):\n",
    "            metric.total = valid_totals[i].clone().detach()\n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SyncRecorderCallback(Callback):\n",
    "    \"\"\"A `Callback` to sync the metrics from each rank and update statistics\n",
    "       accordingly so it will display correctly in the progress callback\n",
    "    \"\"\"\n",
    "    order  = 55 # after Recorder, before ProgressCallback\n",
    "\n",
    "    def before_fit(self):\n",
    "        if not xm.is_master_ordinal():\n",
    "            return\n",
    "        if 'progress' in self.learn.cbs.attrgot('name',None):\n",
    "            self._sync_stats_log = self.progress._write_stats\n",
    "        else:\n",
    "            self._sync_stats_log = self.learn.logger\n",
    "\n",
    "    def before_epoch(self):\n",
    "        self.sync_log = copy.copy(self.recorder.log)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        if 'recorder' not in self.learn.cbs.attrgot('name'):\n",
    "            all_metrics = {\n",
    "                'train_mets': L([]),\n",
    "                'valid_mets': L([]),\n",
    "            }\n",
    "        else:\n",
    "            all_metrics = {\n",
    "                'train_mets': self.recorder._train_mets,\n",
    "                'valid_mets': self.recorder._valid_mets,\n",
    "            }\n",
    "        # send metrics data to sync ranks across spawned processes\n",
    "        device = self.learn.xla_training.pdevice\n",
    "        packed_metrics = pack_metrics(all_metrics, device) # convert metrics to tensor list on TPU\n",
    "        reduced_metrics = xm.all_reduce(xm.REDUCE_SUM, packed_metrics)\n",
    "        xm.mark_step()\n",
    "        if xm.is_master_ordinal():\n",
    "            all_metrics = restore_metrics(reduced_metrics, all_metrics) # convert list to metric objects\n",
    "            for m in self.recorder._train_mets:\n",
    "                self.sync_log += _maybe_item(m)\n",
    "\n",
    "            for m in self.recorder._valid_mets:\n",
    "                self.sync_log += _maybe_item(m)\n",
    "\n",
    "            self.learn.final_record = self.sync_log[:1].copy()\n",
    "            del self.recorder.values[-1] # remove last entry added by recorder\n",
    "            self.recorder.values.append(self.learn.final_record) # add updated metrics\n",
    "            if self.recorder.add_time:\n",
    "                updated_time = (time.time() - self.recorder.start_epoch)\n",
    "                self.sync_log.append(format_time(updated_time))\n",
    "            self.recorder.log = self.sync_log\n",
    "            self._sync_stats_log(self.sync_log) # write_stats to output\n",
    "            self.learn.logger = self.orig_logger # restore orig logger after skipping recorder.logger(log)\n",
    "\n",
    "    def after_validate(self):\n",
    "        if xm.is_master_ordinal():\n",
    "            self.orig_logger = self.learn.logger\n",
    "            self.learn.logger = noop # write to logger disabled so calling recorder.logger(log) wont print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.imports import noop\n",
    "#from fastcore.basics import patch\n",
    "from fastai.learner import Learner\n",
    "from fastai.callback.progress import ProgressCallback\n",
    "from fastcore.xtras import join_path_file\n",
    "#from fastai.torch_core import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@patch\n",
    "@delegates(Learner.save)\n",
    "def save(self:Learner, file, **kwargs):\n",
    "    file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n",
    "    with_opt = self.opt is not None\n",
    "    state = self.model.state_dict()\n",
    "    if with_opt:\n",
    "        # add opt state to state to be saved\n",
    "        opt_state = self.opt.state_dict()\n",
    "        state = {'model': state, 'opt':opt_state}\n",
    "    xm.save(state, file) # use xm.save instead of torch.save\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"Learner.save\" class=\"doc_header\"><code>Learner.save</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Learner.save</code>(**`file`**, **`with_opt`**=*`True`*, **`pickle_protocol`**=*`2`*)\n\n",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "#colab\n",
    "show_doc(Learner.save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Learner.save` has been patched to use the torch xla method `xm.save` which will save the model weights for the model on the TPU device. Moreover, `xm.save` only saves the weights on the master ordinal rank process by default, ensuring that only one copy of the model is written to a file. _Which is fine, since the `xm.optimizer_step` done on each training batch synchronizes the weights across all ranks anyway._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def to_multi_xla(self:Learner,device, rank, sync_valid=False):\n",
    "    \"Sets up the learner on the spawned process for multi core TPU training\"\n",
    "    if 'xla_training' not in self.cbs.attrgot('name'):\n",
    "        self.dls.device = None\n",
    "        self.add_cbs([XLATrainingCallback(device, rank, sync_valid=sync_valid),\n",
    "                      XLAOptCallback()])\n",
    "        self.opt = None # clear opt to ensure\n",
    "\n",
    "    else:\n",
    "        self.xla_training.pdevice = device\n",
    "        self.xla_training.rank = rank\n",
    "        self.xla_training.sync_valid = sync_valid\n",
    "\n",
    "    if sync_valid and 'sync_recorder' not in self.cbs.attrgot('name'):\n",
    "        self.add_cbs(SyncRecorderCallback)\n",
    "    elif not sync_valid:\n",
    "        self.remove_cbs(SyncRecorderCallback)\n",
    "\n",
    "    if rank != 0: # progress bar only for rank 0\n",
    "        self.remove_cbs(ProgressCallback)\n",
    "    self.logger = xm.master_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"Learner.to_multi_xla\" class=\"doc_header\"><code>Learner.to_multi_xla</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Learner.to_multi_xla</code>(**`device`**, **`rank`**, **`sync_valid`**=*`False`*)\n\nSets up the learner on the spawned process for multi core TPU training",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "#colab\n",
    "show_doc(Learner.to_multi_xla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# for testing\n",
    "def do_one_loop(dl, rank, world_size, device, wrap_parallel=True):\n",
    "    n_batches = len(dl)\n",
    "    print(f'xla: {rank} world_size: {world_size} n_batches:{n_batches}')\n",
    "\n",
    "    if wrap_parallel:\n",
    "        print(f'xla: {rank} wrapping ploader')\n",
    "        pdl = wrap_parallel_loader(dl, device=device)\n",
    "    else:\n",
    "        pdl = dl\n",
    "    for i,b in enumerate(pdl):\n",
    "        if i > 1:\n",
    "            break\n",
    "        xb, yb = b\n",
    "        print(f'xla: {rank} iter:{i} xb type {type(xb)} yb type: {type(yb)}')\n",
    "        print(f'xla: {rank} iter:{i} xb.shape {xb.shape} yb.shape: {yb.shape}')\n",
    "        print(f'xla: {rank} iter:{i} xb.device {xb.device} yb.device: {yb.device}')\n",
    "        print(f'xla: {rank} iter:{i} xb.dtype {xb.dtype} yb.device: {yb.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#colab\n",
    "from functools import partial\n",
    "from fastai.metrics import accuracy\n",
    "from fastai.optimizer import SGD, Adam\n",
    "\n",
    "from fastcore.basics import first\n",
    "from fastai.callback.schedule import *\n",
    "from fastai.test_utils import VerboseCallback\n",
    "from my_timesaver_utils.profiling import *\n",
    "from my_timesaver_utils.profiling_callback import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "def run_dataloader_loop(rank):\n",
    "    torch.manual_seed(1)\n",
    "    print(f'xla {rank} start run_dataloader_loop')\n",
    "    xm.rendezvous('start_run_dataloader_loop')\n",
    "    # Scale learning rate to num cores\n",
    "    learning_rate = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
    "    SYNC_VALID = FLAGS['sync_valid']\n",
    "    IS_PROFILING = FLAGS['is_profiling']\n",
    "    # Get loss function, optimizer, and model\n",
    "    device = xm.xla_device()\n",
    "    model = WRAPPED_MODEL.to(device)\n",
    "    bs = FLAGS['batch_size']\n",
    "    world_size = xm.xrt_world_size()\n",
    "    if IS_PROFILING:\n",
    "        rec_name = 'rank' + str(rank) + '_dataloader_build'\n",
    "        print(f'start {rec_name}')\n",
    "        start_record(rec_name)\n",
    "\n",
    "    # dls = make_fastai_dataloaders(\n",
    "    #                         DATA, \n",
    "    #                         PATH, \n",
    "    #                         rank=rank, \n",
    "    #                         world_size=world_size, \n",
    "    #                         sync_valid=SYNC_VALID,\n",
    "    #                         bs=bs,)\n",
    "    dls = DATA.dataloaders(PATH, bs=bs)\n",
    "    # distrib_dls = build_distributed_dataloaders(dls, rank, world_size, \n",
    "    #                                            sync_valid=True)\n",
    "    dl = dls.train\n",
    "    tpu_dl = TPUDistributedDL(dl,rank=rank,world_size=world_size)\n",
    "    print(f'xla: {rank} fake_l.num_workers {tpu_dl.fake_l.num_workers}')\n",
    "    do_one_loop(tpu_dl, rank, world_size, device, wrap_parallel=False)\n",
    "    if IS_PROFILING:\n",
    "        end_record(rec_name)\n",
    "        print_prof_data(rec_name)\n",
    "        print(f'finished {rec_name}')\n",
    "\n",
    "    xm.mark_step()\n",
    "    print(f'xla {rank} completed run_dataloader_loop')\n",
    "    # print_prof_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "def train_model(rank):\n",
    "    torch.manual_seed(1)\n",
    "    xm.rendezvous('start_train_model')\n",
    "    print(f'xla {rank} start train model')\n",
    "\n",
    "    \n",
    "    SYNC_VALID = FLAGS['sync_valid']\n",
    "    IS_PROFILING = FLAGS['is_profiling']\n",
    "    # Get loss function, optimizer, and model\n",
    "    device = xm.xla_device()\n",
    "\n",
    "    bs = FLAGS['batch_size']\n",
    "    world_size = xm.xrt_world_size()\n",
    "    if IS_PROFILING:\n",
    "        rec_name = 'rank' + str(rank) + '_dataloader_build'\n",
    "        print(f'start {rec_name}')\n",
    "        start_record(rec_name)\n",
    "\n",
    "    dls = make_fastai_dataloaders(\n",
    "                            DATA, \n",
    "                            PATH, \n",
    "                            rank=rank, \n",
    "                            world_size=world_size, \n",
    "                            sync_valid=SYNC_VALID,\n",
    "                            bs=bs,)\n",
    "    if IS_PROFILING:\n",
    "        end_record(rec_name)\n",
    "        print_prof_data(rec_name)\n",
    "        print(f'finished {rec_name}')\n",
    "    model = WRAPPED_MODEL.to(device)\n",
    "    moms =(FLAGS['momentum'],FLAGS['momentum'],FLAGS['momentum'])\n",
    "    wd = FLAGS['weight_decay']\n",
    "\n",
    "    xm.master_print('build learner')\n",
    "    learner = Learner(dls, model, \n",
    "                      loss_func=LOSS_FUNC, \n",
    "                      opt_func=OPT_FUNC, \n",
    "                      metrics=accuracy, \n",
    "                      wd=wd,\n",
    "                      moms=moms)\n",
    "                      \n",
    "    learner.to_multi_xla(device, rank=xm.get_ordinal(), sync_valid=SYNC_VALID)\n",
    "    if IS_PROFILING and rank == 0:\n",
    "        learner.to_my_profile()\n",
    "\n",
    "    # Scale learning rate to num cores\n",
    "    learning_rate = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
    "                               \n",
    "    epochs = FLAGS['num_epochs']\n",
    "    xm.master_print('start running fit')\n",
    "    learner.unfreeze()\n",
    "    if IS_PROFILING:\n",
    "        rec_name3 = 'rank' + str(rank) + '_run_fit'\n",
    "        print(f'start {rec_name3}')\n",
    "        start_record(rec_name3)\n",
    "\n",
    "    learner.fit_one_cycle(epochs, lr_max=slice(learning_rate/10))\n",
    "    if IS_PROFILING:\n",
    "        end_record(rec_name3)\n",
    "        print_prof_data(rec_name3)\n",
    "        print(f'finished {rec_name3}')\n",
    "\n",
    "    learner.save('stage-1')\n",
    "    if rank == 0 and IS_PROFILING :\n",
    "        learner.my_profile.print_stats()\n",
    "    xm.mark_step()  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main method that runs the training. \n",
    "\n",
    "It includes some profiling code to measure the building of the `dataloaders` and running of the `fit` methods. \n",
    "\n",
    "At the end of the spawned processes, the master ordinal process saves the model to a temporary file. (see `Learner.save` patch above)\n",
    "\n",
    "The saved model will then be loaded by the main process so that it will now contain the trained weights updated by the spawned training processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# Start training processes\n",
    "def _mp_fn(rank, flags):\n",
    "    global FLAGS\n",
    "    FLAGS = flags\n",
    "    train_model(rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# Start dataloader processes\n",
    "def _mp_fn2(rank, flags):\n",
    "    global FLAGS\n",
    "    FLAGS = flags\n",
    "    run_dataloader_loop(rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fastcore.transform import DisplayedTransform, Transform\n",
    "from fastcore.basics import store_attr\n",
    "from fastai.vision.core import PILImage, PILBase, image2tensor\n",
    "from fastai.data.block import TransformBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.transforms import get_c\n",
    "# from fastai.vision.all import *\n",
    "from fastai.data.block import DataBlock, CategoryBlock\n",
    "from fastai.vision.data import ImageBlock\n",
    "from fastai.data.transforms import get_image_files, parent_label, GrandparentSplitter\n",
    "from fastai.vision.augment import Resize, aug_transforms\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastai.data.transforms import Normalize\n",
    "from fastai.vision.core import imagenet_stats\n",
    "from fastcore.basics import using_attr\n",
    "from fastai.data.transforms import RegexLabeller, CategoryMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "LOSS_FUNC = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.optimizer import Adam\n",
    "OPT_FUNC = Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.transforms import RandomSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.learner import create_cnn_model\n",
    "from fastai.vision.models import resnet34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Define Parameters\n",
    "FLAGS = {}\n",
    "# FLAGS['batch_size'] = 1024\n",
    "FLAGS['sync_valid'] = True\n",
    "FLAGS['is_profiling'] = True\n",
    "FLAGS['batch_size'] = 64\n",
    "FLAGS['num_workers'] = 4\n",
    "FLAGS['learning_rate'] = 1e-3\n",
    "FLAGS['image_size'] = 224\n",
    "FLAGS['momentum'] = 0.85\n",
    "FLAGS['weight_decay'] = 2e-3\n",
    "FLAGS['num_epochs'] = 5\n",
    "FLAGS['num_cores'] = 8 if os.environ.get('TPU_NAME', None) else 1\n",
    "# FLAGS['num_cores'] = 1 \n",
    "ARCH = resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from fastcore.xtras import *\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "PATH = untar_data(URLs.PETS)/'images'\n",
    "# PATH = untar_data(URLs.MNIST)\n",
    "# PATH = untar_data(URLs.MNIST_TINY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "\n",
    "pat = r'(.+)_\\d+.jpg$'\n",
    "fname_labeller = using_attr(RegexLabeller(pat),'name') \n",
    "splitter=RandomSplitter(seed=42)\n",
    "DATA = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),\n",
    "    get_items=get_image_files,\n",
    "    get_y=fname_labeller,\n",
    "    splitter=splitter,\n",
    "    item_tfms=[Resize(FLAGS['image_size']),],\n",
    "    batch_tfms=[]\n",
    ")\n",
    "vocab = CategoryMap(get_image_files(PATH).map(fname_labeller))\n",
    "N_OUT = len(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "assert N_OUT is not None and N_OUT > 0,f'N_OUT {N_OUT} should be > 0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is created by the main process and wrapped by the `xmp.MpModelWrapper`. This is to reduce the memory usage by not having multiple copies of the model in the spawned processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "custom_model = create_cnn_model(ARCH, N_OUT, \n",
    "                                pretrained=True,\n",
    "                                concat_pool=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# Only instantiate model weights once in memory.\n",
    "WRAPPED_MODEL = xmp.MpModelWrapper(custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xla 0 start run_dataloader_loop\n",
      "xla 6 start run_dataloader_loop\n",
      "xla 4 start run_dataloader_loop\n",
      "xla 5 start run_dataloader_loop\n",
      "xla 7 start run_dataloader_loop\n",
      "xla 3 start run_dataloader_loop\n",
      "xla 1 start run_dataloader_loop\n",
      "xla 2 start run_dataloader_loop\n",
      "start rank7_dataloader_build\n",
      "xla: 7 fake_l.num_workers 2\n",
      "xla: 7 world_size: 8 n_batches:12\n",
      "start rank1_dataloader_build\n",
      "xla: 7 iter:0 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 7 iter:0 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 7 iter:0 xb.device cpu yb.device: cpu\n",
      "xla: 7 iter:0 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 7 iter:1 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 7 iter:1 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 7 iter:1 xb.device cpu yb.device: cpu\n",
      "xla: 7 iter:1 xb.dtype torch.float32 yb.device: torch.int64\n",
      "start rank0_dataloader_build\n",
      "xla: 1 fake_l.num_workers 2\n",
      "xla: 1 world_size: 8 n_batches:12\n",
      "start rank5_dataloader_build\n",
      "xla: 0 fake_l.num_workers 2\n",
      "xla: 0 world_size: 8 n_batches:12\n",
      "start rank4_dataloader_build\n",
      "xla: 1 iter:0 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 1 iter:0 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 1 iter:0 xb.device cpu yb.device: cpu\n",
      "xla: 1 iter:0 xb.dtype torch.float32 yb.device: torch.int64\n",
      "Function rank7_dataloader_build called 1 times.\n",
      "Execution time max: 8.000, average: 8.000\n",
      "finished rank7_dataloader_build\n",
      "xla: 1 iter:1 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 1 iter:1 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 1 iter:1 xb.device cpu yb.device: cpu\n",
      "xla: 1 iter:1 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla 7 completed run_dataloader_loop\n",
      "start rank3_dataloader_build\n",
      "xla: 5 fake_l.num_workers 2\n",
      "xla: 5 world_size: 8 n_batches:12\n",
      "start rank6_dataloader_build\n",
      "xla: 0 iter:0 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 0 iter:0 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 0 iter:0 xb.device cpu yb.device: cpu\n",
      "xla: 0 iter:0 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 0 iter:1 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 0 iter:1 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 0 iter:1 xb.device cpu yb.device: cpu\n",
      "xla: 0 iter:1 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 4 fake_l.num_workers 2\n",
      "xla: 4 world_size: 8 n_batches:12\n",
      "start rank2_dataloader_build\n",
      "xla: 3 fake_l.num_workers 2\n",
      "xla: 3 world_size: 8 n_batches:12\n",
      "xla: 5 iter:0 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 5 iter:0 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 5 iter:0 xb.device cpu yb.device: cpu\n",
      "xla: 5 iter:0 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 5 iter:1 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 5 iter:1 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 5 iter:1 xb.device cpu yb.device: cpu\n",
      "xla: 5 iter:1 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 6 fake_l.num_workers 2\n",
      "xla: 6 world_size: 8 n_batches:12\n",
      "Function rank1_dataloader_build called 1 times.\n",
      "Execution time max: 15.677, average: 15.677\n",
      "finished rank1_dataloader_build\n",
      "xla 1 completed run_dataloader_loop\n",
      "xla: 4 iter:0 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 4 iter:0 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 4 iter:0 xb.device cpu yb.device: cpu\n",
      "xla: 4 iter:0 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 2 fake_l.num_workers 2\n",
      "xla: 2 world_size: 8 n_batches:12\n",
      "xla: 4 iter:1 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 4 iter:1 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 4 iter:1 xb.device cpu yb.device: cpu\n",
      "xla: 4 iter:1 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 3 iter:0 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 3 iter:0 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 3 iter:0 xb.device cpu yb.device: cpu\n",
      "xla: 3 iter:0 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 3 iter:1 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 3 iter:1 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 3 iter:1 xb.device cpu yb.device: cpu\n",
      "xla: 3 iter:1 xb.dtype torch.float32 yb.device: torch.int64\n",
      "Function rank0_dataloader_build called 1 times.\n",
      "Execution time max: 18.021, average: 18.021\n",
      "finished rank0_dataloader_build\n",
      "xla 0 completed run_dataloader_loop\n",
      "xla: 6 iter:0 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 6 iter:0 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 6 iter:0 xb.device cpu yb.device: cpu\n",
      "xla: 6 iter:0 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 6 iter:1 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 6 iter:1 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 6 iter:1 xb.device cpu yb.device: cpu\n",
      "xla: 6 iter:1 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 2 iter:0 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 2 iter:0 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 2 iter:0 xb.device cpu yb.device: cpu\n",
      "xla: 2 iter:0 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 2 iter:1 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 2 iter:1 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 2 iter:1 xb.device cpu yb.device: cpu\n",
      "xla: 2 iter:1 xb.dtype torch.float32 yb.device: torch.int64\n",
      "Function rank5_dataloader_build called 1 times.\n",
      "Execution time max: 19.994, average: 19.994\n",
      "finished rank5_dataloader_build\n",
      "xla 5 completed run_dataloader_loop\n",
      "Function rank4_dataloader_build called 1 times.\n",
      "Execution time max: 19.969, average: 19.969\n",
      "finished rank4_dataloader_build\n",
      "xla 4 completed run_dataloader_loop\n",
      "Function rank3_dataloader_build called 1 times.\n",
      "Execution time max: 19.217, average: 19.217\n",
      "finished rank3_dataloader_build\n",
      "xla 3 completed run_dataloader_loop\n",
      "Function rank6_dataloader_build called 1 times.\n",
      "Execution time max: 17.590, average: 17.590\n",
      "finished rank6_dataloader_build\n",
      "xla 6 completed run_dataloader_loop\n",
      "Function rank2_dataloader_build called 1 times.\n",
      "Execution time max: 15.984, average: 15.984\n",
      "finished rank2_dataloader_build\n",
      "xla 2 completed run_dataloader_loop\n",
      "CPU times: user 133 ms, sys: 134 ms, total: 267 ms\n",
      "Wall time: 46.9 s\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "#colab\n",
    "%%time\n",
    "xmp.spawn(_mp_fn2, args=(FLAGS,), nprocs=FLAGS['num_cores'],\n",
    "        start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xla 0 start train model\n",
      "xla 5 start train model\n",
      "xla 3 start train model\n",
      "xla 2 start train model\n",
      "xla 1 start train model\n",
      "xla 7 start train model\n",
      "xla 6 start train model\n",
      "xla 4 start train model\n",
      "build learner\n",
      "start running fit\n",
      "start fit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.759310</td>\n",
       "      <td>1.098083</td>\n",
       "      <td>0.662162</td>\n",
       "      <td>01:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.637405</td>\n",
       "      <td>1.350417</td>\n",
       "      <td>0.690540</td>\n",
       "      <td>01:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.578879</td>\n",
       "      <td>0.578579</td>\n",
       "      <td>0.842568</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.505099</td>\n",
       "      <td>0.347587</td>\n",
       "      <td>0.893243</td>\n",
       "      <td>01:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.429199</td>\n",
       "      <td>0.248740</td>\n",
       "      <td>0.926351</td>\n",
       "      <td>01:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 122 ms, sys: 122 ms, total: 244 ms\n",
      "Wall time: 7min 51s\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "%%time\n",
    "FLAGS['is_profiling'] = False\n",
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'],\n",
    "        start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting-up type transforms pipelines\n",
      "Collecting items from /root/.fastai/data/oxford-iiit-pet/images\n",
      "Found 7390 items\n",
      "2 datasets of sizes 5912,1478\n",
      "Setting up Pipeline: PILBase.create\n",
      "Setting up Pipeline: partial -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n",
      "\n",
      "Building one sample\n",
      "  Pipeline: PILBase.create\n",
      "    starting from\n",
      "      /root/.fastai/data/oxford-iiit-pet/images/newfoundland_143.jpg\n",
      "    applying PILBase.create gives\n",
      "      PILImage mode=RGB size=500x375\n",
      "  Pipeline: partial -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n",
      "    starting from\n",
      "      /root/.fastai/data/oxford-iiit-pet/images/newfoundland_143.jpg\n",
      "    applying partial gives\n",
      "      newfoundland\n",
      "    applying Categorize -- {'vocab': None, 'sort': True, 'add_na': False} gives\n",
      "      TensorCategory(27)\n",
      "\n",
      "Final sample: (PILImage mode=RGB size=500x375, TensorCategory(27))\n",
      "\n",
      "\n",
      "Collecting items from /root/.fastai/data/oxford-iiit-pet/images\n",
      "Found 7390 items\n",
      "2 datasets of sizes 5912,1478\n",
      "Setting up Pipeline: PILBase.create\n",
      "Setting up Pipeline: partial -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n",
      "Setting up after_item: Pipeline: Resize -- {'size': (224, 224), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (2, 0), 'p': 1.0} -> ToTensor\n",
      "Setting up before_batch: Pipeline: \n",
      "Setting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n",
      "\n",
      "Building one batch\n",
      "Applying item_tfms to the first sample:\n",
      "  Pipeline: Resize -- {'size': (224, 224), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (2, 0), 'p': 1.0} -> ToTensor\n",
      "    starting from\n",
      "      (PILImage mode=RGB size=500x375, TensorCategory(27))\n",
      "    applying Resize -- {'size': (224, 224), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (2, 0), 'p': 1.0} gives\n",
      "      (PILImage mode=RGB size=224x224, TensorCategory(27))\n",
      "    applying ToTensor gives\n",
      "      (TensorImage of size 3x224x224, TensorCategory(27))\n",
      "\n",
      "Adding the next 3 samples\n",
      "\n",
      "No before_batch transform to apply\n",
      "\n",
      "Collating items in a batch\n",
      "\n",
      "Applying batch_tfms to the batch built\n",
      "  Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n",
      "    starting from\n",
      "      (TensorImage of size 4x3x224x224, TensorCategory([27, 10,  2,  1]))\n",
      "    applying IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} gives\n",
      "      (TensorImage of size 4x3x224x224, TensorCategory([27, 10,  2,  1]))\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "DATA.summary(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "mdls = DATA.dataloaders(PATH, bs=FLAGS['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.learner.Learner at 0x7ff01f35aba8>"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#colab\n",
    "mlearner = Learner(mdls, custom_model, \n",
    "                    loss_func=LOSS_FUNC, \n",
    "                    opt_func=OPT_FUNC, \n",
    "                    metrics=accuracy, \n",
    "                    wd=FLAGS['weight_decay'],\n",
    "                    moms=(FLAGS['momentum'],FLAGS['momentum'],FLAGS['momentum']))\n",
    "# load trained weights from multi core tpu training\n",
    "mlearner.load('stage-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#colab\n",
    "mlearner.dls.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.torch_core import one_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#colab\n",
    "one_param(mlearner.model).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24909608066082, 0.9282814860343933]\n",
      "CPU times: user 3min 30s, sys: 3.24 s, total: 3min 33s\n",
      "Wall time: 3min 35s\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "%%time\n",
    "valid_metrics = mlearner.validate();print(valid_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
