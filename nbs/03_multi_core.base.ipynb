{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "# attach gdrive holding repo\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp multi_core.base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Core XLA Base \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"https://colab.research.google.com/github/butchland/fastai_xla_extensions/blob/master/nbs/03_multi_core.base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Base module for Multi TPU Core implementation\n",
    "\n",
    "Multi-core TPU implementation is enabled by importing this module.\n",
    "```\n",
    "from fastai_xla_extensions.multi_core.base import *\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 133.6MB 73kB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 2.7MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -Uqq cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp37-cp37m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 194kB 1.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 1.4MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "# !pip install -Uqq git+https://github.com/fastai/fastai.git \n",
    "!pip install -Uqq fastai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for my-timesaver-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -Uqq git+https://github.com/butchland/my_timesaver_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 51kB 444kB/s \n",
      "\u001b[K     |████████████████████████████████| 51kB 1.3MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -qqq nbdev --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating fastai...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!curl -s https://course19.fast.ai/setup/colab | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==1.7.1+cu101\n",
      "torch-xla==1.7\n",
      "torchsummary==1.5.1\n",
      "torchtext==0.3.1\n",
      "torchvision==0.8.2+cu101\n",
      "fastai==2.2.7\n",
      "fastcore==1.3.19\n",
      "fastdtw==0.3.4\n",
      "fastprogress==1.0.0\n",
      "fastrelease==0.1.11\n",
      "fastrlock==0.5\n",
      "my-timesaver-utils==0.0.2\n",
      "nbdev==1.1.13\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!pip freeze | grep torch\n",
    "!pip freeze | grep fast\n",
    "!pip freeze | grep timesaver\n",
    "!pip freeze | grep nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "# link repo to work dir\n",
    "%cd /content\n",
    "!ln -s /content/drive/MyDrive/fastai_xla_extensions fastai_xla_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# <!-- Start of kernel -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/fastai_xla_extensions\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "%cd /content/fastai_xla_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fastai_xla_extensions.utils import xla_imported\n",
    "from fastai_xla_extensions.misc_utils import *\n",
    "from fastai_xla_extensions.core import XLAOptCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "try:\n",
    "    import torch_xla\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#local\n",
    "\n",
    "# fake out torch_xla modules if not running on xla supported envs\n",
    "if not xla_imported():\n",
    "    # replace torch xla modules with fake equivalents\n",
    "    from types import SimpleNamespace\n",
    "    torch_xla = SimpleNamespace (\n",
    "    )\n",
    "    from typing import Union,BinaryIO\n",
    "    import os\n",
    "    import pickle\n",
    "    import torch.cuda\n",
    "\n",
    "    def fake_opt_step(opt,barrier=False):\n",
    "        opt.step()\n",
    "        \n",
    "    def fake_device(n=None, devkind=None):\n",
    "        gpu_available = torch.cuda.is_available()\n",
    "        if gpu_available:\n",
    "            return torch.device(torch.cuda.current_device()) \n",
    "        return torch.device('cpu')\n",
    "\n",
    "    def fake_save(obj, f: Union[str, os.PathLike, BinaryIO], \n",
    "                master_only=True, global_master=False): \n",
    "        return torch.save(obj,f,pickle_module=pickle, \n",
    "                        pickle_protocol=2, \n",
    "                        _use_new_zipfile_serialization=True)\n",
    "    def fake_rate():\n",
    "        return 230.20\n",
    "\n",
    "    def fake_global_rate():\n",
    "        return 830.10\n",
    "\n",
    "    def fake_add(*args,**kwargs):\n",
    "        pass\n",
    "\n",
    "    def fake_RateTracker():\n",
    "        return SimpleNamespace(\n",
    "            rate = fake_rate,\n",
    "            global_rate = fake_global_rate,\n",
    "            add = fake_add\n",
    "        )\n",
    "    def fake_xrt_world_size():\n",
    "        return 1\n",
    "    def fake_get_ordinal():\n",
    "        return 0\n",
    "    def fake_is_master_ordinal(*args,**kwargs): \n",
    "        return True\n",
    "    def fake_maybe_convert_to_cpu(data,*args,**kwargs):\n",
    "        return data\n",
    "\n",
    "    xm = SimpleNamespace(\n",
    "        optimizer_step = fake_opt_step,\n",
    "        xla_device = fake_device,\n",
    "        save = fake_save,\n",
    "        RateTracker = fake_RateTracker,\n",
    "        master_print = print,\n",
    "        xrt_world_size = fake_xrt_world_size,\n",
    "        get_ordinal = fake_get_ordinal,\n",
    "        is_master_ordinal = fake_is_master_ordinal,\n",
    "        _maybe_convert_to_cpu = fake_maybe_convert_to_cpu\n",
    "    )\n",
    "\n",
    "    def fake_metrics_report():\n",
    "        return \"Fake Metrics Report \\n\\n\\n\\n\"\n",
    "    met = SimpleNamespace (\n",
    "        metrics_report = fake_metrics_report\n",
    "    )\n",
    "\n",
    "    class FakePerDeviceLoader:\n",
    "        def __init__(self, *args):\n",
    "            pass\n",
    "        def close(self):\n",
    "            pass\n",
    "            \n",
    "    class FakeParallelLoader:\n",
    "        def __init__(self, loader, *args):\n",
    "            self.loader = loader\n",
    "        def per_device_loader(self,device):\n",
    "            return self.loader\n",
    "        \n",
    "    pl = SimpleNamespace(\n",
    "        ParallelLoader = FakeParallelLoader,\n",
    "        PerDeviceLoader = FakePerDeviceLoader\n",
    "\n",
    "    )\n",
    "\n",
    "    def fake_MpModelWrapper(o):\n",
    "        return o\n",
    "\n",
    "    def fake_run(f,*args, **kwargs):\n",
    "            return f(*args,**kwargs)\n",
    "        \n",
    "    def fake_MpSerialExecutor():\n",
    "        return SimpleNamespace(\n",
    "            run = fake_run\n",
    "        )\n",
    "    def fake_spawn(f, args=None, nprocs=0, start_method=None):\n",
    "        return f(0,*args)\n",
    "\n",
    "    xmp = SimpleNamespace (\n",
    "        MpModelWrapper = fake_MpModelWrapper,\n",
    "        MpSerialExecutor = fake_MpSerialExecutor,\n",
    "        spawn = fake_spawn\n",
    "    )\n",
    "\n",
    "    xu = SimpleNamespace (\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "if xla_imported():\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.parallel_loader as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from fastcore.foundation import L\n",
    "from fastai.data.core import DataLoaders\n",
    "import math\n",
    "from fastcore.basics import store_attr\n",
    "from operator import attrgetter\n",
    "from fastai.data.load import _FakeLoader\n",
    "\n",
    "from fastai.torch_core import TensorBase\n",
    "import random\n",
    "from fastcore.basics import patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Conversion Functions\n",
    "\n",
    "Functions for converting tensors and computing batches across ranks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def revert_tensor(o):\n",
    "    \"Remove tensor subclass and revert to `torch.Tensor`\"\n",
    "    try:\n",
    "        o.__class__ = torch.Tensor\n",
    "    except:\n",
    "        raise RuntimeError(f'could not convert {o} to torch.Tensor')\n",
    "    return o\n",
    "\n",
    "def recast2tensor(o):\n",
    "    \"Recast `fastai.torch_core.TensorBase` subclassed tensors to torch.Tensors\"\n",
    "    if isinstance(o,TensorBase):\n",
    "        # return plain tensor since pl.parallelloader doesn't\n",
    "        # seem to work with tensor subclasses\n",
    "        # return torch.as_tensor(o.numpy())\n",
    "        # TODO: recreate bug in notebook gist to file bug to torch_xla team\n",
    "        return revert_tensor(o)\n",
    "    return o\n",
    "\n",
    "def round_to_multiple(number,multiple):\n",
    "    \"round up batch samples to fill number of cores\"\n",
    "    return int(math.ceil(number/multiple)*multiple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from fastai.data.core import TfmdDL\n",
    "\n",
    "class TPUDistributedDL(TfmdDL):\n",
    "    \"\"\"A `TfmdDL` which splits a batch into equal size pieces for each TPU core\n",
    "       It also recasts the output of a batch from a TensorBase subclass to\n",
    "       a regular tensor since the XLA Parallel loader doesn't seem to be compatible\n",
    "       to it.\n",
    "       Code implementation was based on @tmabraham's `TPUDistributedDL` implementation\n",
    "       here: https://github.com/tmabraham/fastai_tpu/blob/master/fastai_v2/tpu_distributed_dl.py\n",
    "    \"\"\"\n",
    "    _default = 'dl'\n",
    "    def __init__(self,dl,rank,world_size, seed=42):\n",
    "        store_attr()\n",
    "        self.bs,self.device,self.num_workers, \\\n",
    "        self.drop_last,self.dataset,self.offs,fake, self.shuffle = \\\n",
    "            attrgetter('bs','device','num_workers',\n",
    "                       'drop_last','dataset','offs','fake_l', 'shuffle')(dl)\n",
    "        self.fake_l = _FakeLoader(self, fake.pin_memory, fake.num_workers, fake.timeout,\n",
    "                                  persistent_workers=fake.persistent_workers)\n",
    "        self.epoch = 0\n",
    "        random.seed(self.seed)\n",
    "        # setting inner dl rng\n",
    "        self.dl.rng = random.Random(random.randint(0,2**32-1))\n",
    "        self.reset_rng()\n",
    "\n",
    "    def reset_rng(self):\n",
    "        random.seed(self.seed + self.epoch)\n",
    "        # setting outer dl rng\n",
    "        self.rng = random.Random(random.randint(0,2**32-1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return round_to_multiple(len(self.dl),self.world_size)//self.world_size\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def get_idxs(self):\n",
    "        idxs = self.dl.get_idxs()\n",
    "        # do your own shuffling which factors in self.epoch + self.seed in\n",
    "        # generating a random sequence (underlying self.dl does not)\n",
    "        if self.shuffle:\n",
    "            idxs = self.shuffle_fn(idxs)\n",
    "        self.n = len(idxs)\n",
    "        # we assumed n was dl.n but we really care about number of idxs\n",
    "        # add extra samples to make it evenly divisible\n",
    "        self.n_padded = round_to_multiple(self.n,self.world_size)\n",
    "        idxs += (idxs * (self.n_padded//self.n))[:self.n_padded-self.n]\n",
    "        # idx needs to be repeated when n_padded>>n\n",
    "        # slice padded idxs so that each rank gets self.n_padded//self.world_size tensors\n",
    "        start_pos = self.rank*self.n_padded//self.world_size\n",
    "        end_pos = (self.rank+1)*self.n_padded//self.world_size\n",
    "        return idxs[start_pos:end_pos]\n",
    "\n",
    "    def before_iter(self):\n",
    "        self.dl.before_iter()\n",
    "\n",
    "    def randomize(self):\n",
    "        self.reset_rng()\n",
    "        self.dl.randomize()\n",
    "\n",
    "    def after_batch(self,b):\n",
    "        b = self.dl.after_batch(b)\n",
    "        # recast tensor subclasses to plain tensors\n",
    "        # undoing work of self.retain()\n",
    "        tb = [recast2tensor(o) for o in b]\n",
    "        b = tuple(tb)\n",
    "        return b\n",
    "\n",
    "    def after_iter(self):\n",
    "        self.dl.after_iter()\n",
    "\n",
    "    def create_batches(self,samps):\n",
    "        return self.dl.create_batches(samps)\n",
    "\n",
    "    def to(self, device):\n",
    "        self.dl.device = device\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    def one_batch(self):\n",
    "        return self.dl.one_batch()\n",
    "\n",
    "    def new(self, dataset=None, cls=None, **kwargs):\n",
    "        new_dl = self.dl.new(dataset=dataset, cls=cls, **kwargs)\n",
    "        use_rank = self.rank\n",
    "        use_size = self.world_size\n",
    "        seed = self.seed\n",
    "\n",
    "        new_dl = TPUDistributedDL(new_dl,\n",
    "                            rank=use_rank,\n",
    "                            world_size=use_size, \n",
    "                            seed=seed)\n",
    "    \n",
    "        return new_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h2 id=\"TPUDistributedDL\" class=\"doc_header\"><code>class</code> <code>TPUDistributedDL</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n\n> <code>TPUDistributedDL</code>(**`dl`**, **`rank`**, **`world_size`**, **`seed`**=*`42`*) :: `TfmdDL`\n\nA `TfmdDL` which splits a batch into equal size pieces for each TPU core\nIt also recasts the output of a batch from a TensorBase subclass to\na regular tensor since the XLA Parallel loader doesn't seem to be compatible\nto it.\nCode implementation was based on @tmabraham's [`TPUDistributedDL`](/fastai_xla_extensions/multi_core.base.html#TPUDistributedDL) implementation\nhere: https://github.com/tmabraham/fastai_tpu/blob/master/fastai_v2/tpu_distributed_dl.py",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "\n",
    "show_doc(TPUDistributedDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#colab\n",
    "#TODO: add tests for distrib tpu dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "from fastai.torch_core import TensorBase, TensorImage, TensorCategory\n",
    "from fastai.data.core import TfmdDL\n",
    "\n",
    "n_batches = 10\n",
    "bs = 6\n",
    "world_size = 8\n",
    "# setup a dataloader as base dl for tpu \n",
    "items = [(TensorImage(torch.tensor(i).float()), TensorCategory(i)) for i in range(n_batches * bs * world_size)]\n",
    "dl = TfmdDL(items, bs=bs, shuffle=True)\n",
    "assert len(dl) == n_batches * world_size\n",
    "b0 = next(iter(dl))\n",
    "assert isinstance(b0[0], TensorImage)\n",
    "assert isinstance(b0[1],TensorCategory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "tpu_dl = TPUDistributedDL(dl, rank=0, world_size=world_size)\n",
    "# the batches for dl for each rank is divided across all ranks\n",
    "assert len(tpu_dl) == n_batches\n",
    "tpu_b0 = next(iter(tpu_dl))\n",
    "# the types of each batch (x,y) have been reverted to torch tensors\n",
    "# and are no longer Tensor subclasses (e.g. TensorBase)\n",
    "assert isinstance(tpu_b0[0], torch.Tensor)\n",
    "assert isinstance(tpu_b0[1], torch.Tensor)\n",
    "assert not isinstance(tpu_b0[0], TensorBase)\n",
    "assert not isinstance(tpu_b0[1], TensorBase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# add tests to make sure all items are retrieved per epoch\n",
    "# create dl for each rank across all ranks\n",
    "tpu_dls = [TPUDistributedDL(dl, rank=rank, world_size=world_size) for rank in range(world_size)]\n",
    "rank_batches = [list(tpu_dl) for tpu_dl in tpu_dls]\n",
    "# TODO: check that each rank dont contain common items\n",
    "# TODO: check that all items in dl are accounted for in the tpu_dls across all ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Dataloader Patches\n",
    "\n",
    "These patches to the torch dataloader make torch dataloaders\n",
    "more compatible with fastai dataloaders (enabling them to run\n",
    "inside the fastai Learners and use fastai training calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as th_data\n",
    "import torch.utils.data.distributed as th_distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from fastcore.basics import patch\n",
    "\n",
    "# import torch.utils.data as th_data\n",
    "from fastcore.transform import Pipeline\n",
    "# import torch.utils.data.distributed as th_distrib\n",
    "\n",
    "# from torch.data.utils.DataLoader\n",
    "# def __setattr__(self, attr, val):\n",
    "#     if self.__initialized and attr in (\n",
    "#             'batch_size', 'batch_sampler', 'sampler', 'drop_last', 'dataset', 'persistent_workers'):\n",
    "#         raise ValueError('{} attribute should not be set after {} is '\n",
    "#                             'initialized'.format(attr, self.__class__.__name__))\n",
    "@patch\n",
    "def __setattr__(self:th_data.DataLoader, attr, val):\n",
    "    'remove sampler,batch_sampler from list of attrs which should not be set after init'\n",
    "    initialized = getattr(self,'__initialized', False)\n",
    "    if initialized and attr in (\n",
    "            'batch_size', 'drop_last', 'dataset', 'persistent_workers'):\n",
    "        raise ValueError('{} attribute should not be set after {} is '\n",
    "                            'initialized'.format(attr, self.__class__.__name__))\n",
    "    super(th_data.DataLoader, self).__setattr__(attr, val)\n",
    "\n",
    "@patch(as_prop=True)\n",
    "def after_batch(self:th_data.DataLoader):\n",
    "    'return empty pipeline when fastai learner looks for after_batch'\n",
    "    return Pipeline()\n",
    "\n",
    "@patch(as_prop=True)\n",
    "def bs(self:th_data.DataLoader):\n",
    "    'return fastai synonym for torch batch size'\n",
    "    return self.batch_size\n",
    "\n",
    "@patch(as_prop=True)\n",
    "def device(self:th_data.DataLoader):\n",
    "    'return null device'\n",
    "    device = getattr(self,'_device', torch.device('cpu'))\n",
    "    return device\n",
    "    \n",
    "@patch\n",
    "def to(self:th_data.DataLoader, device):\n",
    "    'add impl for to(device)'\n",
    "    self._device = device\n",
    "    return self\n",
    "\n",
    "@patch\n",
    "def set_distributed_sampler(self:th_data.DataLoader, rank, world_size):\n",
    "    'replace sampler with torch distributed sampler'\n",
    "    distrib_sampler = th_distrib.DistributedSampler(self.dataset, \n",
    "                                                    num_replicas=world_size,\n",
    "                                                    rank=rank, shuffle=True)\n",
    "    self.sampler = distrib_sampler\n",
    "    batch_sampler_klass = self.batch_sampler.__class__\n",
    "    self.batch_sampler = batch_sampler_klass(self.sampler, \n",
    "                                             self.batch_size, \n",
    "                                             self.drop_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component Functions for Multi Core TPU Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.utils.data as th_data\n",
    "\n",
    "def build_distributed_dataloaders(dls, rank, world_size, sync_valid=False):\n",
    "    \"\"\"Wrap dataloaders with distributed TPU aware dataloader \"\"\"\n",
    "    new_loaders = []\n",
    "    for i,dl in enumerate(dls.loaders):\n",
    "        if i == 0 or sync_valid:\n",
    "            use_rank = rank\n",
    "            use_size = world_size\n",
    "        else:\n",
    "            use_rank = 0\n",
    "            use_size = 1\n",
    "        if isinstance(dl, th_data.DataLoader):\n",
    "            if i == 0: # set train dl to use distrib sampler\n",
    "                dl.set_distributed_sampler(use_rank, use_size)\n",
    "        else: # fastai dataloader\n",
    "            dl = TPUDistributedDL(dl,\n",
    "                                rank=use_rank,\n",
    "                                world_size=use_size)\n",
    "        new_loaders += [dl]\n",
    "    return DataLoaders(*new_loaders, path=dls.path, device=dls.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"build_distributed_dataloaders\" class=\"doc_header\"><code>build_distributed_dataloaders</code><a href=\"__main__.py#L4\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>build_distributed_dataloaders</code>(**`dls`**, **`rank`**, **`world_size`**, **`sync_valid`**=*`False`*)\n\nWrap dataloaders with distributed TPU aware dataloader ",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "#colab\n",
    "show_doc(build_distributed_dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.meta import delegates\n",
    "from fastai.data.block import DataBlock\n",
    "\n",
    "@delegates(DataBlock.dataloaders,but='datablock,rank,world_size,sync_valid,device')\n",
    "def make_fastai_dataloaders(datablock, source, rank, world_size, device=None, path='.', sync_valid=False, verbose=False,**kwargs):\n",
    "    \"create fastai-based dataloaders from a datablock and wrap a tpu distributed dataloader around them\"\n",
    "    dls = datablock.dataloaders(source=source, path=path, device=device, **kwargs)\n",
    "    distrib_dls = build_distributed_dataloaders(dls, rank, world_size, sync_valid=sync_valid)\n",
    "    return distrib_dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"make_fastai_dataloaders\" class=\"doc_header\"><code>make_fastai_dataloaders</code><a href=\"__main__.py#L5\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>make_fastai_dataloaders</code>(**`datablock`**, **`source`**, **`rank`**, **`world_size`**, **`device`**=*`None`*, **`path`**=*`'.'`*, **`sync_valid`**=*`False`*, **`verbose`**=*`False`*)\n\ncreate fastai-based dataloaders from a datablock and wrap a tpu distributed dataloader around them",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "#colab\n",
    "show_doc(make_fastai_dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def wrap_parallel_loader(loader, device):\n",
    "    'wraps a tpu distributed loader or a torch dataloader (with distributed sampler) with xla parallel loader'\n",
    "    para_loader = pl.ParallelLoader(loader, [device])\n",
    "    loop_loader = para_loader.per_device_loader(device)\n",
    "    return loop_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"wrap_parallel_loader\" class=\"doc_header\"><code>wrap_parallel_loader</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>wrap_parallel_loader</code>(**`loader`**, **`device`**)\n\nwraps a tpu distributed loader or a torch dataloader (with distributed sampler) with xla parallel loader",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "#colab\n",
    "\n",
    "show_doc(wrap_parallel_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "from fastai.learner import Recorder\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.learner import CancelValidException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class XLATrainingCallback(Callback):\n",
    "    \"A callback for training as a spawned process on multi-core TPUs\"\n",
    "    run_before = Recorder\n",
    "    run_valid = False\n",
    "    order = -5 # after TrainEvalCallback\n",
    "    def __init__(self, device, rank=0, sync_valid=False):\n",
    "        self.pdevice = device\n",
    "        self.rank = rank\n",
    "        self.sync_valid = sync_valid\n",
    "\n",
    "    def before_fit(self):\n",
    "        if not getattr(self.learn,'inner_xla', False):\n",
    "            return # skip if not spawned\n",
    "        xm.master_print('start fit')\n",
    "\n",
    "    def before_epoch(self):\n",
    "        # set the epoch on train only to make sure shuffle produces same seq\n",
    "        # across all ranks\n",
    "        if not getattr(self.learn,'inner_xla',False):\n",
    "            return # skip if not spawned\n",
    "\n",
    "        if hasattr(self.learn.dls.train,'sampler'):\n",
    "            if hasattr(self.learn.dls.train.sampler,'set_epoch'):\n",
    "                self.learn.dls.train.sampler.set_epoch(self.learn.epoch)\n",
    "        elif hasattr(self.learn.dls.train,'set_epoch'):\n",
    "            self.learn.dls.train.set_epoch(self.learn.epoch)\n",
    "\n",
    "        if self.sync_valid: # update epoch on valid if sync_valid\n",
    "            if hasattr(self.learn.dls.valid,'sampler'):\n",
    "                if hasattr(self.learn.dls.valid.sampler,'set_epoch'):\n",
    "                    self.learn.dls.valid.sampler.set_epoch(self.learn.epoch)\n",
    "            elif hasattr(self.learn.dls.valid,'set_epoch'):\n",
    "                self.learn.dls.valid.set_epoch(self.learn.epoch)\n",
    "\n",
    "    def before_train(self):\n",
    "        if not getattr(self.learn,'inner_xla',False):\n",
    "            return # skip if not spawned\n",
    "\n",
    "        self.learn.dl = wrap_parallel_loader(self.dls.train, self.pdevice)\n",
    "\n",
    "    def before_validate(self):\n",
    "        \"Set the model in validation mode\"\n",
    "        if not getattr(self.learn,'inner_xla',False):\n",
    "            return # skip if not spawned\n",
    "\n",
    "        if self.rank != 0 and not self.sync_valid:\n",
    "        # no need to compute valid loss/ metric if not master if not sync valid\n",
    "            raise CancelValidException()\n",
    "\n",
    "        if not isinstance(self.learn.dl, pl.PerDeviceLoader):\n",
    "            self.learn.dl = wrap_parallel_loader(self.learn.dl, self.pdevice)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `XLATrainingCallback` is responsible for the following functions:\n",
    "   * sets the `epoch` on either the torch dataloader sampler or the TPU distributed DL before each epoch. This ensures that for each epoch, samples in each batch are the same across all ranks, but each rank will pick the subset of batches for each rank.\n",
    "\n",
    "   The `TPUDistributedDL` (and the torch distributed sampler) ensures that all the samples (with some duplication if the samples are not exactly divisible by the number of ranks) are seen by one of the dataloaders across the ranks least once per epoch.\n",
    "   * wraps the dataloader (either training or validation) with the XLA Parallel Loader (`torch_xla.distributed.parallel_loader.ParallelLoader`) before each training or validation run.\n",
    "   * sidesteps the call to `opt.step` and instead calls `xm.optimizer_step(opt)` to sync the model gradients across all the ranks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "import copy\n",
    "from fastai.learner import _maybe_item\n",
    "from fastprogress.fastprogress import format_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for  `SyncRecorderCallback`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pack_metric(metrics):\n",
    "    \"extract counts and totals from avg metrics and avg losses into a list\"\n",
    "    counts = metrics.attrgot('count',0)\n",
    "    totals = metrics.attrgot('total',0)\n",
    "    metrics_list = counts + totals\n",
    "    return metrics_list\n",
    "\n",
    "def make_tensor(o, device):\n",
    "    \"convert a scalar or tensor into a float tensor and move them to `device`\"\n",
    "    if not isinstance(o, torch.Tensor):\n",
    "        o = torch.tensor(o)\n",
    "    return o.float().to(device)\n",
    "\n",
    "def pack_metrics(all_metrics, device):\n",
    "    \"pack train and valid metrics into a list of float tensors and move them to `device`\"\n",
    "    metrics_list = pack_metric(all_metrics['train_mets']) + pack_metric(all_metrics['valid_mets'])\n",
    "    return [make_tensor(item,device) for item in metrics_list ]\n",
    "\n",
    "def restore_metrics(reduced_metrics, all_metrics):\n",
    "    \"restore list of float tensors (count and values) back into train and valid metrics\"\n",
    "    n_train = len(all_metrics['train_mets'])\n",
    "    n_valid = len(all_metrics['valid_mets'])\n",
    "    train_counts = reduced_metrics[:n_train]\n",
    "    train_totals = reduced_metrics[n_train: n_train*2]\n",
    "    valid_counts = reduced_metrics[n_train*2: n_train*2 + n_valid]\n",
    "    valid_totals = reduced_metrics[n_train*2 + n_valid:]\n",
    "    for i,metric in enumerate(all_metrics['train_mets']):\n",
    "        if hasattr(metric,'count'):\n",
    "            metric.count = train_counts[i].clone().detach().long()\n",
    "        if hasattr(metric,'total'):\n",
    "            metric.total = train_totals[i].clone().detach()\n",
    "    for i,metric in enumerate(all_metrics['valid_mets']):\n",
    "        if hasattr(metric,'count'):\n",
    "            metric.count = valid_counts[i].clone().detach().long()\n",
    "        if hasattr(metric,'total'):\n",
    "            metric.total = valid_totals[i].clone().detach()\n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from fastai.learner import AvgSmoothLoss\n",
    "\n",
    "class SyncedAvgSmoothLoss(AvgSmoothLoss):\n",
    "    \"Smooth average of the losses (exponentially weighted with `beta`) synced across all ranks\"\n",
    "    def __init__(self, beta=0.98):\n",
    "        super(SyncedAvgSmoothLoss, self).__init__(beta=beta)\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        self.count += 1\n",
    "        # get loss across all ranks\n",
    "        synced_loss = xm.all_reduce(xm.REDUCE_SUM, learn.loss.mean())\n",
    "        avg_synced_loss = synced_loss/xm.xrt_world_size()\n",
    "        self.val = torch.lerp(avg_synced_loss, self.val, self.beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SyncRecorderCallback(Callback):\n",
    "    \"\"\"A `Callback` to sync the metrics from each rank and update statistics\n",
    "       accordingly so it will display correctly in the progress callback\n",
    "    \"\"\"\n",
    "    order  = 55 # after Recorder, before ProgressCallback\n",
    "\n",
    "    def before_fit(self):\n",
    "        if not getattr(self.learn,'inner_xla',False):\n",
    "            return # skip if not spawned\n",
    "\n",
    "        # replace AvgSmoothLoss  with SyncedAvgSmoothLoss which\n",
    "        # uses mean loss across all ranks per batch to compute smooth loss\n",
    "        # instead of just using one rank's mean loss    \n",
    "        if not isinstance(self.recorder.smooth_loss, SyncedAvgSmoothLoss):\n",
    "            orig_beta = self.recorder.smooth_loss.beta\n",
    "            self.recorder.smooth_loss = SyncedAvgSmoothLoss(beta=orig_beta)\n",
    "            self.recorder.smooth_loss.reset()\n",
    "\n",
    "        if not xm.is_master_ordinal():\n",
    "            return\n",
    "\n",
    "        if 'progress' in self.learn.cbs.attrgot('name',None):\n",
    "            self._sync_stats_log = self.progress._write_stats\n",
    "        else:\n",
    "            self._sync_stats_log = self.learn.logger\n",
    "\n",
    "    def before_epoch(self):\n",
    "        if not getattr(self.learn,'inner_xla',False):\n",
    "            return # skip if not spawned\n",
    "\n",
    "        self.sync_log = copy.copy(self.recorder.log)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        if not getattr(self.learn,'inner_xla',False):\n",
    "            return # skip if not spawned\n",
    "\n",
    "        if 'recorder' not in self.learn.cbs.attrgot('name'):\n",
    "            all_metrics = {\n",
    "                'train_mets': L([]),\n",
    "                'valid_mets': L([]),\n",
    "            }\n",
    "        else:\n",
    "            all_metrics = {\n",
    "                'train_mets': self.recorder._train_mets,\n",
    "                'valid_mets': self.recorder._valid_mets,\n",
    "            }\n",
    "        # send metrics data to sync ranks across spawned processes\n",
    "        device = self.learn.xla_training.pdevice\n",
    "        packed_metrics = pack_metrics(all_metrics, device) # convert metrics to tensor list on TPU\n",
    "        reduced_metrics = xm.all_reduce(xm.REDUCE_SUM, packed_metrics)\n",
    "        xm.mark_step()\n",
    "        if xm.is_master_ordinal():\n",
    "            all_metrics = restore_metrics(reduced_metrics, all_metrics) # convert list to metric objects\n",
    "            for m in self.recorder._train_mets:\n",
    "                self.sync_log += _maybe_item(m)\n",
    "\n",
    "            for m in self.recorder._valid_mets:\n",
    "                self.sync_log += _maybe_item(m)\n",
    "\n",
    "            self.learn.final_record = self.sync_log[1:].copy()\n",
    "            del self.recorder.values[-1] # remove last entry added by recorder\n",
    "            self.recorder.values.append(self.learn.final_record) # add updated metrics\n",
    "            if self.recorder.add_time:\n",
    "                updated_time = (time.time() - self.recorder.start_epoch)\n",
    "                self.sync_log.append(format_time(updated_time))\n",
    "            self.recorder.log = self.sync_log\n",
    "            self._sync_stats_log(self.sync_log) # write_stats to output\n",
    "            self.learn.logger = self.orig_logger # restore orig logger after skipping recorder.logger(log)\n",
    "\n",
    "    def after_validate(self):\n",
    "        if not getattr(self.learn,'inner_xla',False):\n",
    "            return # skip if not spawned\n",
    "\n",
    "        if xm.is_master_ordinal():\n",
    "            self.orig_logger = self.learn.logger\n",
    "            self.learn.logger = noop # write to logger disabled so calling recorder.logger(log) wont print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from fastcore.basics import patch\n",
    "# uncomment for notebook2html\n",
    "# import torch_xla.distributed.parallel_loader as pl\n",
    "# from fastai_xla_extensions.utils import xla_imported\n",
    "\n",
    "if xla_imported():\n",
    "    @patch\n",
    "    def close(self:pl.PerDeviceLoader):\n",
    "        'close data loader queues on xla parallel loader'\n",
    "        self._loader.close() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"PerDeviceLoader.close\" class=\"doc_header\"><code>PerDeviceLoader.close</code><a href=\"__main__.py#L9\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>PerDeviceLoader.close</code>()\n\nclose data loader queues on xla parallel loader",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "#colab\n",
    "show_doc(pl.PerDeviceLoader.close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import torch\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.learner import CancelFitException\n",
    "\n",
    "class SyncedCancelCallback(Callback):\n",
    "    \"\"\"A Callback to cancel training in sync \n",
    "       (closing data loaders queues across all ranks)\"\"\"\n",
    "    order = 199 # after all other callbacks\n",
    "\n",
    "    def before_fit(self):   \n",
    "        if not getattr(self.learn,'inner_xla',False):\n",
    "            return # skip if not spawned\n",
    " \n",
    "        self.zero = torch.zeros(1).to(self.xla_training.pdevice)\n",
    "        self.one = torch.ones(1).to(self.xla_training.pdevice)\n",
    "        self.sync_cancel_fit = self.zero\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not getattr(self.learn,'inner_xla',False):\n",
    "            return # skip if not spawned\n",
    "\n",
    "        cancel_fit = xm.all_reduce(xm.REDUCE_SUM, self.sync_cancel_fit)\n",
    "\n",
    "        if cancel_fit > self.zero: # a rank triggered a cancel \n",
    "            self.dl.close() # close per device loader\n",
    "            raise CancelFitException() \n",
    "    \n",
    "    def trigger_cancel_fit(self):\n",
    "        self.sync_cancel_fit = self.one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner Patches and Helper Functions for Multi Core TPU Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.imports import noop\n",
    "#from fastcore.basics import patch\n",
    "from fastai.learner import Learner\n",
    "from fastai.callback.progress import ProgressCallback\n",
    "from fastcore.xtras import join_path_file\n",
    "#from fastai.torch_core import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#copied from `torch_xla.core.xla_model.save` with the addition of rendezvous as a param\n",
    "\n",
    "def xm_save(data, file_or_path, master_only=True, global_master=False, rendezvous=True):\n",
    "    \"\"\"Saves the input data into a file.\n",
    "\n",
    "    The saved data is transferred to PyTorch CPU device before being saved, so a\n",
    "    following `torch.load()` will load CPU data.\n",
    "    Care must be taken when working with views. Instead of saving views it's\n",
    "    recommended that you recreate them after the tensors have been loaded and\n",
    "    moved to their destination device(s).\n",
    "\n",
    "    Args:\n",
    "    data: The input data to be saved. Any nested combination of Python objects\n",
    "        (list, tuples, sets, dicts, ...).\n",
    "    file_or_path: The destination for the data saving operation. Either a file\n",
    "        path or a Python file object. If `master_only` is ``False`` the path or\n",
    "        file objects must point to different destinations as otherwise all the\n",
    "        writes from the same host will override each other.\n",
    "    master_only (bool, optional): Whether only the master device should save the\n",
    "        data. If False, the `file_or_path` argument should be a different file or\n",
    "        path for each of the ordinals taking part to the replication, otherwise\n",
    "        all the replicas on the same host will be writing to the same location.\n",
    "        Default: True\n",
    "    global_master (bool, optional): When ``master_only`` is ``True`` this flag\n",
    "        controls whether every host's master (if ``global_master`` is ``False``)\n",
    "        saves the content, or only the global master (ordinal 0).\n",
    "        Default: False\n",
    "    \"\"\"\n",
    "    should_write_data = not master_only or xm.is_master_ordinal(\n",
    "        local=not global_master)\n",
    "\n",
    "    cpu_data = xm._maybe_convert_to_cpu(data, convert=should_write_data)\n",
    "    if should_write_data:\n",
    "        torch.save(cpu_data, file_or_path)\n",
    "    if rendezvous:\n",
    "        xm.rendezvous('torch_xla.core.xla_model.save')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.callback.tracker import SaveModelCallback\n",
    "from fastcore.basics import patch\n",
    "@patch\n",
    "def _save(self:SaveModelCallback, name): \n",
    "    'save best model using `rendezvous=False`'\n",
    "    if getattr(self.learn,'inner_xla', False):\n",
    "        self.last_saved_path = self.learn.save(name, with_opt=self.with_opt, \n",
    "                                           rendezvous=False)\n",
    "    else:\n",
    "        self.last_saved_path = self.learn.save(name, with_opt=self.with_opt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"SaveModelCallback._save\" class=\"doc_header\"><code>SaveModelCallback._save</code><a href=\"__main__.py#L4\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>SaveModelCallback._save</code>(**`name`**)\n\nsave best model using `rendezvous=False`",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "#colab\n",
    "show_doc(SaveModelCallback._save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@patch\n",
    "@delegates(Learner.save, but='rendezvous')\n",
    "def save(self:Learner, file, **kwargs):\n",
    "    file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n",
    "    with_opt = kwargs.pop('with_opt', self.opt is not None)\n",
    "    pickle_protocol = kwargs.pop('pickle_protocol', 2)\n",
    "\n",
    "    state = self.model.state_dict()\n",
    "    if with_opt:\n",
    "        # add opt state to state to be saved\n",
    "        opt_state = self.opt.state_dict()\n",
    "        state = {'model': state, 'opt':opt_state}\n",
    "    if getattr(self,'inner_xla',False):\n",
    "        xm_save(state, file, **kwargs) # use xm_save instead of torch.save\n",
    "    else:\n",
    "        # use default if not spawned\n",
    "        torch.save(state,file,pickle_protocol=pickle_protocol)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"Learner.save\" class=\"doc_header\"><code>Learner.save</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Learner.save</code>(**`file`**, **`with_opt`**=*`True`*, **`pickle_protocol`**=*`2`*)\n\n",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "#colab\n",
    "show_doc(Learner.save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Learner.save` has been patched to use the torch xla method `xm.save` which will save the model weights for the model on the TPU device. Moreover, `xm.save` only saves the weights on the master ordinal rank process by default, ensuring that only one copy of the model is written to a file. _Which is fine, since the `xm.optimizer_step` done on each training batch synchronizes the weights across all ranks anyway._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@patch\n",
    "def to_multi_xla(self:Learner,device, rank, sync_valid=False):\n",
    "    \"Sets up the learner on the spawned process for multi core TPU training\"\n",
    "    # add xla info on learner\n",
    "    self.inner_xla = True\n",
    "    self.xla_rank = rank\n",
    "    if 'xla_training' not in self.cbs.attrgot('name'):\n",
    "        self.dls.device = None\n",
    "        self.add_cbs([XLATrainingCallback(device, rank, sync_valid=sync_valid),\n",
    "                      XLAOptCallback()])\n",
    "        self.opt = None # clear opt to ensure\n",
    "\n",
    "    else:\n",
    "        self.xla_training.pdevice = device\n",
    "        self.xla_training.rank = rank\n",
    "        self.xla_training.sync_valid = sync_valid\n",
    "\n",
    "    if sync_valid and 'sync_recorder' not in self.cbs.attrgot('name'):\n",
    "        self.add_cbs(SyncRecorderCallback)\n",
    "    elif not sync_valid:\n",
    "        self.remove_cbs(SyncRecorderCallback)\n",
    "\n",
    "    if 'synced_cancel' not in self.cbs.attrgot('name'):\n",
    "        self.add_cbs(SyncedCancelCallback)\n",
    "        \n",
    "    if rank != 0: # progress bar only for rank 0\n",
    "        self.remove_cbs(ProgressCallback)\n",
    "    self.logger = xm.master_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"Learner.to_multi_xla\" class=\"doc_header\"><code>Learner.to_multi_xla</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Learner.to_multi_xla</code>(**`device`**, **`rank`**, **`sync_valid`**=*`False`*)\n\nSets up the learner on the spawned process for multi core TPU training",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "#colab\n",
    "show_doc(Learner.to_multi_xla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# for testing\n",
    "def do_one_loop(dl, rank, world_size, device, wrap_parallel=True):\n",
    "    \"test one loop for a tpu distributed dataloader\"\n",
    "    n_batches = len(dl)\n",
    "    print(f'xla: {rank} world_size: {world_size} n_batches:{n_batches}')\n",
    "\n",
    "    if wrap_parallel:\n",
    "        print(f'xla: {rank} wrapping ploader')\n",
    "        pdl = wrap_parallel_loader(dl, device=device)\n",
    "    else:\n",
    "        pdl = dl\n",
    "    for i,b in enumerate(pdl):\n",
    "        if i > 1:\n",
    "            break\n",
    "        xb, yb = b\n",
    "        print(f'xla: {rank} iter:{i} xb type {type(xb)} yb type: {type(yb)}')\n",
    "        print(f'xla: {rank} iter:{i} xb.shape {xb.shape} yb.shape: {yb.shape}')\n",
    "        print(f'xla: {rank} iter:{i} xb.device {xb.device} yb.device: {yb.device}')\n",
    "        print(f'xla: {rank} iter:{i} xb.dtype {xb.dtype} yb.device: {yb.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#colab\n",
    "from functools import partial\n",
    "from fastai.metrics import accuracy\n",
    "from fastai.optimizer import SGD, Adam\n",
    "\n",
    "from fastcore.basics import first\n",
    "from fastai.callback.schedule import *\n",
    "from fastai.test_utils import VerboseCallback\n",
    "from my_timesaver_utils.profiling import *\n",
    "from my_timesaver_utils.profiling_callback import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "def run_dataloader_loop(rank):\n",
    "    torch.manual_seed(1)\n",
    "    print(f'xla {rank} start run_dataloader_loop')\n",
    "    xm.rendezvous('start_run_dataloader_loop')\n",
    "    # Scale learning rate to num cores\n",
    "    learning_rate = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
    "    SYNC_VALID = FLAGS['sync_valid']\n",
    "    IS_PROFILING = FLAGS['is_profiling']\n",
    "    # Get loss function, optimizer, and model\n",
    "    device = xm.xla_device()\n",
    "    model = WRAPPED_MODEL.to(device)\n",
    "    bs = FLAGS['batch_size']\n",
    "    world_size = xm.xrt_world_size()\n",
    "    if IS_PROFILING:\n",
    "        rec_name = 'rank' + str(rank) + '_dataloader_build'\n",
    "        print(f'start {rec_name}')\n",
    "        start_record(rec_name)\n",
    "\n",
    "    # dls = make_fastai_dataloaders(\n",
    "    #                         DATA, \n",
    "    #                         PATH, \n",
    "    #                         rank=rank, \n",
    "    #                         world_size=world_size, \n",
    "    #                         sync_valid=SYNC_VALID,\n",
    "    #                         bs=bs,)\n",
    "    dls = DATA.dataloaders(PATH, bs=bs)\n",
    "    # distrib_dls = build_distributed_dataloaders(dls, rank, world_size, \n",
    "    #                                            sync_valid=True)\n",
    "    dl = dls.train\n",
    "    tpu_dl = TPUDistributedDL(dl,rank=rank,world_size=world_size)\n",
    "    print(f'xla: {rank} fake_l.num_workers {tpu_dl.fake_l.num_workers}')\n",
    "    do_one_loop(tpu_dl, rank, world_size, device, wrap_parallel=False)\n",
    "    if IS_PROFILING:\n",
    "        end_record(rec_name)\n",
    "        print_prof_data(rec_name)\n",
    "        print(f'finished {rec_name}')\n",
    "\n",
    "    xm.mark_step()\n",
    "    print(f'xla {rank} completed run_dataloader_loop')\n",
    "    # print_prof_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "def train_model(rank):\n",
    "    torch.manual_seed(1)\n",
    "    xm.rendezvous('start_train_model')\n",
    "    print(f'xla {rank} start train model')\n",
    "\n",
    "    \n",
    "    SYNC_VALID = FLAGS['sync_valid']\n",
    "    IS_PROFILING = FLAGS['is_profiling']\n",
    "    # Get loss function, optimizer, and model\n",
    "    device = xm.xla_device()\n",
    "\n",
    "    bs = FLAGS['batch_size']\n",
    "    world_size = xm.xrt_world_size()\n",
    "    if IS_PROFILING:\n",
    "        rec_name = 'rank' + str(rank) + '_dataloader_build'\n",
    "        print(f'start {rec_name}')\n",
    "        start_record(rec_name)\n",
    "\n",
    "    dls = make_fastai_dataloaders(\n",
    "                            DATA, \n",
    "                            PATH, \n",
    "                            rank=rank, \n",
    "                            world_size=world_size, \n",
    "                            sync_valid=SYNC_VALID,\n",
    "                            bs=bs,)\n",
    "    if IS_PROFILING:\n",
    "        end_record(rec_name)\n",
    "        print_prof_data(rec_name)\n",
    "        print(f'finished {rec_name}')\n",
    "    model = WRAPPED_MODEL.to(device)\n",
    "    moms =(FLAGS['momentum'],FLAGS['momentum'],FLAGS['momentum'])\n",
    "    wd = FLAGS['weight_decay']\n",
    "\n",
    "    xm.master_print('build learner')\n",
    "    learner = Learner(dls, model, \n",
    "                      loss_func=LOSS_FUNC, \n",
    "                      opt_func=OPT_FUNC, \n",
    "                      metrics=accuracy, \n",
    "                      wd=wd,\n",
    "                      moms=moms)\n",
    "                      \n",
    "    learner.to_multi_xla(device, rank=xm.get_ordinal(), sync_valid=SYNC_VALID)\n",
    "    if IS_PROFILING and rank == 0:\n",
    "        learner.to_my_profile()\n",
    "\n",
    "    # Scale learning rate to num cores\n",
    "    learning_rate = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
    "                               \n",
    "    epochs = FLAGS['num_epochs']\n",
    "    xm.master_print('start running fit')\n",
    "    learner.unfreeze()\n",
    "    if IS_PROFILING:\n",
    "        rec_name3 = 'rank' + str(rank) + '_run_fit'\n",
    "        print(f'start {rec_name3}')\n",
    "        start_record(rec_name3)\n",
    "\n",
    "    learner.fit_one_cycle(epochs, lr_max=slice(learning_rate/10))\n",
    "    if IS_PROFILING:\n",
    "        end_record(rec_name3)\n",
    "        print_prof_data(rec_name3)\n",
    "        print(f'finished {rec_name3}')\n",
    "    xm.rendezvous('end_train_model')\n",
    "    learner.save('stage-1', rendezvous=False)\n",
    "    if rank == 0 and IS_PROFILING :\n",
    "        learner.my_profile.print_stats()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "def train_mnist_model(rank):\n",
    "    torch.manual_seed(1)\n",
    "    xm.rendezvous('start_train_mnist_model')\n",
    "    print(f'xla {rank} start train mnist model')    \n",
    "    SYNC_VALID = FLAGS2['sync_valid']\n",
    "    device = xm.xla_device()\n",
    "\n",
    "    bs = FLAGS2['batch_size']\n",
    "    world_size = xm.xrt_world_size()\n",
    "\n",
    "    dls = make_fastai_dataloaders(\n",
    "                            DATA2, \n",
    "                            PATH2, \n",
    "                            rank=rank, \n",
    "                            world_size=world_size, \n",
    "                            sync_valid=SYNC_VALID,\n",
    "                            bs=bs,)\n",
    "    model = WRAPPED_MODEL2.to(device)\n",
    "    moms =(FLAGS2['momentum'],FLAGS2['momentum'],FLAGS2['momentum'])\n",
    "    wd = FLAGS2['weight_decay']\n",
    "\n",
    "    xm.master_print('build learner')\n",
    "    learner = Learner(dls, model, \n",
    "                      loss_func=LOSS_FUNC, \n",
    "                      opt_func=OPT_FUNC, \n",
    "                      metrics=accuracy, \n",
    "                      wd=wd,\n",
    "                      moms=moms)\n",
    "                      \n",
    "    learner.to_multi_xla(device, rank=xm.get_ordinal(), sync_valid=SYNC_VALID)\n",
    "    # Scale learning rate to num cores\n",
    "    learning_rate = FLAGS2['learning_rate'] * xm.xrt_world_size()\n",
    "                               \n",
    "    epochs = FLAGS2['num_epochs']\n",
    "    xm.master_print('start running fit')\n",
    "    learner.unfreeze()\n",
    "\n",
    "    learner.fit_one_cycle(epochs, lr_max=slice(learning_rate/10))\n",
    "    xm.rendezvous('end_train_mnist_model')\n",
    "    learner.save('mnist-stage-1', rendezvous=False)\n",
    "    xm.mark_step()  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main method that runs the training. \n",
    "\n",
    "It includes some profiling code to measure the building of the `dataloaders` and running of the `fit` methods. \n",
    "\n",
    "At the end of the spawned processes, the master ordinal process saves the model to a temporary file. (see `Learner.save` patch above)\n",
    "\n",
    "The saved model will then be loaded by the main process so that it will now contain the trained weights updated by the spawned training processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# Start training processes\n",
    "def _mp_fn(rank, flags):\n",
    "    global FLAGS\n",
    "    FLAGS = flags\n",
    "    train_model(rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# Start dataloader processes\n",
    "def _mp_fn2(rank, flags):\n",
    "    global FLAGS\n",
    "    FLAGS = flags\n",
    "    run_dataloader_loop(rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# Start training processes\n",
    "def _mp_fn3(rank, flags):\n",
    "    global FLAGS2\n",
    "    FLAGS2 = flags\n",
    "    train_mnist_model(rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fastcore.transform import DisplayedTransform, Transform\n",
    "from fastcore.basics import store_attr\n",
    "from fastai.vision.core import PILImage, PILBase, image2tensor\n",
    "from fastai.data.block import TransformBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.transforms import get_c\n",
    "# from fastai.vision.all import *\n",
    "from fastai.data.block import DataBlock, CategoryBlock\n",
    "from fastai.vision.data import ImageBlock\n",
    "from fastai.data.transforms import get_image_files, parent_label, GrandparentSplitter\n",
    "from fastai.vision.augment import Resize, aug_transforms\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastai.data.transforms import Normalize\n",
    "from fastai.vision.core import imagenet_stats\n",
    "from fastcore.basics import using_attr\n",
    "from fastai.data.transforms import RegexLabeller, CategoryMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "LOSS_FUNC = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.optimizer import Adam\n",
    "OPT_FUNC = Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.transforms import RandomSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.learner import create_cnn_model\n",
    "from fastai.vision.models import resnet34, resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Define Parameters\n",
    "FLAGS = {}\n",
    "# FLAGS['batch_size'] = 1024\n",
    "FLAGS['sync_valid'] = True\n",
    "FLAGS['is_profiling'] = True\n",
    "FLAGS['batch_size'] = 64\n",
    "FLAGS['num_workers'] = 4\n",
    "FLAGS['learning_rate'] = 1e-3\n",
    "FLAGS['image_size'] = 224\n",
    "FLAGS['momentum'] = 0.85\n",
    "FLAGS['weight_decay'] = 2e-3\n",
    "FLAGS['num_epochs'] = 5\n",
    "FLAGS['num_cores'] = 8 if os.environ.get('TPU_NAME', None) else 1\n",
    "# FLAGS['num_cores'] = 1 \n",
    "ARCH = resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Define Parameters\n",
    "FLAGS2 = {}\n",
    "FLAGS2['batch_size'] = 1024\n",
    "FLAGS2['sync_valid'] = True\n",
    "# FLAGS2['batch_size'] = 64\n",
    "FLAGS2['num_workers'] = 4\n",
    "FLAGS2['learning_rate'] = 1e-3\n",
    "FLAGS2['image_size'] = 28\n",
    "FLAGS2['momentum'] = 0.85\n",
    "FLAGS2['weight_decay'] = 2e-3\n",
    "FLAGS2['num_epochs'] = 5\n",
    "FLAGS2['num_cores'] = 8 if os.environ.get('TPU_NAME', None) else 1\n",
    "# FLAGS['num_cores'] = 1 \n",
    "ARCH2 = resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "from pathlib import Path\n",
    "from fastcore.xtras import *\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#colab\n",
    "PATH = untar_data(URLs.PETS)/'images'\n",
    "PATH2 = untar_data(URLs.MNIST)\n",
    "# PATH = untar_data(URLs.MNIST_TINY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "\n",
    "pat = r'(.+)_\\d+.jpg$'\n",
    "fname_labeller = using_attr(RegexLabeller(pat),'name') \n",
    "splitter=RandomSplitter(seed=42)\n",
    "DATA = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),\n",
    "    get_items=get_image_files,\n",
    "    get_y=fname_labeller,\n",
    "    splitter=splitter,\n",
    "    item_tfms=[Resize(FLAGS['image_size']),],\n",
    "    batch_tfms=[Normalize.from_stats(*imagenet_stats)]\n",
    ")\n",
    "vocab = CategoryMap(get_image_files(PATH).map(fname_labeller))\n",
    "N_OUT = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "DATA2 = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),\n",
    "    get_items=get_image_files,\n",
    "    get_y=parent_label,\n",
    "    splitter=GrandparentSplitter(train_name='training',valid_name='testing'),\n",
    "    item_tfms=[Resize(FLAGS2['image_size']),],\n",
    "    batch_tfms=[Normalize.from_stats(*imagenet_stats)]\n",
    ")\n",
    "vocab2 = CategoryMap(get_image_files(PATH2).map(parent_label))\n",
    "N_OUT2 = len(vocab2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "assert N_OUT is not None and N_OUT > 0,f'N_OUT {N_OUT} should be > 0'\n",
    "assert N_OUT2 is not None and N_OUT2 > 0,f'N_OUT2 {N_OUT2} should be > 0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is created by the main process and wrapped by the `xmp.MpModelWrapper`. This is to reduce the memory usage by not having multiple copies of the model in the spawned processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "custom_model = create_cnn_model(ARCH, N_OUT, \n",
    "                                pretrained=True,\n",
    "                                concat_pool=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "custom_model2 = create_cnn_model(ARCH2, N_OUT2, \n",
    "                                pretrained=True,\n",
    "                                concat_pool=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# Only instantiate model weights once in memory.\n",
    "WRAPPED_MODEL = xmp.MpModelWrapper(custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "WRAPPED_MODEL2 = xmp.MpModelWrapper(custom_model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xla 0 start run_dataloader_loop\n",
      "xla 2 start run_dataloader_loop\n",
      "xla 7 start run_dataloader_loop\n",
      "xla 6 start run_dataloader_loop\n",
      "xla 4 start run_dataloader_loop\n",
      "xla 3 start run_dataloader_loop\n",
      "xla 1 start run_dataloader_loop\n",
      "xla 5 start run_dataloader_loop\n",
      "start rank2_dataloader_build\n",
      "xla: 2 fake_l.num_workers 2\n",
      "xla: 2 world_size: 8 n_batches:12\n",
      "start rank4_dataloader_build\n",
      "xla: 2 iter:0 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 2 iter:0 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 2 iter:0 xb.device cpu yb.device: cpu\n",
      "xla: 2 iter:0 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 2 iter:1 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 2 iter:1 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 2 iter:1 xb.device cpu yb.device: cpu\n",
      "xla: 2 iter:1 xb.dtype torch.float32 yb.device: torch.int64\n",
      "start rank0_dataloader_build\n",
      "xla: 4 fake_l.num_workers 2\n",
      "xla: 4 world_size: 8 n_batches:12\n",
      "start rank7_dataloader_build\n",
      "xla: 0 fake_l.num_workers 2\n",
      "xla: 0 world_size: 8 n_batches:12\n",
      "start rank3_dataloader_build\n",
      "Function rank2_dataloader_build called 1 times.\n",
      "Execution time max: 6.657, average: 6.657\n",
      "finished rank2_dataloader_build\n",
      "xla 2 completed run_dataloader_loop\n",
      "xla: 4 iter:0 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 4 iter:0 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 4 iter:0 xb.device cpu yb.device: cpu\n",
      "xla: 4 iter:0 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 4 iter:1 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 4 iter:1 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 4 iter:1 xb.device cpu yb.device: cpu\n",
      "xla: 4 iter:1 xb.dtype torch.float32 yb.device: torch.int64\n",
      "start rank6_dataloader_build\n",
      "xla: 7 fake_l.num_workers 2\n",
      "xla: 7 world_size: 8 n_batches:12\n",
      "start rank5_dataloader_build\n",
      "xla: 3 fake_l.num_workers 2\n",
      "xla: 3 world_size: 8 n_batches:12\n",
      "start rank1_dataloader_build\n",
      "xla: 0 iter:0 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 0 iter:0 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 0 iter:0 xb.device cpu yb.device: cpu\n",
      "xla: 0 iter:0 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 0 iter:1 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 0 iter:1 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 0 iter:1 xb.device cpu yb.device: cpu\n",
      "xla: 0 iter:1 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 6 fake_l.num_workers 2\n",
      "xla: 6 world_size: 8 n_batches:12\n",
      "xla: 7 iter:0 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 7 iter:0 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 7 iter:0 xb.device cpu yb.device: cpu\n",
      "xla: 7 iter:0 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 7 iter:1 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 7 iter:1 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 7 iter:1 xb.device cpu yb.device: cpu\n",
      "xla: 7 iter:1 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 5 fake_l.num_workers 2\n",
      "xla: 5 world_size: 8 n_batches:12\n",
      "Function rank4_dataloader_build called 1 times.\n",
      "Execution time max: 14.426, average: 14.426\n",
      "finished rank4_dataloader_build\n",
      "xla 4 completed run_dataloader_loop\n",
      "xla: 3 iter:0 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 3 iter:0 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 3 iter:0 xb.device cpu yb.device: cpu\n",
      "xla: 1 fake_l.num_workers 2\n",
      "xla: 3 iter:0 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 1 world_size: 8 n_batches:12\n",
      "xla: 3 iter:1 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 3 iter:1 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 3 iter:1 xb.device cpu yb.device: cpu\n",
      "xla: 3 iter:1 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 6 iter:0 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 6 iter:0 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 6 iter:0 xb.device cpu yb.device: cpu\n",
      "xla: 6 iter:0 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 6 iter:1 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 6 iter:1 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 6 iter:1 xb.device cpu yb.device: cpu\n",
      "xla: 6 iter:1 xb.dtype torch.float32 yb.device: torch.int64\n",
      "Function rank0_dataloader_build called 1 times.\n",
      "Execution time max: 18.265, average: 18.265\n",
      "finished rank0_dataloader_build\n",
      "xla 0 completed run_dataloader_loop\n",
      "xla: 5 iter:0 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 5 iter:0 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 5 iter:0 xb.device cpu yb.device: cpu\n",
      "xla: 5 iter:0 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 5 iter:1 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 5 iter:1 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 5 iter:1 xb.device cpu yb.device: cpu\n",
      "xla: 5 iter:1 xb.dtype torch.float32 yb.device: torch.int64\n",
      "Function rank7_dataloader_build called 1 times.\n",
      "Execution time max: 18.361, average: 18.361\n",
      "finished rank7_dataloader_build\n",
      "xla 7 completed run_dataloader_loop\n",
      "xla: 1 iter:0 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 1 iter:0 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 1 iter:0 xb.device cpu yb.device: cpu\n",
      "xla: 1 iter:0 xb.dtype torch.float32 yb.device: torch.int64\n",
      "xla: 1 iter:1 xb type <class 'torch.Tensor'> yb type: <class 'torch.Tensor'>\n",
      "xla: 1 iter:1 xb.shape torch.Size([64, 3, 224, 224]) yb.shape: torch.Size([64])\n",
      "xla: 1 iter:1 xb.device cpu yb.device: cpu\n",
      "xla: 1 iter:1 xb.dtype torch.float32 yb.device: torch.int64\n",
      "Function rank3_dataloader_build called 1 times.\n",
      "Execution time max: 18.655, average: 18.655\n",
      "finished rank3_dataloader_build\n",
      "xla 3 completed run_dataloader_loop\n",
      "Function rank6_dataloader_build called 1 times.\n",
      "Execution time max: 18.129, average: 18.129\n",
      "finished rank6_dataloader_build\n",
      "xla 6 completed run_dataloader_loop\n",
      "Function rank5_dataloader_build called 1 times.\n",
      "Execution time max: 17.149, average: 17.149\n",
      "finished rank5_dataloader_build\n",
      "xla 5 completed run_dataloader_loop\n",
      "Function rank1_dataloader_build called 1 times.\n",
      "Execution time max: 15.682, average: 15.682\n",
      "finished rank1_dataloader_build\n",
      "xla 1 completed run_dataloader_loop\n",
      "CPU times: user 117 ms, sys: 135 ms, total: 252 ms\n",
      "Wall time: 44.7 s\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "#colab\n",
    "%%time\n",
    "xmp.spawn(_mp_fn2, args=(FLAGS,), nprocs=FLAGS['num_cores'],\n",
    "        start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xla 2 start train model\n",
      "xla 0 start train model\n",
      "xla 4 start train model\n",
      "xla 1 start train model\n",
      "xla 5 start train model\n",
      "xla 3 start train model\n",
      "xla 7 start train model\n",
      "xla 6 start train model\n",
      "build learner\n",
      "start running fit\n",
      "start fit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.763699</td>\n",
       "      <td>1.561952</td>\n",
       "      <td>0.612162</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.650145</td>\n",
       "      <td>0.838573</td>\n",
       "      <td>0.752027</td>\n",
       "      <td>01:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.584489</td>\n",
       "      <td>0.449414</td>\n",
       "      <td>0.869595</td>\n",
       "      <td>01:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.507114</td>\n",
       "      <td>0.301581</td>\n",
       "      <td>0.915540</td>\n",
       "      <td>01:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.431596</td>\n",
       "      <td>0.277951</td>\n",
       "      <td>0.915540</td>\n",
       "      <td>01:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 120 ms, sys: 123 ms, total: 243 ms\n",
      "Wall time: 8min 1s\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "%%time\n",
    "FLAGS['is_profiling'] = False\n",
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'],\n",
    "        start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xla 0 start train mnist model\n",
      "xla 2 start train mnist model\n",
      "xla 3 start train mnist model\n",
      "xla 7 start train mnist model\n",
      "xla 1 start train mnist model\n",
      "xla 4 start train mnist model\n",
      "xla 6 start train mnist model\n",
      "xla 5 start train mnist model\n",
      "build learner\n",
      "start running fit\n",
      "start fit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.478821</td>\n",
       "      <td>1.688675</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>01:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.326677</td>\n",
       "      <td>0.627990</td>\n",
       "      <td>0.857900</td>\n",
       "      <td>01:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.282783</td>\n",
       "      <td>0.120351</td>\n",
       "      <td>0.970200</td>\n",
       "      <td>01:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.251341</td>\n",
       "      <td>0.048564</td>\n",
       "      <td>0.986500</td>\n",
       "      <td>01:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.223247</td>\n",
       "      <td>0.040874</td>\n",
       "      <td>0.988300</td>\n",
       "      <td>01:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#colab\n",
    "# %%time\n",
    "xmp.spawn(_mp_fn3, args=(FLAGS2,), nprocs=FLAGS2['num_cores'],\n",
    "        start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting-up type transforms pipelines\n",
      "Collecting items from /root/.fastai/data/oxford-iiit-pet/images\n",
      "Found 7390 items\n",
      "2 datasets of sizes 5912,1478\n",
      "Setting up Pipeline: PILBase.create\n",
      "Setting up Pipeline: partial -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n",
      "\n",
      "Building one sample\n",
      "  Pipeline: PILBase.create\n",
      "    starting from\n",
      "      /root/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_147.jpg\n",
      "    applying PILBase.create gives\n",
      "      PILImage mode=RGB size=225x300\n",
      "  Pipeline: partial -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n",
      "    starting from\n",
      "      /root/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_147.jpg\n",
      "    applying partial gives\n",
      "      yorkshire_terrier\n",
      "    applying Categorize -- {'vocab': None, 'sort': True, 'add_na': False} gives\n",
      "      TensorCategory(36)\n",
      "\n",
      "Final sample: (PILImage mode=RGB size=225x300, TensorCategory(36))\n",
      "\n",
      "\n",
      "Collecting items from /root/.fastai/data/oxford-iiit-pet/images\n",
      "Found 7390 items\n",
      "2 datasets of sizes 5912,1478\n",
      "Setting up Pipeline: PILBase.create\n",
      "Setting up Pipeline: partial -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n",
      "Setting up after_item: Pipeline: Resize -- {'size': (224, 224), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (2, 0), 'p': 1.0} -> ToTensor\n",
      "Setting up before_batch: Pipeline: \n",
      "Setting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -> Normalize -- {'mean': tensor([[[[0.4850]],\n",
      "\n",
      "         [[0.4560]],\n",
      "\n",
      "         [[0.4060]]]]), 'std': tensor([[[[0.2290]],\n",
      "\n",
      "         [[0.2240]],\n",
      "\n",
      "         [[0.2250]]]]), 'axes': (0, 2, 3)}\n",
      "\n",
      "Building one batch\n",
      "Applying item_tfms to the first sample:\n",
      "  Pipeline: Resize -- {'size': (224, 224), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (2, 0), 'p': 1.0} -> ToTensor\n",
      "    starting from\n",
      "      (PILImage mode=RGB size=225x300, TensorCategory(36))\n",
      "    applying Resize -- {'size': (224, 224), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (2, 0), 'p': 1.0} gives\n",
      "      (PILImage mode=RGB size=224x224, TensorCategory(36))\n",
      "    applying ToTensor gives\n",
      "      (TensorImage of size 3x224x224, TensorCategory(36))\n",
      "\n",
      "Adding the next 3 samples\n",
      "\n",
      "No before_batch transform to apply\n",
      "\n",
      "Collating items in a batch\n",
      "\n",
      "Applying batch_tfms to the batch built\n",
      "  Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -> Normalize -- {'mean': tensor([[[[0.4850]],\n",
      "\n",
      "         [[0.4560]],\n",
      "\n",
      "         [[0.4060]]]]), 'std': tensor([[[[0.2290]],\n",
      "\n",
      "         [[0.2240]],\n",
      "\n",
      "         [[0.2250]]]]), 'axes': (0, 2, 3)}\n",
      "    starting from\n",
      "      (TensorImage of size 4x3x224x224, TensorCategory([36, 33, 30,  4]))\n",
      "    applying IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} gives\n",
      "      (TensorImage of size 4x3x224x224, TensorCategory([36, 33, 30,  4]))\n",
      "    applying Normalize -- {'mean': tensor([[[[0.4850]],\n",
      "\n",
      "         [[0.4560]],\n",
      "\n",
      "         [[0.4060]]]]), 'std': tensor([[[[0.2290]],\n",
      "\n",
      "         [[0.2240]],\n",
      "\n",
      "         [[0.2250]]]]), 'axes': (0, 2, 3)} gives\n",
      "      (TensorImage of size 4x3x224x224, TensorCategory([36, 33, 30,  4]))\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "DATA.summary(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "mdls = DATA.dataloaders(PATH, bs=FLAGS['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "mlearner = Learner(mdls, custom_model, \n",
    "                    loss_func=LOSS_FUNC, \n",
    "                    opt_func=OPT_FUNC, \n",
    "                    metrics=accuracy, \n",
    "                    wd=FLAGS['weight_decay'],\n",
    "                    moms=(FLAGS['momentum'],FLAGS['momentum'],FLAGS['momentum']))\n",
    "# load trained weights from multi core tpu training\n",
    "if Path('models/stage-1.pth').is_file():\n",
    "    mlearner.load('stage-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#colab\n",
    "mlearner.dls.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.torch_core import one_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#colab\n",
    "one_param(mlearner.model).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2831439673900604, 0.9147496819496155]\n",
      "CPU times: user 3min 27s, sys: 2.91 s, total: 3min 30s\n",
      "Wall time: 3min 33s\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "%%time\n",
    "valid_metrics = mlearner.validate();print(valid_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "FLAGS3 = {}\n",
    "FLAGS3['batch_size']  = 64\n",
    "FLAGS3['num_workers'] = 4\n",
    "FLAGS3['data_dir'] = Path('/content/data/cifar')\n",
    "FLAGS3['sync_valid'] = True\n",
    "FLAGS3['learning_rate'] = 1e-3\n",
    "FLAGS3['image_size'] = 28\n",
    "FLAGS3['momentum'] = 0.85\n",
    "FLAGS3['weight_decay'] = 2e-3\n",
    "FLAGS3['num_epochs'] = 5\n",
    "FLAGS3['num_cores'] = 8 if os.environ.get('TPU_NAME', None) else 1\n",
    "# FLAGS['num_cores'] = 1 \n",
    "ARCH3 = resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    norm = transforms.Normalize(\n",
    "        mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(FLAGS3['image_size'], padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        norm,\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((FLAGS3['image_size'],FLAGS3['image_size'])),                                 \n",
    "        transforms.ToTensor(),\n",
    "        norm,\n",
    "    ])\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=FLAGS3['data_dir'],\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform_train)\n",
    "    test_dataset = datasets.CIFAR10(\n",
    "        root=FLAGS3['data_dir'],\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform_test)\n",
    "    \n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/data/cifar/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0b5433290047c9b1f5ed3e8d554a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /content/data/cifar/cifar-10-python.tar.gz to /content/data/cifar\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "train_dataset, test_dataset = get_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=FLAGS3['batch_size'],\n",
    "#   sampler=train_sampler,\n",
    "    shuffle=True,\n",
    "    num_workers=FLAGS3['num_workers'],\n",
    "    drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=FLAGS3['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=FLAGS3['num_workers'],\n",
    "    drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "# fastai dls using torch dataloaders\n",
    "CIFAR_DLS = DataLoaders(train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OUT3 = 10 # cifar10 has 10 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "custom_model3 = create_cnn_model(ARCH3, N_OUT3, \n",
    "                                pretrained=True,\n",
    "                                concat_pool=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "WRAPPED_MODEL3 = xmp.MpModelWrapper(custom_model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "class PrintDevicesCallback(Callback):\n",
    "    order = -6 # before XLATrainingCallback\n",
    "    def before_train(self):\n",
    "        self.print_device()\n",
    "    def before_validate(self):\n",
    "        self.print_device()\n",
    "    def print_device(self):\n",
    "        if self.learn.epoch == 0:\n",
    "            print(f'train: {self.learn.training} xla {self.learn.xla_rank}: dl.type: {type(self.learn.dl)} dl.device {self.learn.dl.device} model.device: {one_param(self.learn.model).device}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "def train_cifar_model(rank):\n",
    "    torch.manual_seed(1)\n",
    "    xm.rendezvous('start_train_cifar_model')\n",
    "    print(f'xla {rank} start train cifar model')    \n",
    "    SYNC_VALID = FLAGS3['sync_valid']\n",
    "    device = xm.xla_device()\n",
    "\n",
    "    bs = FLAGS3['batch_size']\n",
    "    world_size = xm.xrt_world_size()\n",
    "\n",
    "    dls = build_distributed_dataloaders(CIFAR_DLS, \n",
    "                                        rank, \n",
    "                                        world_size, \n",
    "                                        sync_valid=SYNC_VALID) \n",
    "    model = WRAPPED_MODEL3.to(device)\n",
    "    moms =(FLAGS3['momentum'],FLAGS3['momentum'],FLAGS3['momentum'])\n",
    "    wd = FLAGS3['weight_decay']\n",
    "\n",
    "    xm.master_print('build learner')\n",
    "    learner = Learner(dls, model, \n",
    "                      loss_func=LOSS_FUNC, \n",
    "                      opt_func=OPT_FUNC, \n",
    "                      metrics=accuracy, \n",
    "                      wd=wd,\n",
    "                      moms=moms)\n",
    "                      \n",
    "    learner.to_multi_xla(device, rank=xm.get_ordinal(), sync_valid=SYNC_VALID)\n",
    "    # Scale learning rate to num cores\n",
    "    learning_rate = FLAGS3['learning_rate'] * xm.xrt_world_size()\n",
    "                               \n",
    "    epochs = FLAGS3['num_epochs']\n",
    "    xm.master_print('start running fit')\n",
    "    learner.unfreeze()\n",
    "    cbs = [PrintDevicesCallback()]\n",
    "    learner.fit_one_cycle(epochs, lr_max=slice(learning_rate/10), cbs=cbs)\n",
    "    xm.rendezvous('end_train_cifar_model')\n",
    "    learner.save('cifar-stage-1', rendezvous=False)\n",
    "    xm.mark_step()  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# Start training processes\n",
    "def _mp_fn4(rank, flags):\n",
    "    global FLAGS3\n",
    "    FLAGS3 = flags\n",
    "    train_cifar_model(rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xla 1 start train cifar model\n",
      "xla 0 start train cifar model\n",
      "xla 2 start train cifar model\n",
      "xla 3 start train cifar model\n",
      "xla 7 start train cifar model\n",
      "xla 5 start train cifar model\n",
      "xla 6 start train cifar model\n",
      "xla 4 start train cifar model\n",
      "train: True xla 1: dl.type: <class 'torch.utils.data.dataloader.DataLoader'> dl.device <bound method DataLoader.device of <torch.utils.data.dataloader.DataLoader object at 0x7f8ed200a550>> model.device: xla:0\n",
      "train: True xla 2: dl.type: <class 'torch.utils.data.dataloader.DataLoader'> dl.device <bound method DataLoader.device of <torch.utils.data.dataloader.DataLoader object at 0x7f8ed200a550>> model.device: xla:0\n",
      "train: True xla 3: dl.type: <class 'torch.utils.data.dataloader.DataLoader'> dl.device <bound method DataLoader.device of <torch.utils.data.dataloader.DataLoader object at 0x7f8ed200a550>> model.device: xla:0\n",
      "build learner\n",
      "start running fit\n",
      "start fit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.841237</td>\n",
       "      <td>1.288982</td>\n",
       "      <td>0.562738</td>\n",
       "      <td>01:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.276613</td>\n",
       "      <td>0.989299</td>\n",
       "      <td>0.660244</td>\n",
       "      <td>01:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.007693</td>\n",
       "      <td>0.830739</td>\n",
       "      <td>0.713980</td>\n",
       "      <td>01:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.858846</td>\n",
       "      <td>0.755246</td>\n",
       "      <td>0.740147</td>\n",
       "      <td>01:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.775464</td>\n",
       "      <td>0.729593</td>\n",
       "      <td>0.749387</td>\n",
       "      <td>01:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: True xla 0: dl.type: <class 'torch.utils.data.dataloader.DataLoader'> dl.device <bound method DataLoader.device of <torch.utils.data.dataloader.DataLoader object at 0x7f8ed200a550>> model.device: xla:1\n",
      "train: True xla 7: dl.type: <class 'torch.utils.data.dataloader.DataLoader'> dl.device <bound method DataLoader.device of <torch.utils.data.dataloader.DataLoader object at 0x7f8ed200a550>> model.device: xla:0\n",
      "train: True xla 6: dl.type: <class 'torch.utils.data.dataloader.DataLoader'> dl.device <bound method DataLoader.device of <torch.utils.data.dataloader.DataLoader object at 0x7f8ed200a550>> model.device: xla:0\n",
      "train: True xla 5: dl.type: <class 'torch.utils.data.dataloader.DataLoader'> dl.device <bound method DataLoader.device of <torch.utils.data.dataloader.DataLoader object at 0x7f8ed200a550>> model.device: xla:0\n",
      "train: True xla 4: dl.type: <class 'torch.utils.data.dataloader.DataLoader'> dl.device <bound method DataLoader.device of <torch.utils.data.dataloader.DataLoader object at 0x7f8ed200a550>> model.device: xla:0\n",
      "train: False xla 3: dl.type: <class 'torch.utils.data.dataloader.DataLoader'> dl.device <bound method DataLoader.device of <torch.utils.data.dataloader.DataLoader object at 0x7f8ed200a510>> model.device: xla:0\n",
      "train: False xla 1: dl.type: <class 'torch.utils.data.dataloader.DataLoader'> dl.device <bound method DataLoader.device of <torch.utils.data.dataloader.DataLoader object at 0x7f8ed200a510>> model.device: xla:0\n",
      "train: False xla 4: dl.type: <class 'torch.utils.data.dataloader.DataLoader'> dl.device <bound method DataLoader.device of <torch.utils.data.dataloader.DataLoader object at 0x7f8ed200a510>> model.device: xla:0\n",
      "train: False xla 2: dl.type: <class 'torch.utils.data.dataloader.DataLoader'> dl.device <bound method DataLoader.device of <torch.utils.data.dataloader.DataLoader object at 0x7f8ed200a510>> model.device: xla:0\n",
      "train: False xla 5: dl.type: <class 'torch.utils.data.dataloader.DataLoader'> dl.device <bound method DataLoader.device of <torch.utils.data.dataloader.DataLoader object at 0x7f8ed200a510>> model.device: xla:0\n",
      "train: False xla 6: dl.type: <class 'torch.utils.data.dataloader.DataLoader'> dl.device <bound method DataLoader.device of <torch.utils.data.dataloader.DataLoader object at 0x7f8ed200a510>> model.device: xla:0\n",
      "train: False xla 0: dl.type: <class 'torch.utils.data.dataloader.DataLoader'> dl.device <bound method DataLoader.device of <torch.utils.data.dataloader.DataLoader object at 0x7f8ed200a510>> model.device: xla:1\n",
      "train: False xla 7: dl.type: <class 'torch.utils.data.dataloader.DataLoader'> dl.device <bound method DataLoader.device of <torch.utils.data.dataloader.DataLoader object at 0x7f8ed200a510>> model.device: xla:0\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "# %%time\n",
    "xmp.spawn(_mp_fn4, args=(FLAGS3,), nprocs=FLAGS3['num_cores'],\n",
    "        start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.callback.core import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "class NumIterCancelCallback(Callback):\n",
    "    order = 20\n",
    "    def __init__(self, num_iters=0, on_train=True, on_valid=True):\n",
    "        store_attr()\n",
    "\n",
    "    def before_fit(self):\n",
    "\n",
    "        if not getattr(self.learn,'inner_xla',False):\n",
    "            return # skip if not spawned\n",
    "\n",
    "        self.my_iter = 0\n",
    "\n",
    "    def after_batch(self):\n",
    "\n",
    "        if not getattr(self.learn,'inner_xla',False):\n",
    "            return # skip if not spawned\n",
    "\n",
    "        if self.learn.training and not self.on_train:\n",
    "            return\n",
    "    \n",
    "        if not self.learn.training and not self.on_valid:\n",
    "            return\n",
    "\n",
    "        if self.num_iters == 0:\n",
    "            return\n",
    "\n",
    "        self.my_iter += 1\n",
    "        if self.my_iter > self.num_iters:\n",
    "            self.synced_cancel.trigger_cancel_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch_xla.distributed.xla_multiprocessing as xmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# path = untar_data(URLs.MNIST_TINY)\n",
    "path = untar_data(URLs.MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "data = DataBlock(\n",
    "    blocks=(ImageBlock,CategoryBlock),\n",
    "    get_items=get_image_files,\n",
    "    get_y=parent_label,\n",
    "    # splitter=GrandparentSplitter(),\n",
    "    splitter=GrandparentSplitter(train_name='training', valid_name='testing'),\n",
    "    item_tfms=Resize(28),\n",
    "    batch_tfms=[Normalize.from_stats(*imagenet_stats)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# MDLS = data.dataloaders(path, bs=8)\n",
    "MDLS = data.dataloaders(path, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "ARCH = resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "custom_model = create_cnn_model(ARCH, n_out=MDLS.c, concat_pool=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "WRAPPED_MODEL = xmp.MpModelWrapper(custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(rank):\n",
    "#     print(f'xla {rank}: start train')\n",
    "#     xm.rendezvous('start_train_model')\n",
    "#     world_size = xm.xrt_world_size()\n",
    "#     device = xm.xla_device()\n",
    "#     dls = build_distributed_dataloaders(MDLS, rank, world_size, sync_valid=True)\n",
    "#     model = WRAPPED_MODEL.to(device)\n",
    "#     learner = Learner(dls, model, loss_func=nn.CrossEntropyLoss(), \n",
    "#                       opt_func=Adam,\n",
    "#                       metrics=accuracy)\n",
    "#     learner.to_multi_xla(device, rank, sync_valid=True)\n",
    "#     learner.fit(5, lr=2e-3)\n",
    "#     learner.save('stage-1', rendezvous=False)\n",
    "#     xm.rendezvous('end_train_model')\n",
    "#     print(f'xla {rank}: end train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# xmp.spawn(train_model, args=(), nprocs=8,\n",
    "#         start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "def train_partial_batches(rank):\n",
    "    print(f'xla {rank}: start train partial batches')\n",
    "    xm.rendezvous('start_train_partial_batches')\n",
    "    world_size = xm.xrt_world_size()\n",
    "    device = xm.xla_device()\n",
    "    dls = build_distributed_dataloaders(MDLS, rank, world_size, sync_valid=True)\n",
    "    model = WRAPPED_MODEL.to(device)\n",
    "    learner = Learner(dls, model, loss_func=nn.CrossEntropyLoss(), \n",
    "                      opt_func=Adam,\n",
    "                      metrics=accuracy)\n",
    "    learner.to_multi_xla(device, rank, sync_valid=True)\n",
    "    learner.add_cbs([SyncedCancelCallback()])\n",
    "    if xm.is_master_ordinal():\n",
    "        cbs = [NumIterCancelCallback(100, on_valid=False)]\n",
    "    else:\n",
    "        cbs = [NumIterCancelCallback()]\n",
    "    learner.fit(5, lr=2e-3, cbs=cbs)\n",
    "    learner.save('partial-stage-1', rendezvous=False)\n",
    "    xm.rendezvous('end_train_partial_batches')\n",
    "    print(f'xla {rank}: end train partial batches')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xla 0: start train partial batches\n",
      "xla 7: start train partial batches\n",
      "xla 1: start train partial batches\n",
      "xla 2: start train partial batches\n",
      "xla 4: start train partial batches\n",
      "xla 6: start train partial batches\n",
      "xla 3: start train partial batches\n",
      "xla 5: start train partial batches\n",
      "start fit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xla 2: end train partial batches\n",
      "xla 0: end train partial batches\n",
      "xla 4: end train partial batches\n",
      "xla 1: end train partial batches\n",
      "xla 3: end train partial batches\n",
      "xla 7: end train partial batches\n",
      "xla 6: end train partial batches\n",
      "xla 5: end train partial batches\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "# %%time\n",
    "xmp.spawn(train_partial_batches, args=(), nprocs=8,\n",
    "        start_method='fork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/fastai_xla_extensions\n",
      "Converted 00_core.ipynb.\n",
      "Converted 01_utils.ipynb.\n",
      "Converted 02_cifar_loader.ipynb.\n",
      "Converted 02b_misc_utils.ipynb.\n",
      "Converted 03_multi_core.base.ipynb.\n",
      "Converted 03a_multi_core.torch_compat.ipynb.\n",
      "Converted 03b_multi_core.learner.ipynb.\n",
      "Converted 03c_multi_core.callback.ipynb.\n",
      "Converted 03d_multi_core.lr_find.ipynb.\n",
      "Converted 99_dev_setup.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%cd /content/fastai_xla_extensions\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/fastai_xla_extensions\n",
      "converting: /content/drive/My Drive/fastai_xla_extensions/nbs/03_multi_core.base.ipynb\n"
     ]
    }
   ],
   "source": [
    "%cd /content/fastai_xla_extensions\n",
    "from nbdev.export2html import notebook2html\n",
    "notebook2html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
