{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#colab\n",
    "# attach gdrive holding repo\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp multi_core.callback    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Core Callback XLA Extensions\n",
    "\n",
    "> Patches to Recorder and ParamScheduler Callbacks\n",
    "to support Multi Core XLA Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifications to existing callback `Recorder`, `ParamScheduler` and `LRFinder` are needed in order to store extra attributes to a temporary file after running the training as spawned processes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 194kB 5.0MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 4.7MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "# !pip install -Uqq git+https://github.com/fastai/fastai.git \n",
    "!pip install -Uqq fastai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 51kB 3.0MB/s \n",
      "\u001b[K     |████████████████████████████████| 51kB 4.1MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -qqq nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for fastai-xla-extensions (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -Uqq git+https://github.com/butchland/fastai_xla_extensions.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for my-timesaver-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -Uqq git+https://github.com/butchland/my_timesaver_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating fastai...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!curl -s https://course19.fast.ai/setup/colab | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#colab\n",
    "# !pip install -Uqq cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  5116  100  5116    0     0   108k      0 --:--:-- --:--:-- --:--:--  108k\n",
      "Updating... This may take around 2 minutes.\n",
      "Updating TPU runtime to pytorch-nightly ...\n",
      "Collecting cloud-tpu-client\n",
      "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
      "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client) (4.1.3)\n",
      "Collecting google-api-python-client==1.8.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 3.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client) (4.7.1)\n",
      "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client) (1.15.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client) (0.2.8)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client) (0.17.4)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.4)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.27.0)\n",
      "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (53.0.0)\n",
      "Uninstalling torch-1.7.0+cu101:\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.52.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2018.9)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.12.4)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n",
      "Installing collected packages: google-api-python-client, cloud-tpu-client\n",
      "  Found existing installation: google-api-python-client 1.7.12\n",
      "    Uninstalling google-api-python-client-1.7.12:\n",
      "      Successfully uninstalled google-api-python-client-1.7.12\n",
      "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n",
      "Done updating TPU runtime\n",
      "  Successfully uninstalled torch-1.7.0+cu101\n",
      "Uninstalling torchvision-0.8.1+cu101:\n",
      "  Successfully uninstalled torchvision-0.8.1+cu101\n",
      "Copying gs://tpu-pytorch/wheels/torch-nightly-cp37-cp37m-linux_x86_64.whl...\n",
      "- [1 files][122.6 MiB/122.6 MiB]                                                \n",
      "Operation completed over 1 objects/122.6 MiB.                                    \n",
      "Copying gs://tpu-pytorch/wheels/torch_xla-nightly-cp37-cp37m-linux_x86_64.whl...\n",
      "\\ [1 files][132.5 MiB/132.5 MiB]                                                \n",
      "Operation completed over 1 objects/132.5 MiB.                                    \n",
      "Copying gs://tpu-pytorch/wheels/torchvision-nightly-cp37-cp37m-linux_x86_64.whl...\n",
      "/ [1 files][  4.5 MiB/  4.5 MiB]                                                \n",
      "Operation completed over 1 objects/4.5 MiB.                                      \n",
      "Processing ./torch-nightly-cp37-cp37m-linux_x86_64.whl\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==nightly) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==nightly) (1.19.5)\n",
      "\u001b[31mERROR: fastai 2.2.7 requires torchvision<0.9,>=0.8, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: fastai 2.2.7 has requirement torch<1.8,>=1.7.0, but you'll have torch 1.9.0a0+64847c7 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.9.0a0+64847c7\n",
      "Processing ./torch_xla-nightly-cp37-cp37m-linux_x86_64.whl\n",
      "Installing collected packages: torch-xla\n",
      "Successfully installed torch-xla-1.9+7671584\n",
      "Processing ./torchvision-nightly-cp37-cp37m-linux_x86_64.whl\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchvision==nightly) (1.9.0a0+64847c7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==nightly) (1.19.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==nightly) (7.0.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchvision==nightly) (3.7.4.3)\n",
      "\u001b[31mERROR: fastai 2.2.7 has requirement torch<1.8,>=1.7.0, but you'll have torch 1.9.0a0+64847c7 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: fastai 2.2.7 has requirement torchvision<0.9,>=0.8, but you'll have torchvision 0.9.0a0+b7f3c81 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.9.0a0+b7f3c81\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  libomp5\n",
      "0 upgraded, 1 newly installed, 0 to remove and 11 not upgraded.\n",
      "Need to get 234 kB of archives.\n",
      "After this operation, 774 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
      "Fetched 234 kB in 1s (338 kB/s)\n",
      "Selecting previously unselected package libomp5:amd64.\n",
      "(Reading database ... 149414 files and directories currently installed.)\n",
      "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
      "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
      "Setting up libomp5:amd64 (5.0.1-1) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
      "\n"
     ]
    }
   ],
   "source": [
    "VERSION = \"nightly\"  #@param [\"1.5\", \"1.7\" , \"20200325\", \"nightly\"]\n",
    "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "!python pytorch-xla-env-setup.py --version $VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#colab\n",
    "# %cd /content\n",
    "# !ln -s /content/drive/MyDrive/fastai_xla_extensions  fastai_xla_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==1.9.0a0+64847c7\n",
      "torch-xla==1.9+7671584\n",
      "torchsummary==1.5.1\n",
      "torchtext==0.3.1\n",
      "torchvision==0.9.0a0+b7f3c81\n",
      "fastai==2.2.7\n",
      "fastai-xla-extensions==0.0.8\n",
      "fastcore==1.3.19\n",
      "fastdtw==0.3.4\n",
      "fastprogress==1.0.0\n",
      "fastrelease==0.1.11\n",
      "fastrlock==0.5\n",
      "my-timesaver-utils==0.0.2\n",
      "nbdev==1.1.13\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!pip freeze | grep torch\n",
    "!pip freeze | grep fast\n",
    "!pip freeze | grep timesaver\n",
    "!pip freeze | grep nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# start of kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#colab\n",
    "# %cd /content/fastai_xla_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastai_xla_extensions.utils import xla_imported\n",
    "from fastai_xla_extensions.misc_utils import *\n",
    "from fastai_xla_extensions.multi_core.base import *\n",
    "from fastai_xla_extensions.multi_core.learner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#colab\n",
    "# %cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "try:\n",
    "    import torch_xla\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "if xla_imported():\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#local\n",
    "# fake out torch_xla modules if not running on xla supported envs\n",
    "if not xla_imported():\n",
    "    # replace torch xla modules with fake equivalents\n",
    "    from types import SimpleNamespace\n",
    "    torch_xla = SimpleNamespace (\n",
    "    )\n",
    "    from typing import Union,BinaryIO\n",
    "    import os\n",
    "    import pickle\n",
    "    import torch.cuda\n",
    "\n",
    "    def fake_opt_step(opt,barrier=False):\n",
    "        opt.step()\n",
    "        \n",
    "    def fake_device(n=None, devkind=None):\n",
    "        gpu_available = torch.cuda.is_available()\n",
    "        if gpu_available:\n",
    "            return torch.device(torch.cuda.current_device()) \n",
    "        return torch.device('cpu')\n",
    "\n",
    "    def fake_save(obj, f: Union[str, os.PathLike, BinaryIO], \n",
    "                master_only=True, global_master=False): \n",
    "        return torch.save(obj,f,pickle_module=pickle, \n",
    "                        pickle_protocol=2, \n",
    "                        _use_new_zipfile_serialization=True)\n",
    "    def fake_rate():\n",
    "        return 230.20\n",
    "\n",
    "    def fake_global_rate():\n",
    "        return 830.10\n",
    "\n",
    "    def fake_add(*args,**kwargs):\n",
    "        pass\n",
    "\n",
    "    def fake_RateTracker():\n",
    "        return SimpleNamespace(\n",
    "            rate = fake_rate,\n",
    "            global_rate = fake_global_rate,\n",
    "            add = fake_add\n",
    "        )\n",
    "    def fake_xrt_world_size():\n",
    "        return 1\n",
    "    def fake_get_ordinal():\n",
    "        return 0\n",
    "    xm = SimpleNamespace(\n",
    "        optimizer_step = fake_opt_step,\n",
    "        xla_device = fake_device,\n",
    "        save = fake_save,\n",
    "        RateTracker = fake_RateTracker,\n",
    "        master_print = print,\n",
    "        xrt_world_size = fake_xrt_world_size,\n",
    "        get_ordinal = fake_get_ordinal\n",
    "    )\n",
    "\n",
    "    def fake_metrics_report():\n",
    "        return \"Fake Metrics Report \\n\\n\\n\\n\"\n",
    "    met = SimpleNamespace (\n",
    "        metrics_report = fake_metrics_report\n",
    "    )\n",
    "\n",
    "    class FakeParallelLoader:\n",
    "        def __init__(self, loader, *args):\n",
    "            self.loader = loader\n",
    "        def per_device_loader(self,device):\n",
    "            return self.loader\n",
    "        \n",
    "    pl = SimpleNamespace(\n",
    "        ParallelLoader = FakeParallelLoader\n",
    "    )\n",
    "\n",
    "    def fake_MpModelWrapper(o):\n",
    "        return o\n",
    "\n",
    "    def fake_run(f,*args, **kwargs):\n",
    "            return f(*args,**kwargs)\n",
    "        \n",
    "    def fake_MpSerialExecutor():\n",
    "        return SimpleNamespace(\n",
    "            run = fake_run\n",
    "        )\n",
    "    def fake_spawn(f, args=None, nprocs=0, start_method=None):\n",
    "        return f(0,*args)\n",
    "\n",
    "    xmp = SimpleNamespace (\n",
    "        MpModelWrapper = fake_MpModelWrapper,\n",
    "        MpSerialExecutor = fake_MpSerialExecutor,\n",
    "        spawn = fake_spawn\n",
    "    )\n",
    "\n",
    "    xu = SimpleNamespace (\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastai.vision.all import *\n",
    "# from fastai_xla_extensions.all import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "def maybe_item(o):\n",
    "    '''extract scalar values from a tensor, lists and dicts of tensors \n",
    "    (and pulling it out of gpu/tpu into cpu) else if not tensor just \n",
    "    use orig value'''\n",
    "    if isinstance(o,torch.Tensor): return o.item()\n",
    "    if is_listy(o):\n",
    "        kls = o.__class__\n",
    "        k = [maybe_item(i) for i in o]\n",
    "        return kls(k)\n",
    "    if isinstance(o,dict):\n",
    "        return {k:maybe_item(v) for k,v in o.items()}\n",
    "    # maybe scalar or object\n",
    "    return o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastcore.test import *\n",
    "# t1 = torch.tensor(5.)\n",
    "# test_eq(maybe_item(t1), 5.)\n",
    "# test_eq(maybe_item(float(5)),5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a tensor, `maybe_item` converts it to a scalar. If given is not a tensor (e.g. already a scalar), it just returns the scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastcore.test import *\n",
    "# tl1 = [tensor(2.)] * 5\n",
    "# test_eq(maybe_item(tl1), [2.] * 5)\n",
    "# dt1 = { 'd1': tensor(3.),\n",
    "#         'd2': [tensor(1.)] * 3}\n",
    "# df1 = { 'd1': 3.,\n",
    "#         'd2': [1.] * 3}\n",
    "# test_eq(maybe_item(dt1), df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`maybe_item` should also work for lists of tensors and dicts of tensors\n",
    "and/or list of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def get_extra_attrs(self:Recorder):\n",
    "    'Extract state attrs of Recorder into a dict (suitable for pickling)'\n",
    "    # state_attrs = lrs','iters','losses','values'\n",
    "    d = {}\n",
    "    for attr in self._stateattrs:\n",
    "        if hasattr(self,attr):\n",
    "            value = getattr(self,attr)\n",
    "            d[attr] = maybe_item(value)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "# from fastai.test_utils import *\n",
    "# learner = synth_learner()\n",
    "# learner.fit(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# setup checks\n",
    "# assert hasattr(learner,'recorder')\n",
    "# assert len(learner.recorder.lrs)  == 5 * 10\n",
    "# assert len(learner.recorder.losses) == 5 * 10\n",
    "# assert len(learner.recorder.iters) == 5\n",
    "# assert len(learner.recorder.values) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra_attrs = learner.recorder.get_extra_attrs()\n",
    "# test_eq(extra_attrs['lrs'], learner.recorder.lrs)\n",
    "# test_eq(extra_attrs['losses'], learner.recorder.losses)\n",
    "# test_eq(extra_attrs['iters'], learner.recorder.iters)\n",
    "# test_eq(extra_attrs['values'], learner.recorder.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Recorder.get_extra_attrs` should copy the state attrs (`lrs`,`losses`,`iters` and `values`) into\n",
    "a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pickle\n",
    "@patch\n",
    "def dump_attrs(self:Recorder, fn='_rec_attr.pkl'):\n",
    "    'dump state attrs to a file'\n",
    "    d = self.get_extra_attrs()\n",
    "    with open(fn,'wb') as f:\n",
    "        pickle.dump(d,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pickle\n",
    "@patch\n",
    "def reload_attrs(self:Recorder, fn='_rec_attr.pkl'):\n",
    "    'reload attrs from file `fn`'\n",
    "    if isinstance(fn,str):\n",
    "        fn = Path(fn)\n",
    "    if not fn.is_file():\n",
    "        return\n",
    "    with open(fn,'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "        for k,v in d.items():\n",
    "            setattr(self,k,v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_fn = 'test_rec_attrs.pkl'\n",
    "# !rm -f {test_fn}\n",
    "# learner.recorder.dump_attrs(fn=test_fn)\n",
    "# f = Path(test_fn)\n",
    "# assert f.is_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delattr(learner.recorder,'lrs')\n",
    "# delattr(learner.recorder,'losses')\n",
    "# delattr(learner.recorder,'iters')\n",
    "# delattr(learner.recorder,'values')\n",
    "# assert not hasattr(learner.recorder,'lrs')\n",
    "# assert not hasattr(learner.recorder,'losses')\n",
    "# assert not hasattr(learner.recorder,'iters')\n",
    "# assert not hasattr(learner.recorder,'values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# learner.recorder.reload_attrs(fn=test_fn)\n",
    "# assert hasattr(learner.recorder,'lrs')\n",
    "# assert hasattr(learner.recorder,'losses')\n",
    "# assert hasattr(learner.recorder,'iters')\n",
    "# assert hasattr(learner.recorder,'values')\n",
    "# !rm -f {test_fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def after_fit(self: Recorder):\n",
    "    'after fit dump extra attrs to file'\n",
    "    if xm.is_master_ordinal():\n",
    "        self.dump_attrs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def dump_hps(self:ParamScheduler, fn='_paramsched_hps.pkl'):\n",
    "    if not hasattr(self, 'hps'): \n",
    "        return\n",
    "\n",
    "    if isinstance(fn,str):\n",
    "        fn = Path(fn)\n",
    "\n",
    "    d = maybe_item(self.hps)\n",
    "    with open(fn,'wb') as f:\n",
    "        pickle.dump(d,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def reload_hps(self:Recorder, fn='_paramsched_hps.pkl'):\n",
    "    'Load hyperparameters saved by ParamScheduler to recorder'\n",
    "    if isinstance(fn,str):\n",
    "        fn = Path(fn)\n",
    "    if not fn.is_file():\n",
    "        return\n",
    "    with open(fn,'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "        setattr(self,'hps',d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def after_fit(self:ParamScheduler):\n",
    "    \"save hps to file\"\n",
    "    if not hasattr(self,'hps'):\n",
    "        return\n",
    "\n",
    "    if hasattr(self.learn, 'recorder'): \n",
    "        self.recorder.hps = self.hps\n",
    "\n",
    "    if xm.is_master_ordinal():\n",
    "        self.dump_hps()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"ParamScheduler.after_fit\" class=\"doc_header\"><code>ParamScheduler.after_fit</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>ParamScheduler.after_fit</code>()\n\nsave hps to file",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "show_doc(ParamScheduler.after_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "# param_fn = '_paramsched_hps.pkl'\n",
    "# !rm -f {param_fn}\n",
    "# learner.fit_one_cycle(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_f = Path(param_fn)\n",
    "# assert param_f.is_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delattr(learner.recorder,'hps')\n",
    "# assert not hasattr(learner.recorder,'hps')\n",
    "# learner.recorder.reload_hps()\n",
    "# assert hasattr(learner.recorder,'hps')\n",
    "# !rm -f {param_fn}\n",
    "# !rm -f _rec_attr.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ParamScheduler (`fit_one_cycle` uses `ParamScheduler`) which means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipValidationCallback(Callback):\n",
    "    order,run_valid = -9, False\n",
    "    # raise CancelValidException before XLATrainingCallback.before_validate\n",
    "    # to prevent call to wrap_parallel_loader on before_validate\n",
    "    def before_validate(self): \n",
    "        raise CancelValidException()\n",
    "\n",
    "    def after_cancel_validate(self):\n",
    "        xm.mark_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLALRFinder(ParamScheduler):\n",
    "    \"Training with exponentially growing learning rate\"\n",
    "    def __init__(self, start_lr=1e-7, end_lr=10, num_it=100, stop_div=True):\n",
    "        if is_listy(start_lr):\n",
    "            self.scheds = {'lr': [SchedExp(s, e) for (s,e) in zip(start_lr,end_lr)]}\n",
    "        else: self.scheds = {'lr': SchedExp(start_lr, end_lr)}\n",
    "        self.num_it,self.stop_div = num_it,stop_div\n",
    "        self.skip_batch = False\n",
    "        self.num_losses = 0\n",
    "\n",
    "    def before_fit(self):\n",
    "        super().before_fit()\n",
    "        # no need to save orig weights \n",
    "        # since learner instances are transient on spawned procs\n",
    "        # self.learn.save('_tmp')\n",
    "        self.best_loss = float('inf')\n",
    "        self.skip_batch = False\n",
    "        self.num_losses = 0\n",
    "        # dont report losses while running lrfind (override sync_recorder)\n",
    "        # run after sync_recorder.before_fit (sync_recorder.order == 55)\n",
    "        # while param scheduler order == 60\n",
    "        if xm.is_master_ordinal() and hasattr(self.learn, 'sync_recorder'):\n",
    "            self.learn.logger = noop\n",
    "            self.learn.sync_recorder._sync_stats_log = noop\n",
    "\n",
    "\n",
    "    def before_batch(self):\n",
    "        if self.skip_batch:\n",
    "            return\n",
    "        self._update_val(self.train_iter/self.num_it)\n",
    "\n",
    "    def after_batch(self):\n",
    "        if self.skip_batch:\n",
    "            return\n",
    "        super().after_batch()\n",
    "        smooth_loss = self.smooth_loss.item() # move xla tensor to cpu\n",
    "        self.num_loss = len(self.recorder.losses)\n",
    "        if smooth_loss < self.best_loss:\n",
    "            self.best_loss = smooth_loss\n",
    "\n",
    "        # handle continuation of batch iteration until all batches exhausted\n",
    "        if smooth_loss > 4*self.best_loss and self.stop_div:\n",
    "            self.skip_batch = True\n",
    "            return\n",
    "            \n",
    "        if self.train_iter >= self.num_it:\n",
    "            self.skip_batch = True\n",
    "            return\n",
    "\n",
    "    def after_fit(self):\n",
    "        # no need to load old weights since these will be transient\n",
    "        # self.learn.opt.zero_grad() #Need to zero the gradients of the model before detaching the optimizer for future fits\n",
    "        # tmp_f = self.path/self.model_dir/'_tmp.pth'\n",
    "        # if tmp_f.exists():\n",
    "        #     self.learn.load('_tmp', with_opt=True)\n",
    "        #     os.remove(tmp_f)\n",
    "        if not xm.is_master_ordinal(): return\n",
    "\n",
    "        if not self.skip_batch: # completed w/o copying lrs and losses from recorder to plot_data\n",
    "            self.num_loss = len(self.recorder.losses)\n",
    "\n",
    "        self.recorder.losses = self.recorder.losses[: self.num_loss]\n",
    "        self.recorder.lrs = self.recorder.lrs[: self.num_loss]\n",
    "        num_iters = len(self.recorder.iters)\n",
    "        for i, iter in enumerate(self.recorder.iters):\n",
    "            if iter >= self.num_it:\n",
    "                num_iters = i + 1\n",
    "                break\n",
    "        self.recorder.iters = self.recorder.iters[:num_iters]\n",
    "        self.recorder.values = self.recorder.values[:num_iters]\n",
    "        self.recorder.dump_attrs() # rewrite updated attrs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def xla_run_lr_find(rank, learner_args, add_args, lr_find_args, ctrl_args):\n",
    "    'run xla lr_find on spawned processes'\n",
    "    xm.rendezvous('start_run_lrfind')\n",
    "    # print(f'xla {rank} : start run lrfind')\n",
    "    sync_valid = True\n",
    "    learner = make_xla_child_learner(rank, sync_valid, learner_args, add_args, ctrl_args)\n",
    "    # start_lr=1e-7 \n",
    "    # end_lr=10 \n",
    "    # num_it=100\n",
    "    # stop_div=True\n",
    "    # show_plot= False\n",
    "    # suggestions= True\n",
    "    num_it = lr_find_args['num_it']\n",
    "    n_epoch = num_it//len(learner.dls.train) + 1\n",
    "    # learner.opt = None\n",
    "    # learner.create_opt()\n",
    "    lr_find_cb = XLALRFinder(**lr_find_args)\n",
    "\n",
    "    skip_valid_cb = SkipValidationCallback()\n",
    "    \n",
    "    with learner.no_logging(): \n",
    "        learner.fit(n_epoch, cbs=[lr_find_cb, skip_valid_cb])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def get_suggested_lrs(self:Learner, num_it):\n",
    "    lrs,losses = tensor(self.recorder.lrs[num_it//10:-5]),tensor(self.recorder.losses[num_it//10:-5])\n",
    "    if len(losses) == 0: return\n",
    "    lr_min = lrs[losses.argmin()].item()\n",
    "    grads = (losses[1:]-losses[:-1]) / (lrs[1:].log()-lrs[:-1].log())\n",
    "    lr_steep = lrs[grads.argmin()].item()\n",
    "    return SuggestedLRs(lr_min/10.,lr_steep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "@delegates(Learner.lr_find, but='num_cores,start_method')\n",
    "def xla_lr_find(self:Learner, num_cores=8, start_method='fork', **kwargs):\n",
    "    # default params for lr_find\n",
    "    lr_find_args = {\n",
    "        'start_lr': 1e-7,\n",
    "        'end_lr': 10.,\n",
    "        'num_it': 100,\n",
    "        'stop_div': True\n",
    "    }\n",
    "    show_plot = True\n",
    "    suggestions = True\n",
    "\n",
    "    # remove show_plot and suggestions param\n",
    "    if 'show_plot' in kwargs:\n",
    "        show_plot = kwargs.pop('show_plot')\n",
    "    if 'suggestions' in kwargs:\n",
    "        suggestions = kwargs.pop('suggestions')\n",
    "    # override default with kwargs\n",
    "    lr_find_args = {**lr_find_args, **kwargs}    \n",
    "\n",
    "    ctrl_args = self.pre_xla_fit()\n",
    "    learner_args, add_args = self.pack_learner_args()\n",
    "    xmp.spawn(xla_run_lr_find,\n",
    "              args=(learner_args, add_args, lr_find_args, ctrl_args),\n",
    "              nprocs=num_cores,\n",
    "              start_method=start_method)\n",
    "    \n",
    "    self.recorder.reload_attrs()\n",
    "    self.recorder.reload_hps()\n",
    "\n",
    "    if show_plot:\n",
    "        self.recorder.plot_lr_find()\n",
    "    if suggestions:\n",
    "        return self.get_suggested_lrs(lr_find_args['num_it'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# path = untar_data(URLs.MNIST_TINY)\n",
    "path = untar_data(URLs.MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),\n",
    "    get_items=get_image_files,\n",
    "    get_y=parent_label,\n",
    "    splitter=GrandparentSplitter(train_name='training', valid_name='testing'),\n",
    "    # splitter=GrandparentSplitter(),\n",
    "    item_tfms=Resize(28),\n",
    "    batch_tfms=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = data.dataloaders(path, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a39c3608834509a8f5e603864d7fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "learner = cnn_learner(dls, resnet18, metrics=accuracy, concat_pool=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      75.00% [3/4 00:51<00:17]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='117' class='' max='118' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      99.15% [117/118 00:16<00:00 2.1327]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in device=TPU:0: Resource exhausted: From /job:tpu_worker/replica:0/task:0:\n",
      "2 root error(s) found.\n",
      "  (0) Resource exhausted: Failed to allocate request for 4.0KiB (4096B) on device ordinal 0\n",
      "\t [[{{node XRTExecute}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted: Failed to allocate request for 4.0KiB (4096B) on device ordinal 0\n",
      "\t [[{{node XRTExecute}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[XRTExecute_G15]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 329, in _mp_start_fn\n",
      "    _start_fn(index, pf_cfg, fn, args)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n",
      "    fn(gindex, *args)\n",
      "  File \"<ipython-input-45-e3f56b236926>\", line 24, in xla_run_lr_find\n",
      "    learner.fit(n_epoch, cbs=[lr_find_cb, skip_valid_cb])\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fastai/learner.py\", line 211, in fit\n",
      "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fastai/learner.py\", line 162, in _with_events\n",
      "    self(f'after_{event_type}');  final()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fastai/learner.py\", line 141, in __call__\n",
      "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fastcore/foundation.py\", line 154, in map\n",
      "    def map(self, f, *args, gen=False, **kwargs): return self._new(map_ex(self, f, *args, gen=gen, **kwargs))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fastcore/basics.py\", line 666, in map_ex\n",
      "    return list(res)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fastcore/basics.py\", line 651, in __call__\n",
      "    return self.func(*fargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fastai/learner.py\", line 145, in _call_one\n",
      "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fastai/callback/core.py\", line 44, in __call__\n",
      "    if self.run and _run: res = getattr(self, event_name, noop)()\n",
      "  File \"<ipython-input-35-de720d2679a6>\", line 6, in after_fit\n",
      "    self.dump_attrs()\n",
      "  File \"<ipython-input-30-b6c5334c1ea2>\", line 6, in dump_attrs\n",
      "    d = self.get_extra_attrs()\n",
      "  File \"<ipython-input-26-c2a96416d51b>\", line 10, in get_extra_attrs\n",
      "    d[attr] = maybe_item(value)\n",
      "  File \"<ipython-input-23-1caf6a9efe3d>\", line 10, in maybe_item\n",
      "    k = [maybe_item(i) for i in o]\n",
      "  File \"<ipython-input-23-1caf6a9efe3d>\", line 10, in <listcomp>\n",
      "    k = [maybe_item(i) for i in o]\n",
      "RuntimeError: Resource exhausted: From /job:tpu_worker/replica:0/task:0:\n",
      "2 root error(s) found.\n",
      "  (0) Resource exhausted: Failed to allocate request for 4.0KiB (4096B) on device ordinal 0\n",
      "\t [[{{node XRTExecute}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted: Failed to allocate request for 4.0KiB (4096B) on device ordinal 0\n",
      "\t [[{{node XRTExecute}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[XRTExecute_G15]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-7b80e9db369c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %%time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# spawn_lrfind(learner)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_lr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_div\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-80f78bd530a0>\u001b[0m in \u001b[0;36mxla_lr_find\u001b[0;34m(self, num_cores, start_method, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m               \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_find_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m               \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_cores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m               start_method=start_method)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mdaemon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         start_method=start_method)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0merror_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0merror_pid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfailed_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                     \u001b[0mexit_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m                 )\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 0 terminated with exit code 17"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# spawn_lrfind(learner)\n",
    "learner.xla_lr_find(stop_div=True,end_lr=100, num_it=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.xla_fit_one_cycle(5, lr_max=slice(0.026))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
