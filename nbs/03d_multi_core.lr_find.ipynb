{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "# attach gdrive holding repo\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp multi_core.lr_find"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Core LR Find XLA Extensions\n",
    "\n",
    "> Classes to replace LRFinder and patches to Learner\n",
    "to support running lr_find using multi core TPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifications to existing callback `LRFinder` are needed in order to run `lr_find` using multiple TPU cores. An equivalent `xla_lr_find` method is patched to `Learner` so it can run on multiple TPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 194kB 6.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 6.7MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "# !pip install -Uqq git+https://github.com/fastai/fastai.git \n",
    "!pip install -Uqq fastai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 51kB 3.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 51kB 3.4MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -qqq nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#colab\n",
    "# !pip install -Uqq git+https://github.com/butchland/fastai_xla_extensions.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#colab\n",
    "# !pip install -Uqq git+https://github.com/butchland/my_timesaver_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating fastai...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!curl -s https://course19.fast.ai/setup/colab | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  5116  100  5116    0     0   108k      0 --:--:-- --:--:-- --:--:--  108k\n",
      "Updating... This may take around 2 minutes.\n",
      "Updating TPU runtime to pytorch-nightly ...\n",
      "Collecting cloud-tpu-client\n",
      "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
      "Collecting google-api-python-client==1.8.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 3.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client) (4.1.3)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.27.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.15.0)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.4)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.17.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client) (0.2.8)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client) (0.4.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client) (4.7.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.2.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (53.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.52.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.12.4)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2018.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n",
      "Uninstalling torch-1.7.0+cu101:\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2020.12.5)\n",
      "Installing collected packages: google-api-python-client, cloud-tpu-client\n",
      "  Found existing installation: google-api-python-client 1.7.12\n",
      "    Uninstalling google-api-python-client-1.7.12:\n",
      "      Successfully uninstalled google-api-python-client-1.7.12\n",
      "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n",
      "Done updating TPU runtime\n",
      "  Successfully uninstalled torch-1.7.0+cu101\n",
      "Uninstalling torchvision-0.8.1+cu101:\n",
      "  Successfully uninstalled torchvision-0.8.1+cu101\n",
      "Copying gs://tpu-pytorch/wheels/torch-nightly-cp37-cp37m-linux_x86_64.whl...\n",
      "\\ [1 files][122.6 MiB/122.6 MiB]                                                \n",
      "Operation completed over 1 objects/122.6 MiB.                                    \n",
      "Copying gs://tpu-pytorch/wheels/torch_xla-nightly-cp37-cp37m-linux_x86_64.whl...\n",
      "\\ [1 files][132.6 MiB/132.6 MiB]                                                \n",
      "Operation completed over 1 objects/132.6 MiB.                                    \n",
      "Copying gs://tpu-pytorch/wheels/torchvision-nightly-cp37-cp37m-linux_x86_64.whl...\n",
      "/ [1 files][  4.5 MiB/  4.5 MiB]                                                \n",
      "Operation completed over 1 objects/4.5 MiB.                                      \n",
      "Processing ./torch-nightly-cp37-cp37m-linux_x86_64.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==nightly) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==nightly) (3.7.4.3)\n",
      "\u001b[31mERROR: fastai 2.2.7 requires torchvision<0.9,>=0.8, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: fastai 2.2.7 has requirement torch<1.8,>=1.7.0, but you'll have torch 1.9.0a0+958d9a8 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.9.0a0+958d9a8\n",
      "Processing ./torch_xla-nightly-cp37-cp37m-linux_x86_64.whl\n",
      "Installing collected packages: torch-xla\n",
      "Successfully installed torch-xla-1.9+7671584\n",
      "Processing ./torchvision-nightly-cp37-cp37m-linux_x86_64.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==nightly) (1.19.5)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchvision==nightly) (1.9.0a0+958d9a8)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==nightly) (7.0.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchvision==nightly) (3.7.4.3)\n",
      "\u001b[31mERROR: fastai 2.2.7 has requirement torch<1.8,>=1.7.0, but you'll have torch 1.9.0a0+958d9a8 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: fastai 2.2.7 has requirement torchvision<0.9,>=0.8, but you'll have torchvision 0.9.0a0+fc33c46 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.9.0a0+fc33c46\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  libomp5\n",
      "0 upgraded, 1 newly installed, 0 to remove and 11 not upgraded.\n",
      "Need to get 234 kB of archives.\n",
      "After this operation, 774 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
      "Fetched 234 kB in 1s (329 kB/s)\n",
      "Selecting previously unselected package libomp5:amd64.\n",
      "(Reading database ... 149414 files and directories currently installed.)\n",
      "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
      "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
      "Setting up libomp5:amd64 (5.0.1-1) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "# !pip install -Uqq cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl\n",
    "VERSION = \"nightly\"  #@param [\"1.5\", \"1.7\" , \"20200325\", \"nightly\"]\n",
    "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "!python pytorch-xla-env-setup.py --version $VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "%cd /content\n",
    "!ln -s /content/drive/MyDrive/fastai_xla_extensions fastai_xla_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==1.9.0a0+958d9a8\n",
      "torch-xla==1.9+7671584\n",
      "torchsummary==1.5.1\n",
      "torchtext==0.3.1\n",
      "torchvision==0.9.0a0+fc33c46\n",
      "fastai==2.2.7\n",
      "fastcore==1.3.19\n",
      "fastdtw==0.3.4\n",
      "fastprogress==1.0.0\n",
      "fastrelease==0.1.11\n",
      "fastrlock==0.5\n",
      "nbdev==1.1.13\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!pip freeze | grep torch\n",
    "!pip freeze | grep fast\n",
    "!pip freeze | grep timesaver\n",
    "!pip freeze | grep nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# start of kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/fastai_xla_extensions\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "%cd /content/fastai_xla_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastai_xla_extensions.utils import xla_imported\n",
    "from fastai_xla_extensions.multi_core.base import *\n",
    "from fastai_xla_extensions.multi_core.callback import *\n",
    "from fastai_xla_extensions.multi_core.learner import *\n",
    "from fastai_xla_extensions.misc_utils import *\n",
    "from fastai_xla_extensions.core import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "try:\n",
    "    import torch_xla\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "if xla_imported():\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#local\n",
    "# fake out torch_xla modules if not running on xla supported envs\n",
    "if not xla_imported():\n",
    "    # replace torch xla modules with fake equivalents\n",
    "    from types import SimpleNamespace\n",
    "    torch_xla = SimpleNamespace (\n",
    "    )\n",
    "    from typing import Union,BinaryIO\n",
    "    import os\n",
    "    import pickle\n",
    "    import torch.cuda\n",
    "\n",
    "    def fake_opt_step(opt,barrier=False):\n",
    "        opt.step()\n",
    "        \n",
    "    def fake_device(n=None, devkind=None):\n",
    "        gpu_available = torch.cuda.is_available()\n",
    "        if gpu_available:\n",
    "            return torch.device(torch.cuda.current_device()) \n",
    "        return torch.device('cpu')\n",
    "\n",
    "    def fake_save(obj, f: Union[str, os.PathLike, BinaryIO], \n",
    "                master_only=True, global_master=False): \n",
    "        return torch.save(obj,f,pickle_module=pickle, \n",
    "                        pickle_protocol=2, \n",
    "                        _use_new_zipfile_serialization=True)\n",
    "    def fake_rate():\n",
    "        return 230.20\n",
    "\n",
    "    def fake_global_rate():\n",
    "        return 830.10\n",
    "\n",
    "    def fake_add(*args,**kwargs):\n",
    "        pass\n",
    "\n",
    "    def fake_RateTracker():\n",
    "        return SimpleNamespace(\n",
    "            rate = fake_rate,\n",
    "            global_rate = fake_global_rate,\n",
    "            add = fake_add\n",
    "        )\n",
    "    def fake_xrt_world_size():\n",
    "        return 1\n",
    "    def fake_get_ordinal():\n",
    "        return 0\n",
    "    xm = SimpleNamespace(\n",
    "        optimizer_step = fake_opt_step,\n",
    "        xla_device = fake_device,\n",
    "        save = fake_save,\n",
    "        RateTracker = fake_RateTracker,\n",
    "        master_print = print,\n",
    "        xrt_world_size = fake_xrt_world_size,\n",
    "        get_ordinal = fake_get_ordinal\n",
    "    )\n",
    "\n",
    "    def fake_metrics_report():\n",
    "        return \"Fake Metrics Report \\n\\n\\n\\n\"\n",
    "    met = SimpleNamespace (\n",
    "        metrics_report = fake_metrics_report\n",
    "    )\n",
    "\n",
    "    class FakeParallelLoader:\n",
    "        def __init__(self, loader, *args):\n",
    "            self.loader = loader\n",
    "        def per_device_loader(self,device):\n",
    "            return self.loader\n",
    "        \n",
    "    pl = SimpleNamespace(\n",
    "        ParallelLoader = FakeParallelLoader\n",
    "    )\n",
    "\n",
    "    def fake_MpModelWrapper(o):\n",
    "        return o\n",
    "\n",
    "    def fake_run(f,*args, **kwargs):\n",
    "            return f(*args,**kwargs)\n",
    "        \n",
    "    def fake_MpSerialExecutor():\n",
    "        return SimpleNamespace(\n",
    "            run = fake_run\n",
    "        )\n",
    "    def fake_spawn(f, args=None, nprocs=0, start_method=None):\n",
    "        return f(0,*args)\n",
    "\n",
    "    xmp = SimpleNamespace (\n",
    "        MpModelWrapper = fake_MpModelWrapper,\n",
    "        MpSerialExecutor = fake_MpSerialExecutor,\n",
    "        spawn = fake_spawn\n",
    "    )\n",
    "\n",
    "    xu = SimpleNamespace (\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.learner import CancelValidException\n",
    "class SkipValidationCallback(Callback):\n",
    "    order,run_valid = -9, False\n",
    "    # raise CancelValidException before XLATrainingCallback.before_validate\n",
    "    # to prevent call to wrap_parallel_loader on before_validate\n",
    "    def before_validate(self): \n",
    "        raise CancelValidException()\n",
    "\n",
    "    def after_cancel_validate(self):\n",
    "        xm.mark_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from fastai.callback.schedule import ParamScheduler, SchedExp\n",
    "from fastcore.xtras import is_listy\n",
    "from fastcore.imports import noop\n",
    "class XLALRFinder(ParamScheduler):\n",
    "    \"Training with exponentially growing learning rate\"\n",
    "    def __init__(self, start_lr=1e-7, end_lr=10, num_it=100, stop_div=True):\n",
    "        if is_listy(start_lr):\n",
    "            self.scheds = {'lr': [SchedExp(s, e) for (s,e) in zip(start_lr,end_lr)]}\n",
    "        else: self.scheds = {'lr': SchedExp(start_lr, end_lr)}\n",
    "        self.num_it,self.stop_div = num_it,stop_div\n",
    "        self.skip_batch = False\n",
    "        \n",
    "\n",
    "\n",
    "    def before_fit(self):\n",
    "        super().before_fit()\n",
    "        # no need to save orig weights \n",
    "        # since learner instances are transient on spawned procs\n",
    "        # self.learn.save('_tmp')\n",
    "        self.best_loss = float('inf')\n",
    "        self.skip_batch = False\n",
    "\n",
    "    def before_epoch(self):\n",
    "        # dont report losses while running lrfind (override sync_recorder)\n",
    "        if not xm.is_master_ordinal():\n",
    "            return\n",
    "        if hasattr(self.learn, 'sync_recorder'):\n",
    "            self.learn.logger = noop\n",
    "            self.learn.sync_recorder._sync_stats_log = noop\n",
    "\n",
    "    def before_batch(self):\n",
    "        if self.skip_batch:\n",
    "            return\n",
    "        self._update_val(self.train_iter/self.num_it)\n",
    "\n",
    "    def after_batch(self):\n",
    "        if self.skip_batch:\n",
    "            return\n",
    "        super().after_batch()\n",
    "        smooth_loss = self.smooth_loss.item() # move xla tensor to cpu\n",
    "        if smooth_loss < self.best_loss:\n",
    "            self.best_loss = smooth_loss\n",
    "\n",
    "        # handle continuation of batch iteration until all batches exhausted\n",
    "        if smooth_loss > 4*self.best_loss and self.stop_div:\n",
    "            # print(f'xla {xm.get_ordinal()}: stop stats collection due to loss')\n",
    "            self.skip_batch = True\n",
    "            self.copy_losses_and_lrs()\n",
    "            return\n",
    "            \n",
    "\n",
    "        if self.train_iter >= self.num_it:\n",
    "            # print(f'xla {xm.get_ordinal()}: stop stats collection due to num_iter')\n",
    "            # return and stop updating losses\n",
    "            self.skip_batch = True\n",
    "            self.copy_losses_and_lrs()\n",
    "            return\n",
    "\n",
    "    def copy_losses_and_lrs(self):\n",
    "        if xm.is_master_ordinal():     \n",
    "            losses = [loss.item() for loss in self.recorder.losses]\n",
    "            iters = self.recorder.iters[:]\n",
    "            values = self.recorder.values[:]\n",
    "        \n",
    "            self.plot_data = {'lrs': self.recorder.lrs[:],\n",
    "                              'losses': losses,\n",
    "                              'iters': iters,\n",
    "                              'values': values}\n",
    "            if hasattr(self,'hps'):\n",
    "                self.plot_data['hps']  = {**self.hps}\n",
    "\n",
    "    def after_fit(self):\n",
    "        super().after_fit()\n",
    "        # no need to load old weights since these will be transient\n",
    "        # self.learn.opt.zero_grad() #Need to zero the gradients of the model before detaching the optimizer for future fits\n",
    "        # tmp_f = self.path/self.model_dir/'_tmp.pth'\n",
    "        # if tmp_f.exists():\n",
    "        #     self.learn.load('_tmp', with_opt=True)\n",
    "        #     os.remove(tmp_f)\n",
    "        if not self.skip_batch:\n",
    "            self.copy_losses_and_lrs()\n",
    "        if xm.is_master_ordinal():\n",
    "            with open('_plt_loss.pkl','wb') as f:\n",
    "                pickle.dump(self.plot_data,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.learner import Learner\n",
    "from fastai.callback.schedule import SuggestedLRs\n",
    "from fastcore.basics import patch\n",
    "from fastai.torch_core import tensor\n",
    "@patch\n",
    "def get_suggested_lrs(self:Learner, num_it):\n",
    "    'compute Suggested LRs'\n",
    "    lrs,losses = tensor(self.recorder.lrs[num_it//10:-5]),tensor(self.recorder.losses[num_it//10:-5])\n",
    "    if len(losses) == 0: return\n",
    "    lr_min = lrs[losses.argmin()].item()\n",
    "    grads = (losses[1:]-losses[:-1]) / (lrs[1:].log()-lrs[:-1].log())\n",
    "    lr_steep = lrs[grads.argmin()].item()\n",
    "    return SuggestedLRs(lr_min/10.,lr_steep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"Learner.get_suggested_lrs\" class=\"doc_header\"><code>Learner.get_suggested_lrs</code><a href=\"__main__.py#L6\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Learner.get_suggested_lrs</code>(**`num_it`**)\n\ncompute Suggested LRs",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "show_doc(Learner.get_suggested_lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pickle\n",
    "from fastai.learner import Recorder\n",
    "from fastcore.basics import patch   \n",
    "@patch\n",
    "def reload_lr_find_attrs(self:Recorder, fn='_plt_loss.pkl'):\n",
    "    if isinstance(fn,str):\n",
    "        fn = Path(fn)\n",
    "\n",
    "    if not fn.is_file():\n",
    "        return\n",
    "       \n",
    "    with open(fn,'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "        self.lrs,self.losses = d['lrs'],d['losses']\n",
    "        self.values, self.iters = d['values'], d['iters']\n",
    "        if 'hps' in d:\n",
    "            self.hps = d['hps']\n",
    "    # delete file after\n",
    "    if fn.is_file():\n",
    "        fn.unlink()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"Recorder.reload_lr_find_attrs\" class=\"doc_header\"><code>Recorder.reload_lr_find_attrs</code><a href=\"__main__.py#L5\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Recorder.reload_lr_find_attrs</code>(**`fn`**=*`'_plt_loss.pkl'`*)\n\n",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "show_doc(Recorder.reload_lr_find_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def xla_run_lr_find(rank, learner_args, add_args, lr_find_args, ctrl_args):\n",
    "    xm.rendezvous('start_xla_run_lr_find')\n",
    "    # print(f'xla {rank} : start run lrfind')\n",
    "    sync_valid = True\n",
    "    learner = make_xla_child_learner(rank, sync_valid, learner_args, add_args, ctrl_args)\n",
    "\n",
    "    num_it = lr_find_args['num_it']\n",
    "    n_epoch = num_it//len(learner.dls.train) + 1\n",
    "    learner.opt = None\n",
    "    learner.create_opt()\n",
    "    cb = XLALRFinder(**lr_find_args) \n",
    " \n",
    "    skip_valid_cb = SkipValidationCallback()\n",
    "    \n",
    "    with learner.no_logging(): \n",
    "        learner.fit(n_epoch, cbs=[cb, skip_valid_cb])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from pathlib import Path\n",
    "from fastai.learner import Learner\n",
    "from fastcore.basics import patch\n",
    "from fastcore.meta import delegates\n",
    "\n",
    "@patch\n",
    "@delegates(Learner.lr_find)\n",
    "def xla_lr_find(self:Learner, num_cores=8, start_method='fork', **kwargs):\n",
    "    lr_find_args = {\n",
    "        'start_lr': 1e-7,\n",
    "        'end_lr': 10.,\n",
    "        'num_it': 100,\n",
    "        'stop_div': True\n",
    "    }\n",
    "    fn = Path('_plt_loss.pkl')\n",
    "    if fn.is_file():\n",
    "        fn.unlink()\n",
    "    # remove show_plot and suggestions param\n",
    "    show_plot = kwargs.pop('show_plot', True)\n",
    "    suggestions = kwargs.pop('suggestions',True)\n",
    "    # override default with kwargs\n",
    "    lr_find_args = {**lr_find_args, **kwargs}    \n",
    "\n",
    "    ctrl_args = self.pre_xla_fit()\n",
    "    learner_args, add_args = self.pack_learner_args()\n",
    "    xmp.spawn(xla_run_lr_find,\n",
    "              args=(learner_args, add_args, lr_find_args, ctrl_args),\n",
    "              nprocs=num_cores,\n",
    "              start_method=start_method)\n",
    "    self.post_xla_fit(ctrl_args)\n",
    "    self.recorder.reload_lr_find_attrs()\n",
    "    if show_plot:\n",
    "        # show_loss()\n",
    "        self.recorder.plot_lr_find()\n",
    "    if suggestions:\n",
    "        return self.get_suggested_lrs(lr_find_args['num_it'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out `xla_lr_find`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#colab\n",
    "from fastai.vision.all import *\n",
    "# path = untar_data(URLs.MNIST_TINY)\n",
    "path = untar_data(URLs.MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "data = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),\n",
    "    get_items=get_image_files,\n",
    "    get_y=parent_label,\n",
    "    # splitter=GrandparentSplitter(),\n",
    "    splitter=GrandparentSplitter(train_name='training', valid_name='testing'),\n",
    "    item_tfms=Resize(28),\n",
    "    batch_tfms=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# dls = data.dataloaders(path, bs=8)\n",
    "dls = data.dataloaders(path, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d8347fafd148579d58cb3f09cad14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "learner = cnn_learner(dls, resnet18, metrics=accuracy, concat_pool=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# %%time\n",
    "# learner.xla_lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
