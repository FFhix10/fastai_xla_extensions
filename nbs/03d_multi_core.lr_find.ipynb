{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "# attach gdrive holding repo\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp multi_core.lr_find"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Core LR Find XLA Extensions\n",
    "\n",
    "> Classes to replace LRFinder and patches to Learner\n",
    "to support running lr_find using multi core TPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifications to existing callback `LRFinder` are needed in order to run `lr_find` using multiple TPU cores. An equivalent `xla_lr_find` method is patched to `Learner` so it can run on multiple TPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 133.6MB 88kB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 3.2MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -Uqq cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp37-cp37m-linux_x86_64.whl\n",
    "# VERSION = \"nightly\"  #@param [\"1.5\", \"1.7\" , \"20200325\", \"nightly\"]\n",
    "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "# !python pytorch-xla-env-setup.py --version $VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 194kB 5.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 5.1MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "# !pip install -Uqq git+https://github.com/fastai/fastai.git \n",
    "!pip install -Uqq fastai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 51kB 2.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 51kB 4.9MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -qqq nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#colab\n",
    "# !pip install -Uqq git+https://github.com/butchland/fastai_xla_extensions.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#colab\n",
    "# !pip install -Uqq git+https://github.com/butchland/my_timesaver_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating fastai...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!curl -s https://course19.fast.ai/setup/colab | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "%cd /content\n",
    "!ln -s /content/drive/MyDrive/fastai_xla_extensions fastai_xla_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==1.7.1+cu101\n",
      "torch-xla==1.7\n",
      "torchsummary==1.5.1\n",
      "torchtext==0.3.1\n",
      "torchvision==0.8.2+cu101\n",
      "fastai==2.2.7\n",
      "fastcore==1.3.19\n",
      "fastdtw==0.3.4\n",
      "fastprogress==1.0.0\n",
      "fastrelease==0.1.11\n",
      "fastrlock==0.5\n",
      "nbdev==1.1.13\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!pip freeze | grep torch\n",
    "!pip freeze | grep fast\n",
    "!pip freeze | grep timesaver\n",
    "!pip freeze | grep nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# start of kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/fastai_xla_extensions\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "%cd /content/fastai_xla_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:TPU has started up successfully with version pytorch-1.7\n"
     ]
    }
   ],
   "source": [
    "#exporti\n",
    "from fastai_xla_extensions.utils import xla_imported\n",
    "from fastai_xla_extensions.multi_core.base import *\n",
    "from fastai_xla_extensions.multi_core.callback import *\n",
    "from fastai_xla_extensions.multi_core.learner import *\n",
    "from fastai_xla_extensions.misc_utils import *\n",
    "from fastai_xla_extensions.core import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "try:\n",
    "    import torch_xla\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "if xla_imported():\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#local\n",
    "# fake out torch_xla modules if not running on xla supported envs\n",
    "if not xla_imported():\n",
    "    # replace torch xla modules with fake equivalents\n",
    "    from types import SimpleNamespace\n",
    "    torch_xla = SimpleNamespace (\n",
    "    )\n",
    "    from typing import Union,BinaryIO\n",
    "    import os\n",
    "    import pickle\n",
    "    import torch.cuda\n",
    "\n",
    "    def fake_opt_step(opt,barrier=False):\n",
    "        opt.step()\n",
    "        \n",
    "    def fake_device(n=None, devkind=None):\n",
    "        gpu_available = torch.cuda.is_available()\n",
    "        if gpu_available:\n",
    "            return torch.device(torch.cuda.current_device()) \n",
    "        return torch.device('cpu')\n",
    "\n",
    "    def fake_save(obj, f: Union[str, os.PathLike, BinaryIO], \n",
    "                master_only=True, global_master=False): \n",
    "        return torch.save(obj,f,pickle_module=pickle, \n",
    "                        pickle_protocol=2, \n",
    "                        _use_new_zipfile_serialization=True)\n",
    "    def fake_rate():\n",
    "        return 230.20\n",
    "\n",
    "    def fake_global_rate():\n",
    "        return 830.10\n",
    "\n",
    "    def fake_add(*args,**kwargs):\n",
    "        pass\n",
    "\n",
    "    def fake_RateTracker():\n",
    "        return SimpleNamespace(\n",
    "            rate = fake_rate,\n",
    "            global_rate = fake_global_rate,\n",
    "            add = fake_add\n",
    "        )\n",
    "    def fake_xrt_world_size():\n",
    "        return 1\n",
    "    def fake_get_ordinal():\n",
    "        return 0\n",
    "    xm = SimpleNamespace(\n",
    "        optimizer_step = fake_opt_step,\n",
    "        xla_device = fake_device,\n",
    "        save = fake_save,\n",
    "        RateTracker = fake_RateTracker,\n",
    "        master_print = print,\n",
    "        xrt_world_size = fake_xrt_world_size,\n",
    "        get_ordinal = fake_get_ordinal\n",
    "    )\n",
    "\n",
    "    def fake_metrics_report():\n",
    "        return \"Fake Metrics Report \\n\\n\\n\\n\"\n",
    "    met = SimpleNamespace (\n",
    "        metrics_report = fake_metrics_report\n",
    "    )\n",
    "\n",
    "    class FakeParallelLoader:\n",
    "        def __init__(self, loader, *args):\n",
    "            self.loader = loader\n",
    "        def per_device_loader(self,device):\n",
    "            return self.loader\n",
    "        \n",
    "    pl = SimpleNamespace(\n",
    "        ParallelLoader = FakeParallelLoader\n",
    "    )\n",
    "\n",
    "    def fake_MpModelWrapper(o):\n",
    "        return o\n",
    "\n",
    "    def fake_run(f,*args, **kwargs):\n",
    "            return f(*args,**kwargs)\n",
    "        \n",
    "    def fake_MpSerialExecutor():\n",
    "        return SimpleNamespace(\n",
    "            run = fake_run\n",
    "        )\n",
    "    def fake_spawn(f, args=None, nprocs=0, start_method=None):\n",
    "        return f(0,*args)\n",
    "\n",
    "    xmp = SimpleNamespace (\n",
    "        MpModelWrapper = fake_MpModelWrapper,\n",
    "        MpSerialExecutor = fake_MpSerialExecutor,\n",
    "        spawn = fake_spawn\n",
    "    )\n",
    "\n",
    "    xu = SimpleNamespace (\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.learner import CancelValidException\n",
    "class SkipValidationCallback(Callback):\n",
    "    order,run_valid = -9, False\n",
    "    # raise CancelValidException before XLATrainingCallback.before_validate\n",
    "    # to prevent call to wrap_parallel_loader on before_validate\n",
    "    def before_validate(self): \n",
    "        raise CancelValidException()\n",
    "\n",
    "    def after_cancel_validate(self):\n",
    "        xm.mark_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from fastai.callback.schedule import ParamScheduler, SchedExp\n",
    "from fastcore.xtras import is_listy\n",
    "from fastcore.imports import noop\n",
    "class XLALRFinder(ParamScheduler):\n",
    "    \"Training with exponentially growing learning rate\"\n",
    "    def __init__(self, start_lr=1e-7, end_lr=10, num_it=100, stop_div=True):\n",
    "        if is_listy(start_lr):\n",
    "            self.scheds = {'lr': [SchedExp(s, e) for (s,e) in zip(start_lr,end_lr)]}\n",
    "        else: self.scheds = {'lr': SchedExp(start_lr, end_lr)}\n",
    "        self.num_it,self.stop_div = num_it,stop_div\n",
    "        self.skip_batch = False\n",
    "        \n",
    "\n",
    "\n",
    "    def before_fit(self):\n",
    "        super().before_fit()\n",
    "        # no need to save orig weights \n",
    "        # since learner instances are transient on spawned procs\n",
    "        # self.learn.save('_tmp')\n",
    "        self.best_loss = float('inf')\n",
    "        self.skip_batch = False\n",
    "\n",
    "    def before_epoch(self):\n",
    "        # dont report losses while running lrfind (override sync_recorder)\n",
    "        if not xm.is_master_ordinal():\n",
    "            return\n",
    "        if hasattr(self.learn, 'sync_recorder'):\n",
    "            self.learn.logger = noop\n",
    "            self.learn.sync_recorder._sync_stats_log = noop\n",
    "\n",
    "    def before_batch(self):\n",
    "        if self.skip_batch:\n",
    "            return\n",
    "        self._update_val(self.train_iter/self.num_it)\n",
    "\n",
    "    def after_batch(self):\n",
    "        if self.skip_batch:\n",
    "            return\n",
    "        super().after_batch()\n",
    "        smooth_loss = self.smooth_loss.item() # move xla tensor to cpu\n",
    "        if smooth_loss < self.best_loss:\n",
    "            self.best_loss = smooth_loss\n",
    "\n",
    "        # handle continuation of batch iteration until all batches exhausted\n",
    "        if smooth_loss > 4*self.best_loss and self.stop_div:\n",
    "            # print(f'xla {xm.get_ordinal()}: stop stats collection due to loss')\n",
    "            self.skip_batch = True\n",
    "            self.copy_losses_and_lrs()\n",
    "            return\n",
    "            \n",
    "\n",
    "        if self.train_iter >= self.num_it:\n",
    "            # print(f'xla {xm.get_ordinal()}: stop stats collection due to num_iter')\n",
    "            # return and stop updating losses\n",
    "            self.skip_batch = True\n",
    "            self.copy_losses_and_lrs()\n",
    "            return\n",
    "\n",
    "    def copy_losses_and_lrs(self):\n",
    "        if xm.is_master_ordinal():     \n",
    "            losses = [loss.item() for loss in self.recorder.losses]\n",
    "            iters = self.recorder.iters[:]\n",
    "            values = self.recorder.values[:]\n",
    "        \n",
    "            self.plot_data = {'lrs': self.recorder.lrs[:],\n",
    "                              'losses': losses,\n",
    "                              'iters': iters,\n",
    "                              'values': values}\n",
    "            if hasattr(self,'hps'):\n",
    "                self.plot_data['hps']  = {**self.hps}\n",
    "\n",
    "    def after_fit(self):\n",
    "        super().after_fit()\n",
    "        # no need to load old weights since these will be transient\n",
    "        # self.learn.opt.zero_grad() #Need to zero the gradients of the model before detaching the optimizer for future fits\n",
    "        # tmp_f = self.path/self.model_dir/'_tmp.pth'\n",
    "        # if tmp_f.exists():\n",
    "        #     self.learn.load('_tmp', with_opt=True)\n",
    "        #     os.remove(tmp_f)\n",
    "        if not self.skip_batch:\n",
    "            self.copy_losses_and_lrs()\n",
    "        if xm.is_master_ordinal():\n",
    "            with open('_plt_loss.pkl','wb') as f:\n",
    "                pickle.dump(self.plot_data,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.learner import Learner\n",
    "from fastai.callback.schedule import SuggestedLRs\n",
    "from fastcore.basics import patch\n",
    "from fastai.torch_core import tensor\n",
    "@patch\n",
    "def get_suggested_lrs(self:Learner, num_it):\n",
    "    'compute Suggested LRs'\n",
    "    lrs,losses = tensor(self.recorder.lrs[num_it//10:-5]),tensor(self.recorder.losses[num_it//10:-5])\n",
    "    if len(losses) == 0: return\n",
    "    lr_min = lrs[losses.argmin()].item()\n",
    "    grads = (losses[1:]-losses[:-1]) / (lrs[1:].log()-lrs[:-1].log())\n",
    "    lr_steep = lrs[grads.argmin()].item()\n",
    "    return SuggestedLRs(lr_min/10.,lr_steep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"Learner.get_suggested_lrs\" class=\"doc_header\"><code>Learner.get_suggested_lrs</code><a href=\"__main__.py#L6\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Learner.get_suggested_lrs</code>(**`num_it`**)\n\ncompute Suggested LRs",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "show_doc(Learner.get_suggested_lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pickle\n",
    "from fastai.learner import Recorder\n",
    "from fastcore.basics import patch   \n",
    "@patch\n",
    "def reload_lr_find_attrs(self:Recorder, fn='_plt_loss.pkl'):\n",
    "    if isinstance(fn,str):\n",
    "        fn = Path(fn)\n",
    "\n",
    "    if not fn.is_file():\n",
    "        return\n",
    "       \n",
    "    with open(fn,'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "        self.lrs,self.losses = d['lrs'],d['losses']\n",
    "        self.values, self.iters = d['values'], d['iters']\n",
    "        if 'hps' in d:\n",
    "            self.hps = d['hps']\n",
    "    # delete file after\n",
    "    if fn.is_file():\n",
    "        fn.unlink()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"Recorder.reload_lr_find_attrs\" class=\"doc_header\"><code>Recorder.reload_lr_find_attrs</code><a href=\"__main__.py#L5\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Recorder.reload_lr_find_attrs</code>(**`fn`**=*`'_plt_loss.pkl'`*)\n\n",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "show_doc(Recorder.reload_lr_find_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def xla_run_lr_find(rank, learner_args, add_args, lr_find_args, ctrl_args):\n",
    "    xm.rendezvous('start_xla_run_lr_find')\n",
    "    # print(f'xla {rank} : start run lrfind')\n",
    "    sync_valid = True\n",
    "    learner = make_xla_child_learner(rank, sync_valid, learner_args, add_args, ctrl_args)\n",
    "\n",
    "    num_it = lr_find_args['num_it']\n",
    "    n_epoch = num_it//len(learner.dls.train) + 1\n",
    "    learner.opt = None\n",
    "    learner.create_opt()\n",
    "    cb = XLALRFinder(**lr_find_args) \n",
    " \n",
    "    skip_valid_cb = SkipValidationCallback()\n",
    "    \n",
    "    with learner.no_logging(): \n",
    "        learner.fit(n_epoch, cbs=[cb, skip_valid_cb])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from pathlib import Path\n",
    "from fastai.learner import Learner\n",
    "from fastcore.basics import patch\n",
    "from fastcore.meta import delegates\n",
    "\n",
    "@patch\n",
    "@delegates(Learner.lr_find)\n",
    "def xla_lr_find(self:Learner, num_cores=8, start_method='fork', **kwargs):\n",
    "    lr_find_args = {\n",
    "        'start_lr': 1e-7,\n",
    "        'end_lr': 10.,\n",
    "        'num_it': 100,\n",
    "        'stop_div': True\n",
    "    }\n",
    "    fn = Path('_plt_loss.pkl')\n",
    "    if fn.is_file():\n",
    "        fn.unlink()\n",
    "    # remove show_plot and suggestions param\n",
    "    show_plot = kwargs.pop('show_plot', True)\n",
    "    suggestions = kwargs.pop('suggestions',True)\n",
    "    # override default with kwargs\n",
    "    lr_find_args = {**lr_find_args, **kwargs}    \n",
    "\n",
    "    ctrl_args = self.pre_xla_fit()\n",
    "    learner_args, add_args = self.pack_learner_args()\n",
    "    xmp.spawn(xla_run_lr_find,\n",
    "              args=(learner_args, add_args, lr_find_args, ctrl_args),\n",
    "              nprocs=num_cores,\n",
    "              start_method=start_method)\n",
    "    self.post_xla_fit(ctrl_args)\n",
    "    self.recorder.reload_lr_find_attrs()\n",
    "    if show_plot:\n",
    "        # show_loss()\n",
    "        self.recorder.plot_lr_find()\n",
    "    if suggestions:\n",
    "        return self.get_suggested_lrs(lr_find_args['num_it'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out `xla_lr_find`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "from fastai.vision.all import *\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "# path = untar_data(URLs.MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "data = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),\n",
    "    get_items=get_image_files,\n",
    "    get_y=parent_label,\n",
    "    splitter=GrandparentSplitter(),\n",
    "    # splitter=GrandparentSplitter(train_name='training', valid_name='testing'),\n",
    "    item_tfms=Resize(28),\n",
    "    batch_tfms=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "dls = data.dataloaders(path, bs=8)\n",
    "# dls = data.dataloaders(path, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "learner = cnn_learner(dls, resnet18, metrics=accuracy, concat_pool=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.63 µs\n"
     ]
    }
   ],
   "source": [
    "#colab\n",
    "%%time\n",
    "# learner.xla_lr_find()\n",
    "# learner.xla_lr_find(num_it=117, stop_div=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
