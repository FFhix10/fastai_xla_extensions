{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "# attach gdrive holding repo\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp multi_core.lr_find"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Core LR Find XLA Extensions\n",
    "\n",
    "> Classes to replace LRFinder and patches to Learner\n",
    "to support running lr_find using multi core TPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifications to existing callback `LRFinder` are needed in order to run `lr_find` using multiple TPU cores. An equivalent `xla_lr_find` method is patched to `Learner` so it can run on multiple TPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 133.6MB 81kB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 3.1MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -Uqq cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 194kB 3.0MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 3.2MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "# !pip install -Uqq git+https://github.com/fastai/fastai.git \n",
    "!pip install -Uqq fastai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r\u001b[K     |███████▏                        | 10kB 21.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 20kB 13.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 30kB 11.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 40kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 3.0MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!pip install -qqq nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#colab\n",
    "# !pip install -Uqq git+https://github.com/butchland/fastai_xla_extensions.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#colab\n",
    "# !pip install -Uqq git+https://github.com/butchland/my_timesaver_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating fastai...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "!curl -s https://course19.fast.ai/setup/colab | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "%cd /content\n",
    "!ln -s /content/drive/MyDrive/fastai_xla_extensions  fastai_xla_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==1.7.0+cu101\n",
      "torch-xla==1.7\n",
      "torchsummary==1.5.1\n",
      "torchtext==0.3.1\n",
      "torchvision==0.8.1+cu101\n",
      "fastai==2.2.7\n",
      "fastcore==1.3.19\n",
      "fastdtw==0.3.4\n",
      "fastprogress==1.0.0\n",
      "fastrlock==0.5\n",
      "nbdev==1.1.12\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!pip freeze | grep torch\n",
    "!pip freeze | grep fast\n",
    "!pip freeze | grep timesaver\n",
    "!pip freeze | grep nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# start of kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/fastai_xla_extensions\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "# colab\n",
    "%cd /content/fastai_xla_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:TPU has started up successfully with version pytorch-1.7\n"
     ]
    }
   ],
   "source": [
    "#exporti\n",
    "from fastai_xla_extensions.utils import xla_imported\n",
    "from fastai_xla_extensions.misc_utils import *\n",
    "from fastai_xla_extensions.multi_core.base import *\n",
    "from fastai_xla_extensions.multi_core.learner import *\n",
    "from fastai_xla_extensions.multi_core.callback import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#colab\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "try:\n",
    "    import torch_xla\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "if xla_imported():\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#local\n",
    "# fake out torch_xla modules if not running on xla supported envs\n",
    "if not xla_imported():\n",
    "    # replace torch xla modules with fake equivalents\n",
    "    from types import SimpleNamespace\n",
    "    torch_xla = SimpleNamespace (\n",
    "    )\n",
    "    from typing import Union,BinaryIO\n",
    "    import os\n",
    "    import pickle\n",
    "    import torch.cuda\n",
    "\n",
    "    def fake_opt_step(opt,barrier=False):\n",
    "        opt.step()\n",
    "        \n",
    "    def fake_device(n=None, devkind=None):\n",
    "        gpu_available = torch.cuda.is_available()\n",
    "        if gpu_available:\n",
    "            return torch.device(torch.cuda.current_device()) \n",
    "        return torch.device('cpu')\n",
    "\n",
    "    def fake_save(obj, f: Union[str, os.PathLike, BinaryIO], \n",
    "                master_only=True, global_master=False): \n",
    "        return torch.save(obj,f,pickle_module=pickle, \n",
    "                        pickle_protocol=2, \n",
    "                        _use_new_zipfile_serialization=True)\n",
    "    def fake_rate():\n",
    "        return 230.20\n",
    "\n",
    "    def fake_global_rate():\n",
    "        return 830.10\n",
    "\n",
    "    def fake_add(*args,**kwargs):\n",
    "        pass\n",
    "\n",
    "    def fake_RateTracker():\n",
    "        return SimpleNamespace(\n",
    "            rate = fake_rate,\n",
    "            global_rate = fake_global_rate,\n",
    "            add = fake_add\n",
    "        )\n",
    "    def fake_xrt_world_size():\n",
    "        return 1\n",
    "    def fake_get_ordinal():\n",
    "        return 0\n",
    "    xm = SimpleNamespace(\n",
    "        optimizer_step = fake_opt_step,\n",
    "        xla_device = fake_device,\n",
    "        save = fake_save,\n",
    "        RateTracker = fake_RateTracker,\n",
    "        master_print = print,\n",
    "        xrt_world_size = fake_xrt_world_size,\n",
    "        get_ordinal = fake_get_ordinal\n",
    "    )\n",
    "\n",
    "    def fake_metrics_report():\n",
    "        return \"Fake Metrics Report \\n\\n\\n\\n\"\n",
    "    met = SimpleNamespace (\n",
    "        metrics_report = fake_metrics_report\n",
    "    )\n",
    "\n",
    "    class FakeParallelLoader:\n",
    "        def __init__(self, loader, *args):\n",
    "            self.loader = loader\n",
    "        def per_device_loader(self,device):\n",
    "            return self.loader\n",
    "        \n",
    "    pl = SimpleNamespace(\n",
    "        ParallelLoader = FakeParallelLoader\n",
    "    )\n",
    "\n",
    "    def fake_MpModelWrapper(o):\n",
    "        return o\n",
    "\n",
    "    def fake_run(f,*args, **kwargs):\n",
    "            return f(*args,**kwargs)\n",
    "        \n",
    "    def fake_MpSerialExecutor():\n",
    "        return SimpleNamespace(\n",
    "            run = fake_run\n",
    "        )\n",
    "    def fake_spawn(f, args=None, nprocs=0, start_method=None):\n",
    "        return f(0,*args)\n",
    "\n",
    "    xmp = SimpleNamespace (\n",
    "        MpModelWrapper = fake_MpModelWrapper,\n",
    "        MpSerialExecutor = fake_MpSerialExecutor,\n",
    "        spawn = fake_spawn\n",
    "    )\n",
    "\n",
    "    xu = SimpleNamespace (\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "# from fastai.vision.all import *\n",
    "# from fastai_xla_extensions.all import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.learner import CancelValidException\n",
    "\n",
    "class SkipValidationCallback(Callback):\n",
    "    order,run_valid = -9, False\n",
    "    # raise CancelValidException before XLATrainingCallback.before_validate\n",
    "    # to prevent call to wrap_parallel_loader on before_validate\n",
    "    def before_validate(self): \n",
    "        raise CancelValidException()\n",
    "\n",
    "    def after_cancel_validate(self):\n",
    "        if getattr(self.learn,'inner_xla', False):\n",
    "            xm.mark_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.callback.schedule import ParamScheduler, SchedExp\n",
    "\n",
    "class XLALRFinder(ParamScheduler):\n",
    "    \"Training with exponentially growing learning rate\"\n",
    "    def __init__(self, start_lr=1e-7, end_lr=10, num_it=100, stop_div=True):\n",
    "        if is_listy(start_lr):\n",
    "            self.scheds = {'lr': [SchedExp(s, e) for (s,e) in zip(start_lr,end_lr)]}\n",
    "        else: self.scheds = {'lr': SchedExp(start_lr, end_lr)}\n",
    "        self.num_it,self.stop_div = num_it,stop_div\n",
    "        self.skip_batch = False\n",
    "        self.num_losses = 0\n",
    "\n",
    "    def before_fit(self):\n",
    "        super().before_fit()\n",
    "        # no need to save orig weights \n",
    "        # since learner instances are transient on spawned procs\n",
    "        # self.learn.save('_tmp')\n",
    "        self.best_loss = float('inf')\n",
    "        self.skip_batch = False\n",
    "        self.num_losses = 0\n",
    "        # dont report losses while running lrfind (override sync_recorder)\n",
    "        # run after sync_recorder.before_fit (sync_recorder.order == 55)\n",
    "        # while param scheduler order == 60\n",
    "        if getattr(self.learn,'inner_xla',False) \\\n",
    "        and xm.is_master_ordinal() and hasattr(self.learn, 'sync_recorder'):\n",
    "            self.learn.logger = noop\n",
    "            self.learn.sync_recorder._sync_stats_log = noop\n",
    "\n",
    "\n",
    "    def before_batch(self):\n",
    "        if self.skip_batch:\n",
    "            return\n",
    "        self._update_val(self.train_iter/self.num_it)\n",
    "\n",
    "    def after_batch(self):\n",
    "        if self.skip_batch:\n",
    "            return\n",
    "        super().after_batch()\n",
    "        smooth_loss = self.smooth_loss.item() # move xla tensor to cpu\n",
    "        self.num_loss = len(self.recorder.losses)\n",
    "        if smooth_loss < self.best_loss:\n",
    "            self.best_loss = smooth_loss\n",
    "\n",
    "        # handle continuation of batch iteration until all batches exhausted\n",
    "        if smooth_loss > 4*self.best_loss and self.stop_div:\n",
    "            self.skip_batch = True\n",
    "            return\n",
    "            \n",
    "        if self.train_iter >= self.num_it:\n",
    "            self.skip_batch = True\n",
    "            return\n",
    "\n",
    "    def after_fit(self):\n",
    "        # no need to load old weights since these will be transient\n",
    "        # self.learn.opt.zero_grad() \n",
    "        # Need to zero the gradients of the model before detaching the optimizer for future fits\n",
    "        # tmp_f = self.path/self.model_dir/'_tmp.pth'\n",
    "        # if tmp_f.exists():\n",
    "        #     self.learn.load('_tmp', with_opt=True)\n",
    "        #     os.remove(tmp_f)\n",
    "        if not getattr(self.learn,'inner_xla', False):\n",
    "            return # skip if not on spawned process\n",
    "            \n",
    "        if not xm.is_master_ordinal(): return\n",
    "\n",
    "        if not self.skip_batch: # completed w/o copying lrs and losses from recorder to plot_data\n",
    "            self.num_loss = len(self.recorder.losses)\n",
    "\n",
    "        self.recorder.losses = self.recorder.losses[: self.num_loss]\n",
    "        self.recorder.lrs = self.recorder.lrs[: self.num_loss]\n",
    "        num_iters = len(self.recorder.iters)\n",
    "        for i, iter in enumerate(self.recorder.iters):\n",
    "            if iter >= self.num_it:\n",
    "                num_iters = i + 1\n",
    "                break\n",
    "        self.recorder.iters = self.recorder.iters[:num_iters]\n",
    "        self.recorder.values = self.recorder.values[:num_iters]\n",
    "        self.recorder.dump_attrs() # rewrite updated attrs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def xla_run_lr_find(rank, learner_args, add_args, lr_find_args, ctrl_args):\n",
    "    'run xla lr_find on spawned processes'\n",
    "    xm.rendezvous('start_run_lrfind')\n",
    "    # print(f'xla {rank} : start run lrfind')\n",
    "    sync_valid = True\n",
    "    learner = make_xla_child_learner(rank, sync_valid, learner_args, add_args, ctrl_args)\n",
    "    num_it = lr_find_args['num_it']\n",
    "    n_epoch = num_it//len(learner.dls.train) + 1\n",
    "    lr_find_cb = XLALRFinder(**lr_find_args)\n",
    "\n",
    "    skip_valid_cb = SkipValidationCallback()\n",
    "    \n",
    "    with learner.no_logging(): \n",
    "        learner.fit(n_epoch, cbs=[lr_find_cb, skip_valid_cb])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.learner import Learner\n",
    "from fastai.callback.schedule import SuggestedLRs\n",
    "from fastai.basics import patch\n",
    "from fastai.torch_core import tensor\n",
    "@patch\n",
    "def get_suggested_lrs(self:Learner, num_it):\n",
    "    'compute Suggested LRs'\n",
    "    lrs,losses = tensor(self.recorder.lrs[num_it//10:-5]),tensor(self.recorder.losses[num_it//10:-5])\n",
    "    if len(losses) == 0: return\n",
    "    lr_min = lrs[losses.argmin()].item()\n",
    "    grads = (losses[1:]-losses[:-1]) / (lrs[1:].log()-lrs[:-1].log())\n",
    "    lr_steep = lrs[grads.argmin()].item()\n",
    "    return SuggestedLRs(lr_min/10.,lr_steep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"Learner.get_suggested_lrs\" class=\"doc_header\"><code>Learner.get_suggested_lrs</code><a href=\"__main__.py#L6\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>Learner.get_suggested_lrs</code>(**`num_it`**)\n\ncompute Suggested LRs",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "show_doc(Learner.get_suggested_lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.learner import Learner\n",
    "from fastcore.basics import patch\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.foundation import L\n",
    "from fastai.callback.progress import ProgressCallback\n",
    "@patch\n",
    "@delegates(Learner.lr_find, but='num_cores,start_method')\n",
    "def xla_lr_find(self:Learner, num_cores=8, start_method='fork', **kwargs):\n",
    "    'multi core xla equivalent of `lr_find`'\n",
    "    # default params for lr_find\n",
    "    lr_find_args = {\n",
    "        'start_lr': 1e-7,\n",
    "        'end_lr': 10.,\n",
    "        'num_it': 100,\n",
    "        'stop_div': True\n",
    "    }\n",
    "    has_progress = 'progress' in L(self.cbs).attrgot('name')\n",
    "    show_plot = True\n",
    "    suggestions = True\n",
    "\n",
    "    # remove show_plot and suggestions param\n",
    "    if 'show_plot' in kwargs:\n",
    "        show_plot = kwargs.pop('show_plot')\n",
    "    if 'suggestions' in kwargs:\n",
    "        suggestions = kwargs.pop('suggestions')\n",
    "    # override default with kwargs\n",
    "    lr_find_args = {**lr_find_args, **kwargs}    \n",
    "\n",
    "    ctrl_args = self.pre_xla_fit()\n",
    "    learner_args, add_args = self.pack_learner_args()\n",
    "    xmp.spawn(xla_run_lr_find,\n",
    "              args=(learner_args, add_args, lr_find_args, ctrl_args),\n",
    "              nprocs=num_cores,\n",
    "              start_method=start_method)\n",
    "     \n",
    "    # self.recorder.reload_attrs()\n",
    "    # self.recorder.reload_hps()\n",
    "    # if has_progress and 'progress' not in L(self.cbs).attrgot('name'):\n",
    "    #     self.add_cbs([ProgressCallback])\n",
    "    self.post_xla_fit(ctrl_args)\n",
    "    if show_plot:\n",
    "        self.recorder.plot_lr_find()\n",
    "    if suggestions:\n",
    "        return self.get_suggested_lrs(lr_find_args['num_it'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "from fastai.vision.all import *\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "# path = untar_data(URLs.MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "data = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),\n",
    "    get_items=get_image_files,\n",
    "    get_y=parent_label,\n",
    "    # splitter=GrandparentSplitter(train_name='training', valid_name='testing'),\n",
    "    splitter=GrandparentSplitter(),\n",
    "    item_tfms=Resize(28),\n",
    "    batch_tfms=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# dls = data.dataloaders(path, bs=64)\n",
    "dls = data.dataloaders(path, bs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "learner = cnn_learner(dls, resnet18, metrics=accuracy, concat_pool=False, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.336851</td>\n",
       "      <td>0.693206</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.303227</td>\n",
       "      <td>0.835756</td>\n",
       "      <td>0.518466</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.303836</td>\n",
       "      <td>1.038976</td>\n",
       "      <td>0.566761</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.281499</td>\n",
       "      <td>0.663321</td>\n",
       "      <td>0.751420</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.280513</td>\n",
       "      <td>0.194353</td>\n",
       "      <td>0.916193</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#colab\n",
    "learner.xla_fit_one_cycle(5,lr_max=slice(8e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# %%time\n",
    "learner.xla_lr_find(stop_div=True,end_lr=100, num_it=400)\n",
    "# learner.xla_lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab\n",
    "# learner.xla_fit_one_cycle(5, lr_max=slice(0.026))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
