# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00_core.ipynb (unless otherwise specified).

__all__ = ['xla_imported', 'xla_available_config', 'xla_module_exist', '__getstate__', '__setstate__', 'XLAOptimProxy',
           'XLAOptCallback']

# Cell
#hide_output
import importlib
import os
import sys

def xla_imported(): return 'torch_xla' in sys.modules
def xla_available_config(): return os.environ.get("XRT_DEVICE_MAP", False) and os.environ.get("XRT_WORKERS", False)
def xla_module_exist(): return importlib.util.find_spec('torch_xla')

# Internal Cell
if not xla_imported():
    from types import SimpleNamespace
    import torch.cuda
    def fake_opt_step(opt,barrier=False):
        opt.step()
    def fake_device(n=None, devkind=None):
        gpu_available = torch.cuda.is_available()
        return torch.device(torch.cuda.current_device()) if gpu_available else torch.device('cpu')
    #xm = SimpleNamespace(
    #    optimizer_step = fake_opt_step,
    #    xla_device = fake_device
    #)

# Cell
from fastcore.foundation import GetAttr
from fastai.optimizer import Optimizer
from copy import deepcopy

# Right now deciding to patch instead of add with a PickableOpt(Optimizer) class like in previous versions
from fastcore.basics import patch_to
from fastai.optimizer import _BaseOptimizer

@patch_to(_BaseOptimizer)
def __getstate__(self):
    # https://github.com/pytorch/pytorch/blob/46b252b83a97bba0926cead050d76fcef129cb6b/torch/optim/optimizer.py#L54
    d = {
            'defaults': self.defaults,
            'state': self.state_dict(),
            'param_groups': self.param_groups,
        }
    return

@patch_to(_BaseOptimizer)
def __setstate__(self, data):
    # https://github.com/pytorch/pytorch/blob/46b252b83a97bba0926cead050d76fcef129cb6b/torch/optim/optimizer.py#L61
    self.defaults = data['defaults']
    self.load_state_dict(data['state'])
    self.param_groups = data['param_groups']

# Cell
#colab
import torch_xla.core.xla_model as xm

# Cell

class XLAOptimProxy(GetAttr):
    _default='opt'
    "Proxy optimizer to override `opt.step` with Pytorch XLA sync method `xm.optimizer_step` "
    def __init__(self,opt, barrier):
        self.opt = opt # because not using PickableOpt(opt) for the moment
        self._barrier = barrier

    def step(self):
        xm.optimizer_step(self.opt,barrier=self._barrier) if xla_imported() else self.opt.step()

    @property
    def barrier(self): return self._barrier
    @barrier.setter
    def barrier(self,v): self._barrier = v

# Cell
from fastai.callback.core import Callback
from fastai.data.core import DataLoaders
from fastai.vision.all import to_device


class XLAOptCallback(Callback):
    'Callback to replace `opt.step` with `xm.optimizer_step(opt)` as required to run on TPU'
    def __init__(self, barrier=True):
        self._barrier = barrier

    def before_fit(self):
        'replace opt with proxy which calls `xm.optimizer_step` instead of `opt.step` and set `dls.device` and model to `xla_device`'
        if self.learn.opt is not None:
            if not isinstance(self.learn.opt,XLAOptimProxy):
                opt = self.learn.opt
                self.learn.opt = XLAOptimProxy(opt, barrier=self._barrier)

    def after_fit(self):
        'restore original opt '
        if isinstance(self.learn.opt, XLAOptimProxy):
            opt = self.learn.opt.opt
            self.learn.opt = opt
    @property
    def barrier(self): return self._barrier
    @barrier.setter
    def barrier(self,v): self._barrier = v

# Cell
# if xla_imported():
#     from fastcore.foundation import defaults
#     if hasattr(defaults,'callbacks'):
#         if XLAOptCallback not in defaults.callbacks:
#             defaults.callbacks.append(XLAOptCallback)
#     else:
#         defaults.callbacks = [XLAOptCallback]

# Cell
if xla_imported():
    from fastcore.foundation import patch
    from fastai.learner import Learner
    from fastai.callback.hook import summary as orig_summary
    @patch
    def xlasummary(self:Learner):
        to_device(self.dls, device=xm.xla_device())
        return orig_summary(self)